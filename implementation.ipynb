{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "131a7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                           f1_score, roc_auc_score, average_precision_score,\n",
    "                           classification_report, confusion_matrix)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b155f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0f10007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2150cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103121 entries, 0 to 103120\n",
      "Data columns (total 79 columns):\n",
      " #   Column                      Non-Null Count   Dtype   \n",
      "---  ------                      --------------   -----   \n",
      " 0   Protocol                    103121 non-null  int8    \n",
      " 1   Flow Duration               103121 non-null  int32   \n",
      " 2   Total Fwd Packet            103121 non-null  int32   \n",
      " 3   Total Bwd packets           103121 non-null  int32   \n",
      " 4   Total Length of Fwd Packet  103121 non-null  int32   \n",
      " 5   Total Length of Bwd Packet  103121 non-null  int32   \n",
      " 6   Fwd Packet Length Max       103121 non-null  int32   \n",
      " 7   Fwd Packet Length Min       103121 non-null  int16   \n",
      " 8   Fwd Packet Length Mean      103121 non-null  float32 \n",
      " 9   Fwd Packet Length Std       103121 non-null  float32 \n",
      " 10  Bwd Packet Length Max       103121 non-null  int32   \n",
      " 11  Bwd Packet Length Min       103121 non-null  int16   \n",
      " 12  Bwd Packet Length Mean      103121 non-null  float32 \n",
      " 13  Bwd Packet Length Std       103121 non-null  float32 \n",
      " 14  Flow Bytes/s                103121 non-null  float64 \n",
      " 15  Flow Packets/s              103121 non-null  float64 \n",
      " 16  Flow IAT Mean               103121 non-null  float32 \n",
      " 17  Flow IAT Std                103121 non-null  float32 \n",
      " 18  Flow IAT Max                103121 non-null  int32   \n",
      " 19  Flow IAT Min                103121 non-null  int32   \n",
      " 20  Fwd IAT Total               103121 non-null  int32   \n",
      " 21  Fwd IAT Mean                103121 non-null  float32 \n",
      " 22  Fwd IAT Std                 103121 non-null  float32 \n",
      " 23  Fwd IAT Max                 103121 non-null  int32   \n",
      " 24  Fwd IAT Min                 103121 non-null  int32   \n",
      " 25  Bwd IAT Total               103121 non-null  int32   \n",
      " 26  Bwd IAT Mean                103121 non-null  float32 \n",
      " 27  Bwd IAT Std                 103121 non-null  float32 \n",
      " 28  Bwd IAT Max                 103121 non-null  int32   \n",
      " 29  Bwd IAT Min                 103121 non-null  int32   \n",
      " 30  Fwd PSH Flags               103121 non-null  int8    \n",
      " 31  Bwd PSH Flags               103121 non-null  int8    \n",
      " 32  Fwd URG Flags               103121 non-null  int8    \n",
      " 33  Bwd URG Flags               103121 non-null  int8    \n",
      " 34  Fwd Header Length           103121 non-null  int32   \n",
      " 35  Bwd Header Length           103121 non-null  int32   \n",
      " 36  Fwd Packets/s               103121 non-null  float32 \n",
      " 37  Bwd Packets/s               103121 non-null  float32 \n",
      " 38  Packet Length Min           103121 non-null  int16   \n",
      " 39  Packet Length Max           103121 non-null  int32   \n",
      " 40  Packet Length Mean          103121 non-null  float32 \n",
      " 41  Packet Length Std           103121 non-null  float32 \n",
      " 42  Packet Length Variance      103121 non-null  float32 \n",
      " 43  FIN Flag Count              103121 non-null  int8    \n",
      " 44  SYN Flag Count              103121 non-null  int8    \n",
      " 45  RST Flag Count              103121 non-null  int8    \n",
      " 46  PSH Flag Count              103121 non-null  int32   \n",
      " 47  ACK Flag Count              103121 non-null  int32   \n",
      " 48  URG Flag Count              103121 non-null  int8    \n",
      " 49  CWE Flag Count              103121 non-null  int8    \n",
      " 50  ECE Flag Count              103121 non-null  int8    \n",
      " 51  Down/Up Ratio               103121 non-null  int16   \n",
      " 52  Avg Packet Size             103121 non-null  float32 \n",
      " 53  Fwd Segment Size Avg        103121 non-null  float32 \n",
      " 54  Bwd Segment Size Avg        103121 non-null  float32 \n",
      " 55  Fwd Bytes/Bulk Avg          103121 non-null  int8    \n",
      " 56  Fwd Packet/Bulk Avg         103121 non-null  int8    \n",
      " 57  Fwd Bulk Rate Avg           103121 non-null  int8    \n",
      " 58  Bwd Bytes/Bulk Avg          103121 non-null  int8    \n",
      " 59  Bwd Packet/Bulk Avg         103121 non-null  int32   \n",
      " 60  Bwd Bulk Rate Avg           103121 non-null  int32   \n",
      " 61  Subflow Fwd Packets         103121 non-null  int8    \n",
      " 62  Subflow Fwd Bytes           103121 non-null  int16   \n",
      " 63  Subflow Bwd Packets         103121 non-null  int8    \n",
      " 64  Subflow Bwd Bytes           103121 non-null  int16   \n",
      " 65  FWD Init Win Bytes          103121 non-null  int32   \n",
      " 66  Bwd Init Win Bytes          103121 non-null  int32   \n",
      " 67  Fwd Act Data Packets        103121 non-null  int32   \n",
      " 68  Fwd Seg Size Min            103121 non-null  int8    \n",
      " 69  Active Mean                 103121 non-null  int8    \n",
      " 70  Active Std                  103121 non-null  int8    \n",
      " 71  Active Max                  103121 non-null  int8    \n",
      " 72  Active Min                  103121 non-null  int8    \n",
      " 73  Idle Mean                   103121 non-null  float32 \n",
      " 74  Idle Std                    103121 non-null  float32 \n",
      " 75  Idle Max                    103121 non-null  float32 \n",
      " 76  Idle Min                    103121 non-null  float32 \n",
      " 77  Label                       103121 non-null  category\n",
      " 78  Label.1                     103121 non-null  category\n",
      "dtypes: category(2), float32(22), float64(2), int16(6), int32(25), int8(22)\n",
      "memory usage: 23.6 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Label\n",
       "Non-Tor    64804\n",
       "NonVPN     20216\n",
       "VPN        16922\n",
       "Tor         1179\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"cicdarknet2020.parquet\", engine=\"fastparquet\")\n",
    "df.info()\n",
    "df['Label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c1ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INITIAL DATA INSPECTION ===\n",
      "DataFrame shape: (103121, 79)\n",
      "Memory usage: 23.60 MB\n",
      "\n",
      "=== DATA TYPES DETAILED ===\n",
      "Label column info:\n",
      "  dtype: category\n",
      "  type of dtype: <class 'pandas.core.dtypes.dtypes.CategoricalDtype'>\n",
      "  is categorical? True\n",
      "  is string? True\n",
      "\n",
      "Label.1 column info:\n",
      "  dtype: category\n",
      "  type of dtype: <class 'pandas.core.dtypes.dtypes.CategoricalDtype'>\n",
      "  is categorical? True\n",
      "  is string? True\n",
      "\n",
      "=== DECIDING WHICH LABEL TO USE ===\n",
      "Based on your output:\n",
      "1. Label column: Has values 0, 1, 2, 3 (4 classes)\n",
      "2. Label.1 column: Has actual names like 'Browsing', 'P2P', etc.\n",
      "\n",
      "Checking if Label is already encoded...\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: NonVPN -> Label.1: Chat\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Tor -> Label.1: VOIP\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: NonVPN -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Email\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Email\n",
      "Label: VPN -> Label.1: VOIP\n",
      "Label: Non-Tor -> Label.1: Video-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: VOIP\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: VPN -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Video-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Video-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: NonVPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Email\n",
      "Label: NonVPN -> Label.1: Chat\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: File-Transfer\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: VPN -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Email\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: NonVPN -> Label.1: Email\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: NonVPN -> Label.1: Email\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: AUDIO-STREAMING\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: NonVPN -> Label.1: Video-Streaming\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: NonVPN -> Label.1: File-transfer\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: NonVPN -> Label.1: Video-Streaming\n",
      "Label: NonVPN -> Label.1: File-Transfer\n",
      "Label: VPN -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: AUDIO-STREAMING\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Video-Streaming\n",
      "\n",
      "=== CREATING PROPER LABEL MAPPING ===\n",
      "             most_common  num_unique  \\\n",
      "Label                                  \n",
      "Non-Tor  AUDIO-STREAMING           8   \n",
      "NonVPN              Chat           8   \n",
      "Tor      Audio-Streaming           8   \n",
      "VPN        File-Transfer           6   \n",
      "\n",
      "                                             sample_values  \n",
      "Label                                                       \n",
      "Non-Tor  [AUDIO-STREAMING, Browsing, Chat, Email, File-...  \n",
      "NonVPN   [Chat, Audio-Streaming, Email, File-Transfer, ...  \n",
      "Tor      [Audio-Streaming, Browsing, Chat, File-Transfe...  \n",
      "VPN      [File-Transfer, Chat, Audio-Streaming, Email, ...  \n",
      "\n",
      "Based on analysis, I recommend using Label.1 as it has the actual class names\n",
      "\n",
      "=== DATA CLEANING ===\n",
      "Checking for duplicate columns...\n",
      "Duplicate columns: []\n",
      "Constant columns: ['Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Bwd Bytes/Bulk Avg', 'Subflow Bwd Packets', 'Active Mean', 'Active Std', 'Active Max', 'Active Min']\n",
      "Dropped constant columns: ['Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Bwd Bytes/Bulk Avg', 'Subflow Bwd Packets', 'Active Mean', 'Active Std', 'Active Max', 'Active Min']\n",
      "\n",
      "=== HANDLING MISSING/INFINITE VALUES ===\n",
      "\n",
      "=== LABEL PROCESSING ===\n",
      "\n",
      "Cleaned label distribution:\n",
      "  'browsing': 29862 samples (28.96%)\n",
      "  'p2p': 23404 samples (22.70%)\n",
      "  'audio-streaming': 11328 samples (10.99%)\n",
      "  'file-transfer': 10647 samples (10.32%)\n",
      "  'chat': 10365 samples (10.05%)\n",
      "  'video-streaming': 9012 samples (8.74%)\n",
      "  'email': 5442 samples (5.28%)\n",
      "  'voip': 3061 samples (2.97%)\n",
      "\n",
      "=== FINAL LABEL ENCODING ===\n",
      "Class mapping:\n",
      "  0: 'audio-streaming' - 11328 samples (10.99%)\n",
      "  1: 'browsing' - 29862 samples (28.96%)\n",
      "  2: 'chat' - 10365 samples (10.05%)\n",
      "  3: 'email' - 5442 samples (5.28%)\n",
      "  4: 'file-transfer' - 10647 samples (10.32%)\n",
      "  5: 'p2p' - 23404 samples (22.70%)\n",
      "  6: 'video-streaming' - 9012 samples (8.74%)\n",
      "  7: 'voip' - 3061 samples (2.97%)\n",
      "\n",
      "=== FEATURE PREPARATION ===\n",
      "Features shape: (103121, 62)\n",
      "Labels shape: (103121,)\n",
      "\n",
      "=== FEATURE SCALING ===\n",
      "Scaled 62 numeric features\n",
      "\n",
      "=== DATA SPLITTING ===\n",
      "Class distribution:\n",
      "  1: 'browsing' - 29862 samples (28.96%)\n",
      "  5: 'p2p' - 23404 samples (22.70%)\n",
      "  0: 'audio-streaming' - 11328 samples (10.99%)\n",
      "  4: 'file-transfer' - 10647 samples (10.32%)\n",
      "  2: 'chat' - 10365 samples (10.05%)\n",
      "  6: 'video-streaming' - 9012 samples (8.74%)\n",
      "  3: 'email' - 5442 samples (5.28%)\n",
      "  7: 'voip' - 3061 samples (2.97%)\n",
      "\n",
      "Using stratified split\n",
      "\n",
      "Training set: 82496 samples\n",
      "Testing set: 20625 samples\n",
      "\n",
      "=== SAVING DATA ===\n",
      "Saved to: cicdarknet_preprocessed_20251210_164345.pkl\n",
      "\n",
      "=== SUMMARY ===\n",
      "Original data: (103121, 67)\n",
      "Features: 62\n",
      "Classes: 8\n",
      "Training samples: 82496\n",
      "Test samples: 20625\n",
      "\n",
      "Data preprocessing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(\"=== INITIAL DATA INSPECTION ===\")\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage().sum() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Let's check the actual dtypes more carefully\n",
    "print(\"\\n=== DATA TYPES DETAILED ===\")\n",
    "print(\"Label column info:\")\n",
    "print(f\"  dtype: {df['Label'].dtype}\")\n",
    "print(f\"  type of dtype: {type(df['Label'].dtype)}\")\n",
    "print(f\"  is categorical? {pd.api.types.is_categorical_dtype(df['Label'])}\")\n",
    "print(f\"  is string? {pd.api.types.is_string_dtype(df['Label'])}\")\n",
    "\n",
    "print(\"\\nLabel.1 column info:\")\n",
    "print(f\"  dtype: {df['Label.1'].dtype}\")\n",
    "print(f\"  type of dtype: {type(df['Label.1'].dtype)}\")\n",
    "print(f\"  is categorical? {pd.api.types.is_categorical_dtype(df['Label.1'])}\")\n",
    "print(f\"  is string? {pd.api.types.is_string_dtype(df['Label.1'])}\")\n",
    "\n",
    "# DECISION TIME: Which label to use?\n",
    "print(\"\\n=== DECIDING WHICH LABEL TO USE ===\")\n",
    "print(\"Based on your output:\")\n",
    "print(\"1. Label column: Has values 0, 1, 2, 3 (4 classes)\")\n",
    "print(\"2. Label.1 column: Has actual names like 'Browsing', 'P2P', etc.\")\n",
    "\n",
    "# Let me check if Label might be encoded already\n",
    "print(\"\\nChecking if Label is already encoded...\")\n",
    "# Get a mapping by sampling\n",
    "sample_size = min(100, len(df))\n",
    "sample = df[['Label', 'Label.1']].sample(sample_size)\n",
    "for _, row in sample.iterrows():\n",
    "    print(f\"Label: {row['Label']} -> Label.1: {row['Label.1']}\")\n",
    "\n",
    "\n",
    "\n",
    "# Let's create a proper mapping\n",
    "print(\"\\n=== CREATING PROPER LABEL MAPPING ===\")\n",
    "# Group by Label and see what Label.1 values correspond\n",
    "label_mapping_df = df.groupby('Label')['Label.1'].agg(['first', 'nunique', lambda x: list(x.unique())[:5]])\n",
    "label_mapping_df.columns = ['most_common', 'num_unique', 'sample_values']\n",
    "print(label_mapping_df)\n",
    "\n",
    "# If Label is already encoded and Label.1 has the names, use Label.1\n",
    "print(\"\\nBased on analysis, I recommend using Label.1 as it has the actual class names\")\n",
    "\n",
    "# Clean up the data\n",
    "print(\"\\n=== DATA CLEANING ===\")\n",
    "print(\"Checking for duplicate columns...\")\n",
    "duplicate_columns = df.columns[df.columns.duplicated()]\n",
    "print(f\"Duplicate columns: {list(duplicate_columns)}\")\n",
    "\n",
    "# Check for constant columns\n",
    "constant_columns = [col for col in df.columns if df[col].nunique() == 1]\n",
    "print(f\"Constant columns: {constant_columns}\")\n",
    "\n",
    "if constant_columns:\n",
    "    df = df.drop(columns=constant_columns)\n",
    "    print(f\"Dropped constant columns: {constant_columns}\")\n",
    "\n",
    "# Handle missing/infinite values\n",
    "print(\"\\n=== HANDLING MISSING/INFINITE VALUES ===\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "nan_counts = df.isnull().sum()\n",
    "if nan_counts.any():\n",
    "    nan_cols = nan_counts[nan_counts > 0].index.tolist()\n",
    "    print(f\"Columns with NaN: {nan_cols}\")\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Now process the labels\n",
    "print(\"\\n=== LABEL PROCESSING ===\")\n",
    "# Use Label.1 since it has the actual names\n",
    "df['Label_original'] = df['Label.1'].astype(str)\n",
    "\n",
    "# Clean the labels\n",
    "df['Label_cleaned'] = df['Label_original'].str.lower().str.strip()\n",
    "\n",
    "# Check cleaned labels\n",
    "print(\"\\nCleaned label distribution:\")\n",
    "cleaned_counts = df['Label_cleaned'].value_counts()\n",
    "for label, count in cleaned_counts.items():\n",
    "    proportion = count / len(df) * 100\n",
    "    print(f\"  '{label}': {count} samples ({proportion:.2f}%)\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['Label_encoded'] = label_encoder.fit_transform(df['Label_cleaned'])\n",
    "\n",
    "print(\"\\n=== FINAL LABEL ENCODING ===\")\n",
    "print(\"Class mapping:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    count = (df['Label_cleaned'] == label).sum()\n",
    "    proportion = count / len(df) * 100\n",
    "    print(f\"  {i}: '{label}' - {count} samples ({proportion:.2f}%)\")\n",
    "\n",
    "label_mapping = dict(zip(range(len(label_encoder.classes_)), label_encoder.classes_))\n",
    "\n",
    "# Prepare features\n",
    "print(\"\\n=== FEATURE PREPARATION ===\")\n",
    "# Exclude all label-related columns\n",
    "label_cols = ['Label', 'Label.1', 'Label_original', 'Label_cleaned', 'Label_encoded']\n",
    "exclude_cols = [col for col in label_cols if col in df.columns]\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['Label_encoded']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "# Scale features\n",
    "print(\"\\n=== FEATURE SCALING ===\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "if len(numeric_features) > 0:\n",
    "    X_scaled[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "    print(f\"Scaled {len(numeric_features)} numeric features\")\n",
    "else:\n",
    "    print(\"No numeric features to scale\")\n",
    "\n",
    "# Split data\n",
    "print(\"\\n=== DATA SPLITTING ===\")\n",
    "class_counts = y.value_counts()\n",
    "print(\"Class distribution:\")\n",
    "for class_id, count in class_counts.items():\n",
    "    class_name = label_encoder.inverse_transform([class_id])[0]\n",
    "    proportion = count / len(y) * 100\n",
    "    print(f\"  {class_id}: '{class_name}' - {count} samples ({proportion:.2f}%)\")\n",
    "\n",
    "# Use stratification if possible\n",
    "if class_counts.min() >= 2:\n",
    "    print(\"\\nUsing stratified split\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nUsing random split (some classes have < 2 samples)\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Save processed data\n",
    "print(\"\\n=== SAVING DATA ===\")\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "preprocessed_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'scaler': scaler,\n",
    "    'label_encoder': label_encoder,\n",
    "    'feature_names': X_train.columns.tolist(),\n",
    "    'label_mapping': label_mapping,\n",
    "    'num_classes': len(label_encoder.classes_)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Quick Summary \n",
    "print(\"\\nData preprocessing completed successfully.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea64c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_encrypted'] = df['Label'].apply(lambda x: 1 if x in ['Tor','VPN'] else 0)\n",
    "df['is_encrypted'] = df['is_encrypted'].astype(int)\n",
    "df['is_encrypted'].value_counts()\n",
    "X = df.drop(columns=['Label', 'is_encrypted'])\n",
    "y_multiclass = df['Label']         # for multiclass classification\n",
    "y_binary = df['is_encrypted']      # for encrypted vs non-encrypted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "091c982f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTE:\n",
      "  Class 0: 8987 samples\n",
      "  Class 1: 23732 samples\n",
      "  Class 2: 8321 samples\n",
      "  Class 3: 4405 samples\n",
      "  Class 4: 8603 samples\n",
      "  Class 5: 18782 samples\n",
      "  Class 6: 7206 samples\n",
      "  Class 7: 2460 samples\n",
      "\n",
      "Class distribution after SMOTE:\n",
      "  Class 0: 23732 samples\n",
      "  Class 1: 23732 samples\n",
      "  Class 2: 23732 samples\n",
      "  Class 3: 23732 samples\n",
      "  Class 4: 23732 samples\n",
      "  Class 5: 23732 samples\n",
      "  Class 6: 23732 samples\n",
      "  Class 7: 23732 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df['Label_encoded'] exists\n",
    "X = X_scaled  # scaled numeric features\n",
    "y = df['Label_encoded'].values  # integer-encoded labels\n",
    "\n",
    "# Split train/test (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "print(\"Class distribution before SMOTE:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  Class {u}: {c} samples\")\n",
    "\n",
    "# Apply SMOTE to training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "unique_res, counts_res = np.unique(y_train_res, return_counts=True)\n",
    "for u, c in zip(unique_res, counts_res):\n",
    "    print(f\"  Class {u}: {c} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5c9669b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTE (binary):\n",
      "  Non-encrypted: 85020 samples\n",
      "  Encrypted: 18101 samples\n",
      "\n",
      "Class distribution after SMOTE (binary):\n",
      "  Non-encrypted: 68015 samples\n",
      "  Encrypted: 68015 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Binary target\n",
    "y_binary = df['is_encrypted'].values  # 0 = non-encrypted, 1 = Tor/VPN\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_binary, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_binary\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Class distribution before SMOTE (binary):\")\n",
    "unique, counts = np.unique(y_binary, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    label_name = \"Encrypted\" if u == 1 else \"Non-encrypted\"\n",
    "    print(f\"  {label_name}: {c} samples\")\n",
    "\n",
    "# Apply SMOTE to training set\n",
    "smote_bin = SMOTE(random_state=42)\n",
    "X_train_bin_res, y_train_bin_res = smote_bin.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE (binary):\")\n",
    "unique_res, counts_res = np.unique(y_train_bin_res, return_counts=True)\n",
    "for u, c in zip(unique_res, counts_res):\n",
    "    label_name = \"Encrypted\" if u == 1 else \"Non-encrypted\"\n",
    "    print(f\"  {label_name}: {c} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f27b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train IF on all training data (or only normal traffic if you have labels)\n",
    "if_model = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
    "if_model.fit(X_train)\n",
    "\n",
    "# Get anomaly scores (more negative = more anomalous)\n",
    "if_scores_train = if_model.decision_function(X_train)\n",
    "if_scores_test = if_model.decision_function(X_test)\n",
    "\n",
    "# Add IF score as an additional feature\n",
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0407832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train encrypted count: 14481\n",
      "Test encrypted count: 3620\n",
      "Unique values in y_binary: [0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_binary, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_binary\n",
    ")\n",
    "\n",
    "print(\"Train encrypted count:\", sum(y_train))\n",
    "print(\"Test encrypted count:\", sum(y_test))\n",
    "\n",
    "print(\"Unique values in y_binary:\", np.unique(y_binary))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10c83f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ISOLATION FOREST ===\n",
      "Accuracy: 0.8000484848484849\n",
      "ROC-AUC: 0.6621479220443776\n",
      "Precision: 0.28716216216216217\n",
      "Recall: 0.09392265193370165\n",
      "F1-score: 0.14154870940882597\n",
      "Confusion Matrix:\n",
      " [[16161   844]\n",
      " [ 3280   340]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Train IF only on normal samples (y_train == 0)\n",
    "if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "if_model.fit(X_train[y_train == 0])\n",
    "\n",
    "# anomaly scores (more negative = more anomalous)\n",
    "if_scores_train = -if_model.decision_function(X_train)\n",
    "if_scores_test = -if_model.decision_function(X_test)\n",
    "\n",
    "# pick threshold = 95th percentile of normal scores\n",
    "threshold_if = np.percentile(if_scores_train[y_train == 0], 95)\n",
    "if_pred = (if_scores_test > threshold_if).astype(int)\n",
    "\n",
    "print(\"\\n=== ISOLATION FOREST ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, if_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, if_scores_test))\n",
    "print(\"Precision:\", precision_score(y_test, if_pred))\n",
    "print(\"Recall:\", recall_score(y_test, if_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, if_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, if_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7647701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.4918 - val_loss: 0.3173\n",
      "Epoch 2/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.2482 - val_loss: 0.2205\n",
      "Epoch 3/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1868 - val_loss: 0.1879\n",
      "Epoch 4/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1603 - val_loss: 0.1906\n",
      "Epoch 5/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1496 - val_loss: 0.1402\n",
      "Epoch 6/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1317 - val_loss: 0.1262\n",
      "Epoch 7/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1138 - val_loss: 0.1291\n",
      "Epoch 8/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1071 - val_loss: 0.1154\n",
      "Epoch 9/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0991 - val_loss: 0.1233\n",
      "Epoch 10/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0997 - val_loss: 0.1292\n",
      "Epoch 11/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0994 - val_loss: 0.1173\n",
      "Epoch 12/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0907 - val_loss: 0.0929\n",
      "Epoch 13/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0726 - val_loss: 0.0902\n",
      "Epoch 14/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0666 - val_loss: 0.0841\n",
      "Epoch 15/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0624 - val_loss: 0.0829\n",
      "Epoch 16/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0598 - val_loss: 0.0826\n",
      "Epoch 17/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0580 - val_loss: 0.0837\n",
      "Epoch 18/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0668 - val_loss: 0.0738\n",
      "Epoch 19/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0603 - val_loss: 0.0764\n",
      "Epoch 20/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0617 - val_loss: 0.0735\n",
      "Epoch 21/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0575 - val_loss: 0.0951\n",
      "Epoch 22/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0552 - val_loss: 0.0952\n",
      "Epoch 23/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0515 - val_loss: 0.0730\n",
      "Epoch 24/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0502 - val_loss: 0.0696\n",
      "Epoch 25/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0497 - val_loss: 0.0653\n",
      "Epoch 26/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0510 - val_loss: 0.0726\n",
      "Epoch 27/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0457 - val_loss: 0.0594\n",
      "Epoch 28/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0426 - val_loss: 0.1004\n",
      "Epoch 29/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0698 - val_loss: 0.0704\n",
      "Epoch 30/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0440 - val_loss: 0.0708\n",
      "Epoch 31/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0416 - val_loss: 0.0676\n",
      "Epoch 32/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0423 - val_loss: 0.0737\n",
      "Epoch 33/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0393 - val_loss: 0.0619\n",
      "Epoch 34/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0383 - val_loss: 0.0656\n",
      "Epoch 35/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0377 - val_loss: 0.0584\n",
      "Epoch 36/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0512 - val_loss: 0.0572\n",
      "Epoch 37/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0396 - val_loss: 0.0961\n",
      "Epoch 38/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0383 - val_loss: 0.0626\n",
      "Epoch 39/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0391 - val_loss: 0.0530\n",
      "Epoch 40/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0371 - val_loss: 0.0581\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step  \n"
     ]
    }
   ],
   "source": [
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "from tensorflow.keras import layers, models, Model\n",
    "\n",
    "input_dim = X_train_if.shape[1]\n",
    "reconstruction_error = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "x = layers.Dense(64, activation='relu')(input_layer)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "\n",
    "x = layers.Dense(32, activation='relu')(latent)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "output = layers.Dense(input_dim, activation='linear')(x)\n",
    "\n",
    "autoencoder = Model(input_layer, output)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "autoencoder.fit(\n",
    "    X_train_if[y_train == 0], X_train_if[y_train == 0],\n",
    "    epochs=40,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "# 4️⃣ Predict reconstruction for test set\n",
    "X_test_pred = autoencoder.predict(X_test_if)\n",
    "\n",
    "# 5️⃣ Compute reconstruction error\n",
    "reconstruction_error = np.mean(np.square(X_test_if - X_test_pred), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52760e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583us/step\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 639us/step\n",
      "\n",
      "=== AUTOENCODER ===\n",
      "Accuracy: 0.8111030303030303\n",
      "ROC-AUC: 0.5614007579831086\n",
      "Precision: 0.3968609865470852\n",
      "Recall: 0.14668508287292817\n",
      "F1-score: 0.21419927390076643\n",
      "Confusion Matrix:\n",
      " [[16198   807]\n",
      " [ 3089   531]]\n"
     ]
    }
   ],
   "source": [
    "X_test_pred = autoencoder.predict(X_test_if)\n",
    "test_rec_err = np.mean((X_test_if - X_test_pred)**2, axis=1)\n",
    "\n",
    "X_train_pred = autoencoder.predict(X_train_if[y_train == 0])\n",
    "train_rec_err = np.mean((X_train_if[y_train == 0] - X_train_pred)**2, axis=1)\n",
    "\n",
    "threshold_ae = np.percentile(train_rec_err, 95)\n",
    "ae_pred = (test_rec_err > threshold_ae).astype(int)\n",
    "print(\"\\n=== AUTOENCODER ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, ae_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, test_rec_err))\n",
    "print(\"Precision:\", precision_score(y_test, ae_pred))\n",
    "print(\"Recall:\", recall_score(y_test, ae_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, ae_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, ae_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb04ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Deep SVDD implementation in PyTorch\n",
    "# --- Encoder network (small MLP) ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Deep SVDD trainer ---\n",
    "class DeepSVDD:\n",
    "    def __init__(self, input_dim, c=None, nu=0.1, lr=1e-3):\n",
    "        self.encoder = Encoder(input_dim).to(device)\n",
    "        self.c = c  # center (torch tensor) or None -> init from data\n",
    "        self.nu = nu\n",
    "        self.optimizer = optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "        self.criterion = lambda z, c: ((z - c)**2).sum(dim=1)  # per-sample squared dist\n",
    "\n",
    "    def init_center_c(self, loader):\n",
    "        # initialize center as mean of encoder outputs on normal data\n",
    "        self.encoder.eval()\n",
    "        n = 0\n",
    "        c_sum = None\n",
    "        with torch.no_grad():\n",
    "            for x in loader:\n",
    "                x = x[0].to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                if c_sum is None:\n",
    "                    c_sum = z.sum(dim=0)\n",
    "                else:\n",
    "                    c_sum += z.sum(dim=0)\n",
    "                n += z.size(0)\n",
    "        c = c_sum / n\n",
    "        # avoid components too close to zero\n",
    "        c[(abs(c) < 1e-6)] = 1e-6\n",
    "        self.c = c\n",
    "\n",
    "    def train(self, loader, epochs=50):\n",
    "        if self.c is None:\n",
    "            self.init_center_c(loader)\n",
    "        self.encoder.train()\n",
    "        for ep in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for x, in loader:\n",
    "                x = x.to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                dist = self.criterion(z, self.c)\n",
    "                loss = dist.mean()  # minimize avg distance of normal samples\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item() * x.size(0)\n",
    "            # print progress\n",
    "            if ep % 5 == 0:\n",
    "                print(f\"DeepSVDD epoch {ep}/{epochs} loss: {epoch_loss/len(loader.dataset):.6f}\")\n",
    "\n",
    "    def score(self, X):  # X: numpy array\n",
    "        self.encoder.eval()\n",
    "        ds = TensorDataset(torch.from_numpy(X).float())\n",
    "        loader = DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                x = batch[0].to(device)\n",
    "                z = self.encoder(x)\n",
    "                dist = ((z - self.c) ** 2).sum(dim=1)  # per-sample\n",
    "                scores.append(dist.cpu().numpy())\n",
    "        return np.concatenate(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e819807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSVDD epoch 0/50 loss: 0.023820\n",
      "DeepSVDD epoch 5/50 loss: 0.000158\n",
      "DeepSVDD epoch 10/50 loss: 0.000060\n",
      "DeepSVDD epoch 15/50 loss: 0.000040\n",
      "DeepSVDD epoch 20/50 loss: 0.000021\n",
      "DeepSVDD epoch 25/50 loss: 0.000015\n",
      "DeepSVDD epoch 30/50 loss: 0.000012\n",
      "DeepSVDD epoch 35/50 loss: 0.000010\n",
      "DeepSVDD epoch 40/50 loss: 0.000006\n",
      "DeepSVDD epoch 45/50 loss: 0.000006\n"
     ]
    }
   ],
   "source": [
    "# Train DeepSVDD on *normal* training samples only (y_train==0)\n",
    "# Convert DataFrame → NumPy → Tensor\n",
    "X_train_normal_np = X_train[y_train == 0].to_numpy().astype(\"float32\")\n",
    "X_test_np = X_test.to_numpy().astype(\"float32\")\n",
    "\n",
    "\n",
    "ds = TensorDataset(torch.from_numpy(X_train_normal_np).float())\n",
    "loader = DataLoader(ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "svdd = DeepSVDD(input_dim=X_train.shape[1])\n",
    "svdd.init_center_c(DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024))\n",
    "svdd.train(loader, epochs=50)\n",
    "\n",
    "# get scores on test set (higher = more anomalous)\n",
    "# Convert X_test and X_train to NumPy arrays\n",
    "X_test_np = X_test.to_numpy().astype(\"float32\")\n",
    "X_train_np = X_train.to_numpy().astype(\"float32\")\n",
    "\n",
    "# Then score\n",
    "svdd_scores_test = svdd.score(X_test_np)\n",
    "svdd_scores_train = svdd.score(X_train_np)  # optional for thresholding\n",
    " # optional for thresholding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26629e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DeepSVDD ===\n",
      "Accuracy: 0.8127\n",
      "ROC-AUC: 0.6597\n",
      "Precision: 0.4209\n",
      "Recall: 0.1793\n",
      "F1-score: 0.2515\n",
      "Confusion Matrix:\n",
      "[[16112   893]\n",
      " [ 2971   649]]\n"
     ]
    }
   ],
   "source": [
    "# Threshold for binary prediction\n",
    "# Usually you can set it based on 95th percentile of training normal scores\n",
    "threshold = np.percentile(svdd_scores_train[y_train == 0], 95)\n",
    "svdd_pred = (svdd_scores_test > threshold).astype(int)\n",
    "\n",
    "# Evaluation metrics\n",
    "y_test_binary = (y_test != 0).astype(int)  # treat all non-normal as anomaly\n",
    "print(\"=== DeepSVDD ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_binary, svdd_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test_binary, svdd_scores_test):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_binary, svdd_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_binary, svdd_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test_binary, svdd_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_binary, svdd_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "505ea6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ensemble (Majority Vote) ===\n",
      "Accuracy: 0.8151\n",
      "Precision: 0.4130\n",
      "Recall: 0.1265\n",
      "F1-score: 0.1937\n",
      "Confusion Matrix:\n",
      "[[16354   651]\n",
      " [ 3162   458]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Suppose you have binary predictions from each model:\n",
    "pred_if = if_pred\n",
    "pred_ae = ae_pred\n",
    "pred_svdd = svdd_pred\n",
    "\n",
    "# Stack predictions\n",
    "pred_stack = np.vstack([pred_if, pred_ae, pred_svdd]).T\n",
    "\n",
    "# Majority vote\n",
    "ensemble_pred = mode(pred_stack, axis=1).mode.flatten()\n",
    "\n",
    "# Evaluate\n",
    "print(\"=== Ensemble (Majority Vote) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_binary, ensemble_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_binary, ensemble_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_binary, ensemble_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test_binary, ensemble_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_binary, ensemble_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b0e0251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stacking Ensemble (Random Forest) ===\n",
      "Accuracy: 0.9991\n",
      "Precision: 0.9992\n",
      "Recall: 0.9956\n",
      "F1-score: 0.9974\n",
      "Confusion Matrix:\n",
      "[[17002     3]\n",
      " [   16  3604]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Use anomaly scores from base models as features\n",
    "X_meta_train = np.vstack([if_scores_test, reconstruction_error, svdd_scores_test]).T\n",
    "y_meta_train = y_test_binary  # binary labels\n",
    "\n",
    "meta_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "meta_pred = meta_model.predict(X_meta_train)\n",
    "\n",
    "print(\"=== Stacking Ensemble (Random Forest) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_meta_train, meta_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_meta_train, meta_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_meta_train, meta_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_meta_train, meta_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_meta_train, meta_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b346d66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSVDD epoch 0/50 loss: 0.018432\n",
      "DeepSVDD epoch 5/50 loss: 0.000139\n",
      "DeepSVDD epoch 10/50 loss: 0.000055\n",
      "DeepSVDD epoch 15/50 loss: 0.000032\n",
      "DeepSVDD epoch 20/50 loss: 0.000021\n",
      "DeepSVDD epoch 25/50 loss: 0.000015\n",
      "DeepSVDD epoch 30/50 loss: 0.000011\n",
      "DeepSVDD epoch 35/50 loss: 0.000009\n",
      "DeepSVDD epoch 40/50 loss: 0.000006\n",
      "DeepSVDD epoch 45/50 loss: 0.000004\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 538us/step\n",
      "=== Stacking Ensemble (Random Forest) ===\n",
      "Accuracy: 0.9452\n",
      "Precision: 0.8687\n",
      "Recall: 0.8099\n",
      "F1-score: 0.8383\n",
      "Confusion Matrix:\n",
      " [[16562   443]\n",
      " [  688  2932]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# --- Step 1: Ensure DeepSVDD uses correct input dimension ---\n",
    "input_dim = X_train_if.shape[1]  # include IF score if added\n",
    "svdd = DeepSVDD(input_dim=input_dim)\n",
    "X_train_normal_np = X_train_if[y_train == 0].astype(\"float32\")\n",
    "loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "svdd.init_center_c(loader)\n",
    "svdd.train(loader, epochs=50)\n",
    "\n",
    "# --- Step 2: Compute base model scores on test set ---\n",
    "# IF scores\n",
    "if_scores_test = -if_model.decision_function(X_test_if)\n",
    "\n",
    "# Autoencoder reconstruction error\n",
    "X_test_pred = autoencoder.predict(X_test_if)\n",
    "reconstruction_error = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "# DeepSVDD scores\n",
    "X_test_np = X_test_if.astype(\"float32\")\n",
    "svdd_scores_test = svdd.score(X_test_np)\n",
    "\n",
    "# --- Step 3: Stack as meta-features ---\n",
    "X_meta_test = np.vstack([if_scores_test, reconstruction_error, svdd_scores_test]).T\n",
    "y_meta_test = y_test  # binary labels\n",
    "\n",
    "# --- Step 4: Train meta-classifier ---\n",
    "meta_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# For proper training, compute meta-features on the training set as well:\n",
    "if_scores_train = -if_model.decision_function(X_train_if)\n",
    "X_train_pred = autoencoder.predict(X_train_if)\n",
    "reconstruction_error_train = np.mean(np.square(X_train_if - X_train_pred), axis=1)\n",
    "X_train_np = X_train_if.astype(\"float32\")\n",
    "svdd_scores_train = svdd.score(X_train_np)\n",
    "\n",
    "X_meta_train = np.vstack([if_scores_train, reconstruction_error_train, svdd_scores_train]).T\n",
    "y_meta_train = y_train\n",
    "\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# --- Step 5: Evaluate stacking ensemble ---\n",
    "meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "print(\"=== Stacking Ensemble (Random Forest) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, meta_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6477f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta_train = np.vstack([if_scores_train, reconstruction_error, svdd_scores_train]).T\n",
    "X_meta_test = np.vstack([if_scores_test, reconstruction_error_test, svdd_scores_test]).T\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "meta_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "meta_model.fit(X_meta_train, y_train_binary)\n",
    "meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "print(\"=== Stacking Ensemble (XGBoost) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_binary, meta_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_binary, meta_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_binary, meta_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test_binary, meta_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_binary, meta_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
