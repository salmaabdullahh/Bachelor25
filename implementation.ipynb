{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "131a7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                           f1_score, roc_auc_score, average_precision_score,\n",
    "                           classification_report, confusion_matrix)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b155f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0f10007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2150cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103121 entries, 0 to 103120\n",
      "Data columns (total 79 columns):\n",
      " #   Column                      Non-Null Count   Dtype   \n",
      "---  ------                      --------------   -----   \n",
      " 0   Protocol                    103121 non-null  int8    \n",
      " 1   Flow Duration               103121 non-null  int32   \n",
      " 2   Total Fwd Packet            103121 non-null  int32   \n",
      " 3   Total Bwd packets           103121 non-null  int32   \n",
      " 4   Total Length of Fwd Packet  103121 non-null  int32   \n",
      " 5   Total Length of Bwd Packet  103121 non-null  int32   \n",
      " 6   Fwd Packet Length Max       103121 non-null  int32   \n",
      " 7   Fwd Packet Length Min       103121 non-null  int16   \n",
      " 8   Fwd Packet Length Mean      103121 non-null  float32 \n",
      " 9   Fwd Packet Length Std       103121 non-null  float32 \n",
      " 10  Bwd Packet Length Max       103121 non-null  int32   \n",
      " 11  Bwd Packet Length Min       103121 non-null  int16   \n",
      " 12  Bwd Packet Length Mean      103121 non-null  float32 \n",
      " 13  Bwd Packet Length Std       103121 non-null  float32 \n",
      " 14  Flow Bytes/s                103121 non-null  float64 \n",
      " 15  Flow Packets/s              103121 non-null  float64 \n",
      " 16  Flow IAT Mean               103121 non-null  float32 \n",
      " 17  Flow IAT Std                103121 non-null  float32 \n",
      " 18  Flow IAT Max                103121 non-null  int32   \n",
      " 19  Flow IAT Min                103121 non-null  int32   \n",
      " 20  Fwd IAT Total               103121 non-null  int32   \n",
      " 21  Fwd IAT Mean                103121 non-null  float32 \n",
      " 22  Fwd IAT Std                 103121 non-null  float32 \n",
      " 23  Fwd IAT Max                 103121 non-null  int32   \n",
      " 24  Fwd IAT Min                 103121 non-null  int32   \n",
      " 25  Bwd IAT Total               103121 non-null  int32   \n",
      " 26  Bwd IAT Mean                103121 non-null  float32 \n",
      " 27  Bwd IAT Std                 103121 non-null  float32 \n",
      " 28  Bwd IAT Max                 103121 non-null  int32   \n",
      " 29  Bwd IAT Min                 103121 non-null  int32   \n",
      " 30  Fwd PSH Flags               103121 non-null  int8    \n",
      " 31  Bwd PSH Flags               103121 non-null  int8    \n",
      " 32  Fwd URG Flags               103121 non-null  int8    \n",
      " 33  Bwd URG Flags               103121 non-null  int8    \n",
      " 34  Fwd Header Length           103121 non-null  int32   \n",
      " 35  Bwd Header Length           103121 non-null  int32   \n",
      " 36  Fwd Packets/s               103121 non-null  float32 \n",
      " 37  Bwd Packets/s               103121 non-null  float32 \n",
      " 38  Packet Length Min           103121 non-null  int16   \n",
      " 39  Packet Length Max           103121 non-null  int32   \n",
      " 40  Packet Length Mean          103121 non-null  float32 \n",
      " 41  Packet Length Std           103121 non-null  float32 \n",
      " 42  Packet Length Variance      103121 non-null  float32 \n",
      " 43  FIN Flag Count              103121 non-null  int8    \n",
      " 44  SYN Flag Count              103121 non-null  int8    \n",
      " 45  RST Flag Count              103121 non-null  int8    \n",
      " 46  PSH Flag Count              103121 non-null  int32   \n",
      " 47  ACK Flag Count              103121 non-null  int32   \n",
      " 48  URG Flag Count              103121 non-null  int8    \n",
      " 49  CWE Flag Count              103121 non-null  int8    \n",
      " 50  ECE Flag Count              103121 non-null  int8    \n",
      " 51  Down/Up Ratio               103121 non-null  int16   \n",
      " 52  Avg Packet Size             103121 non-null  float32 \n",
      " 53  Fwd Segment Size Avg        103121 non-null  float32 \n",
      " 54  Bwd Segment Size Avg        103121 non-null  float32 \n",
      " 55  Fwd Bytes/Bulk Avg          103121 non-null  int8    \n",
      " 56  Fwd Packet/Bulk Avg         103121 non-null  int8    \n",
      " 57  Fwd Bulk Rate Avg           103121 non-null  int8    \n",
      " 58  Bwd Bytes/Bulk Avg          103121 non-null  int8    \n",
      " 59  Bwd Packet/Bulk Avg         103121 non-null  int32   \n",
      " 60  Bwd Bulk Rate Avg           103121 non-null  int32   \n",
      " 61  Subflow Fwd Packets         103121 non-null  int8    \n",
      " 62  Subflow Fwd Bytes           103121 non-null  int16   \n",
      " 63  Subflow Bwd Packets         103121 non-null  int8    \n",
      " 64  Subflow Bwd Bytes           103121 non-null  int16   \n",
      " 65  FWD Init Win Bytes          103121 non-null  int32   \n",
      " 66  Bwd Init Win Bytes          103121 non-null  int32   \n",
      " 67  Fwd Act Data Packets        103121 non-null  int32   \n",
      " 68  Fwd Seg Size Min            103121 non-null  int8    \n",
      " 69  Active Mean                 103121 non-null  int8    \n",
      " 70  Active Std                  103121 non-null  int8    \n",
      " 71  Active Max                  103121 non-null  int8    \n",
      " 72  Active Min                  103121 non-null  int8    \n",
      " 73  Idle Mean                   103121 non-null  float32 \n",
      " 74  Idle Std                    103121 non-null  float32 \n",
      " 75  Idle Max                    103121 non-null  float32 \n",
      " 76  Idle Min                    103121 non-null  float32 \n",
      " 77  Label                       103121 non-null  category\n",
      " 78  Label.1                     103121 non-null  category\n",
      "dtypes: category(2), float32(22), float64(2), int16(6), int32(25), int8(22)\n",
      "memory usage: 23.6 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Label\n",
       "Non-Tor    64804\n",
       "NonVPN     20216\n",
       "VPN        16922\n",
       "Tor         1179\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"cicdarknet2020.parquet\", engine=\"fastparquet\")\n",
    "df.info()\n",
    "df['Label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c1ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INITIAL DATA INSPECTION ===\n",
      "DataFrame shape: (103121, 79)\n",
      "Memory usage: 23.60 MB\n",
      "\n",
      "=== DATA TYPES DETAILED ===\n",
      "Label column info:\n",
      "  dtype: category\n",
      "  type of dtype: <class 'pandas.core.dtypes.dtypes.CategoricalDtype'>\n",
      "  is categorical? True\n",
      "  is string? True\n",
      "\n",
      "Label.1 column info:\n",
      "  dtype: category\n",
      "  type of dtype: <class 'pandas.core.dtypes.dtypes.CategoricalDtype'>\n",
      "  is categorical? True\n",
      "  is string? True\n",
      "\n",
      "=== DECIDING WHICH LABEL TO USE ===\n",
      "Based on your output:\n",
      "1. Label column: Has values 0, 1, 2, 3 (4 classes)\n",
      "2. Label.1 column: Has actual names like 'Browsing', 'P2P', etc.\n",
      "\n",
      "Checking if Label is already encoded...\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: NonVPN -> Label.1: Chat\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Tor -> Label.1: VOIP\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: NonVPN -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Email\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Email\n",
      "Label: VPN -> Label.1: VOIP\n",
      "Label: Non-Tor -> Label.1: Video-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: VOIP\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: VPN -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Video-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Video-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: NonVPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Email\n",
      "Label: NonVPN -> Label.1: Chat\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: File-Transfer\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: VPN -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Email\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: NonVPN -> Label.1: Email\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: NonVPN -> Label.1: Email\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: AUDIO-STREAMING\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: NonVPN -> Label.1: Video-Streaming\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: NonVPN -> Label.1: File-transfer\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: NonVPN -> Label.1: Video-Streaming\n",
      "Label: NonVPN -> Label.1: File-Transfer\n",
      "Label: VPN -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: AUDIO-STREAMING\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Video-Streaming\n",
      "\n",
      "=== CREATING PROPER LABEL MAPPING ===\n",
      "             most_common  num_unique  \\\n",
      "Label                                  \n",
      "Non-Tor  AUDIO-STREAMING           8   \n",
      "NonVPN              Chat           8   \n",
      "Tor      Audio-Streaming           8   \n",
      "VPN        File-Transfer           6   \n",
      "\n",
      "                                             sample_values  \n",
      "Label                                                       \n",
      "Non-Tor  [AUDIO-STREAMING, Browsing, Chat, Email, File-...  \n",
      "NonVPN   [Chat, Audio-Streaming, Email, File-Transfer, ...  \n",
      "Tor      [Audio-Streaming, Browsing, Chat, File-Transfe...  \n",
      "VPN      [File-Transfer, Chat, Audio-Streaming, Email, ...  \n",
      "\n",
      "Based on analysis, I recommend using Label.1 as it has the actual class names\n",
      "\n",
      "=== DATA CLEANING ===\n",
      "Checking for duplicate columns...\n",
      "Duplicate columns: []\n",
      "Constant columns: ['Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Bwd Bytes/Bulk Avg', 'Subflow Bwd Packets', 'Active Mean', 'Active Std', 'Active Max', 'Active Min']\n",
      "Dropped constant columns: ['Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Bwd Bytes/Bulk Avg', 'Subflow Bwd Packets', 'Active Mean', 'Active Std', 'Active Max', 'Active Min']\n",
      "\n",
      "=== HANDLING MISSING/INFINITE VALUES ===\n",
      "\n",
      "=== LABEL PROCESSING ===\n",
      "\n",
      "Cleaned label distribution:\n",
      "  'browsing': 29862 samples (28.96%)\n",
      "  'p2p': 23404 samples (22.70%)\n",
      "  'audio-streaming': 11328 samples (10.99%)\n",
      "  'file-transfer': 10647 samples (10.32%)\n",
      "  'chat': 10365 samples (10.05%)\n",
      "  'video-streaming': 9012 samples (8.74%)\n",
      "  'email': 5442 samples (5.28%)\n",
      "  'voip': 3061 samples (2.97%)\n",
      "\n",
      "=== FINAL LABEL ENCODING ===\n",
      "Class mapping:\n",
      "  0: 'audio-streaming' - 11328 samples (10.99%)\n",
      "  1: 'browsing' - 29862 samples (28.96%)\n",
      "  2: 'chat' - 10365 samples (10.05%)\n",
      "  3: 'email' - 5442 samples (5.28%)\n",
      "  4: 'file-transfer' - 10647 samples (10.32%)\n",
      "  5: 'p2p' - 23404 samples (22.70%)\n",
      "  6: 'video-streaming' - 9012 samples (8.74%)\n",
      "  7: 'voip' - 3061 samples (2.97%)\n",
      "\n",
      "=== FEATURE PREPARATION ===\n",
      "Features shape: (103121, 62)\n",
      "Labels shape: (103121,)\n",
      "\n",
      "=== FEATURE SCALING ===\n",
      "Scaled 62 numeric features\n",
      "\n",
      "=== DATA SPLITTING ===\n",
      "Class distribution:\n",
      "  1: 'browsing' - 29862 samples (28.96%)\n",
      "  5: 'p2p' - 23404 samples (22.70%)\n",
      "  0: 'audio-streaming' - 11328 samples (10.99%)\n",
      "  4: 'file-transfer' - 10647 samples (10.32%)\n",
      "  2: 'chat' - 10365 samples (10.05%)\n",
      "  6: 'video-streaming' - 9012 samples (8.74%)\n",
      "  3: 'email' - 5442 samples (5.28%)\n",
      "  7: 'voip' - 3061 samples (2.97%)\n",
      "\n",
      "Using stratified split\n",
      "\n",
      "Training set: 82496 samples\n",
      "Testing set: 20625 samples\n",
      "\n",
      "=== SAVING DATA ===\n",
      "Saved to: cicdarknet_preprocessed_20251210_164345.pkl\n",
      "\n",
      "=== SUMMARY ===\n",
      "Original data: (103121, 67)\n",
      "Features: 62\n",
      "Classes: 8\n",
      "Training samples: 82496\n",
      "Test samples: 20625\n",
      "\n",
      "Data preprocessing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(\"=== INITIAL DATA INSPECTION ===\")\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage().sum() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Let's check the actual dtypes more carefully\n",
    "print(\"\\n=== DATA TYPES DETAILED ===\")\n",
    "print(\"Label column info:\")\n",
    "print(f\"  dtype: {df['Label'].dtype}\")\n",
    "print(f\"  type of dtype: {type(df['Label'].dtype)}\")\n",
    "print(f\"  is categorical? {pd.api.types.is_categorical_dtype(df['Label'])}\")\n",
    "print(f\"  is string? {pd.api.types.is_string_dtype(df['Label'])}\")\n",
    "\n",
    "print(\"\\nLabel.1 column info:\")\n",
    "print(f\"  dtype: {df['Label.1'].dtype}\")\n",
    "print(f\"  type of dtype: {type(df['Label.1'].dtype)}\")\n",
    "print(f\"  is categorical? {pd.api.types.is_categorical_dtype(df['Label.1'])}\")\n",
    "print(f\"  is string? {pd.api.types.is_string_dtype(df['Label.1'])}\")\n",
    "\n",
    "# DECISION TIME: Which label to use?\n",
    "print(\"\\n=== DECIDING WHICH LABEL TO USE ===\")\n",
    "print(\"Based on your output:\")\n",
    "print(\"1. Label column: Has values 0, 1, 2, 3 (4 classes)\")\n",
    "print(\"2. Label.1 column: Has actual names like 'Browsing', 'P2P', etc.\")\n",
    "\n",
    "# Let me check if Label might be encoded already\n",
    "print(\"\\nChecking if Label is already encoded...\")\n",
    "# Get a mapping by sampling\n",
    "sample_size = min(100, len(df))\n",
    "sample = df[['Label', 'Label.1']].sample(sample_size)\n",
    "for _, row in sample.iterrows():\n",
    "    print(f\"Label: {row['Label']} -> Label.1: {row['Label.1']}\")\n",
    "\n",
    "\n",
    "\n",
    "# Let's create a proper mapping\n",
    "print(\"\\n=== CREATING PROPER LABEL MAPPING ===\")\n",
    "# Group by Label and see what Label.1 values correspond\n",
    "label_mapping_df = df.groupby('Label')['Label.1'].agg(['first', 'nunique', lambda x: list(x.unique())[:5]])\n",
    "label_mapping_df.columns = ['most_common', 'num_unique', 'sample_values']\n",
    "print(label_mapping_df)\n",
    "\n",
    "# If Label is already encoded and Label.1 has the names, use Label.1\n",
    "print(\"\\nBased on analysis, I recommend using Label.1 as it has the actual class names\")\n",
    "\n",
    "# Clean up the data\n",
    "print(\"\\n=== DATA CLEANING ===\")\n",
    "print(\"Checking for duplicate columns...\")\n",
    "duplicate_columns = df.columns[df.columns.duplicated()]\n",
    "print(f\"Duplicate columns: {list(duplicate_columns)}\")\n",
    "\n",
    "# Check for constant columns\n",
    "constant_columns = [col for col in df.columns if df[col].nunique() == 1]\n",
    "print(f\"Constant columns: {constant_columns}\")\n",
    "\n",
    "if constant_columns:\n",
    "    df = df.drop(columns=constant_columns)\n",
    "    print(f\"Dropped constant columns: {constant_columns}\")\n",
    "\n",
    "# Handle missing/infinite values\n",
    "print(\"\\n=== HANDLING MISSING/INFINITE VALUES ===\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "nan_counts = df.isnull().sum()\n",
    "if nan_counts.any():\n",
    "    nan_cols = nan_counts[nan_counts > 0].index.tolist()\n",
    "    print(f\"Columns with NaN: {nan_cols}\")\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Now process the labels\n",
    "print(\"\\n=== LABEL PROCESSING ===\")\n",
    "# Use Label.1 since it has the actual names\n",
    "df['Label_original'] = df['Label.1'].astype(str)\n",
    "\n",
    "# Clean the labels\n",
    "df['Label_cleaned'] = df['Label_original'].str.lower().str.strip()\n",
    "\n",
    "# Check cleaned labels\n",
    "print(\"\\nCleaned label distribution:\")\n",
    "cleaned_counts = df['Label_cleaned'].value_counts()\n",
    "for label, count in cleaned_counts.items():\n",
    "    proportion = count / len(df) * 100\n",
    "    print(f\"  '{label}': {count} samples ({proportion:.2f}%)\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['Label_encoded'] = label_encoder.fit_transform(df['Label_cleaned'])\n",
    "\n",
    "print(\"\\n=== FINAL LABEL ENCODING ===\")\n",
    "print(\"Class mapping:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    count = (df['Label_cleaned'] == label).sum()\n",
    "    proportion = count / len(df) * 100\n",
    "    print(f\"  {i}: '{label}' - {count} samples ({proportion:.2f}%)\")\n",
    "\n",
    "label_mapping = dict(zip(range(len(label_encoder.classes_)), label_encoder.classes_))\n",
    "\n",
    "# Prepare features\n",
    "print(\"\\n=== FEATURE PREPARATION ===\")\n",
    "# Exclude all label-related columns\n",
    "label_cols = ['Label', 'Label.1', 'Label_original', 'Label_cleaned', 'Label_encoded']\n",
    "exclude_cols = [col for col in label_cols if col in df.columns]\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['Label_encoded']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "# Scale features\n",
    "print(\"\\n=== FEATURE SCALING ===\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "if len(numeric_features) > 0:\n",
    "    X_scaled[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "    print(f\"Scaled {len(numeric_features)} numeric features\")\n",
    "else:\n",
    "    print(\"No numeric features to scale\")\n",
    "\n",
    "# Split data\n",
    "print(\"\\n=== DATA SPLITTING ===\")\n",
    "class_counts = y.value_counts()\n",
    "print(\"Class distribution:\")\n",
    "for class_id, count in class_counts.items():\n",
    "    class_name = label_encoder.inverse_transform([class_id])[0]\n",
    "    proportion = count / len(y) * 100\n",
    "    print(f\"  {class_id}: '{class_name}' - {count} samples ({proportion:.2f}%)\")\n",
    "\n",
    "# Use stratification if possible\n",
    "if class_counts.min() >= 2:\n",
    "    print(\"\\nUsing stratified split\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nUsing random split (some classes have < 2 samples)\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Save processed data\n",
    "print(\"\\n=== SAVING DATA ===\")\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "preprocessed_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'scaler': scaler,\n",
    "    'label_encoder': label_encoder,\n",
    "    'feature_names': X_train.columns.tolist(),\n",
    "    'label_mapping': label_mapping,\n",
    "    'num_classes': len(label_encoder.classes_)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Quick Summary \n",
    "print(\"\\nData preprocessing completed successfully.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea64c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_encrypted'] = df['Label'].apply(lambda x: 1 if x in ['Tor','VPN'] else 0)\n",
    "df['is_encrypted'] = df['is_encrypted'].astype(int)\n",
    "df['is_encrypted'].value_counts()\n",
    "X = df.drop(columns=['Label', 'is_encrypted'])\n",
    "y_multiclass = df['Label']         # for multiclass classification\n",
    "y_binary = df['is_encrypted']      # for encrypted vs non-encrypted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "091c982f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTE:\n",
      "  Class 0: 8987 samples\n",
      "  Class 1: 23732 samples\n",
      "  Class 2: 8321 samples\n",
      "  Class 3: 4405 samples\n",
      "  Class 4: 8603 samples\n",
      "  Class 5: 18782 samples\n",
      "  Class 6: 7206 samples\n",
      "  Class 7: 2460 samples\n",
      "\n",
      "Class distribution after SMOTE:\n",
      "  Class 0: 23732 samples\n",
      "  Class 1: 23732 samples\n",
      "  Class 2: 23732 samples\n",
      "  Class 3: 23732 samples\n",
      "  Class 4: 23732 samples\n",
      "  Class 5: 23732 samples\n",
      "  Class 6: 23732 samples\n",
      "  Class 7: 23732 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df['Label_encoded'] exists\n",
    "X = X_scaled  # scaled numeric features\n",
    "y = df['Label_encoded'].values  # integer-encoded labels\n",
    "\n",
    "# Split train/test (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "print(\"Class distribution before SMOTE:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  Class {u}: {c} samples\")\n",
    "\n",
    "# Apply SMOTE to training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "unique_res, counts_res = np.unique(y_train_res, return_counts=True)\n",
    "for u, c in zip(unique_res, counts_res):\n",
    "    print(f\"  Class {u}: {c} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5c9669b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTE (binary):\n",
      "  Non-encrypted: 85020 samples\n",
      "  Encrypted: 18101 samples\n",
      "\n",
      "Class distribution after SMOTE (binary):\n",
      "  Non-encrypted: 68015 samples\n",
      "  Encrypted: 68015 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Binary target\n",
    "y_binary = df['is_encrypted'].values  # 0 = non-encrypted, 1 = Tor/VPN\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_binary, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_binary\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Class distribution before SMOTE (binary):\")\n",
    "unique, counts = np.unique(y_binary, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    label_name = \"Encrypted\" if u == 1 else \"Non-encrypted\"\n",
    "    print(f\"  {label_name}: {c} samples\")\n",
    "\n",
    "# Apply SMOTE to training set\n",
    "smote_bin = SMOTE(random_state=42)\n",
    "X_train_bin_res, y_train_bin_res = smote_bin.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE (binary):\")\n",
    "unique_res, counts_res = np.unique(y_train_bin_res, return_counts=True)\n",
    "for u, c in zip(unique_res, counts_res):\n",
    "    label_name = \"Encrypted\" if u == 1 else \"Non-encrypted\"\n",
    "    print(f\"  {label_name}: {c} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4f27b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train IF on all training data (or only normal traffic if you have labels)\n",
    "if_model = IsolationForest(n_estimators=200, contamination=0.03, random_state=42)\n",
    "if_model.fit(X_train)\n",
    "\n",
    "# Get anomaly scores (more negative = more anomalous)\n",
    "if_scores_train = if_model.decision_function(X_train)\n",
    "if_scores_test = if_model.decision_function(X_test)\n",
    "\n",
    "# Add IF score as an additional feature\n",
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0407832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train encrypted count: 14481\n",
      "Test encrypted count: 3620\n",
      "Unique values in y_binary: [0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_binary, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_binary\n",
    ")\n",
    "\n",
    "print(\"Train encrypted count:\", sum(y_train))\n",
    "print(\"Test encrypted count:\", sum(y_test))\n",
    "\n",
    "print(\"Unique values in y_binary:\", np.unique(y_binary))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "10c83f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ISOLATION FOREST ===\n",
      "Accuracy: 0.8000484848484849\n",
      "ROC-AUC: 0.6621479220443776\n",
      "Precision: 0.28716216216216217\n",
      "Recall: 0.09392265193370165\n",
      "F1-score: 0.14154870940882597\n",
      "Confusion Matrix:\n",
      " [[16161   844]\n",
      " [ 3280   340]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Train IF only on normal samples (y_train == 0)\n",
    "if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "if_model.fit(X_train[y_train == 0])\n",
    "\n",
    "# anomaly scores (more negative = more anomalous)\n",
    "if_scores_train = -if_model.decision_function(X_train)\n",
    "if_scores_test = -if_model.decision_function(X_test)\n",
    "\n",
    "# pick threshold = 95th percentile of normal scores\n",
    "threshold_if = np.percentile(if_scores_train[y_train == 0], 95)\n",
    "if_pred = (if_scores_test > threshold_if).astype(int)\n",
    "\n",
    "print(\"\\n=== ISOLATION FOREST ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, if_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, if_scores_test))\n",
    "print(\"Precision:\", precision_score(y_test, if_pred))\n",
    "print(\"Recall:\", recall_score(y_test, if_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, if_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, if_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7647701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5313 - val_loss: 0.4068\n",
      "Epoch 2/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2600 - val_loss: 0.2274\n",
      "Epoch 3/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1893 - val_loss: 0.1830\n",
      "Epoch 4/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1575 - val_loss: 0.2465\n",
      "Epoch 5/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1372 - val_loss: 0.1539\n",
      "Epoch 6/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1186 - val_loss: 0.1726\n",
      "Epoch 7/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1102 - val_loss: 0.1333\n",
      "Epoch 8/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1135 - val_loss: 0.1205\n",
      "Epoch 9/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0946 - val_loss: 0.1224\n",
      "Epoch 10/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0781 - val_loss: 0.1410\n",
      "Epoch 11/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0867 - val_loss: 0.1082\n",
      "Epoch 12/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0795 - val_loss: 0.1026\n",
      "Epoch 13/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0726 - val_loss: 0.1269\n",
      "Epoch 14/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1256 - val_loss: 0.1011\n",
      "Epoch 15/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0735 - val_loss: 0.0911\n",
      "Epoch 16/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0627 - val_loss: 0.0950\n",
      "Epoch 17/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0596 - val_loss: 0.0818\n",
      "Epoch 18/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0562 - val_loss: 0.0732\n",
      "Epoch 19/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0532 - val_loss: 0.1056\n",
      "Epoch 20/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1054 - val_loss: 0.1054\n",
      "Epoch 21/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0628 - val_loss: 0.0919\n",
      "Epoch 22/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0534 - val_loss: 0.0779\n",
      "Epoch 23/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0504 - val_loss: 0.0730\n",
      "Epoch 24/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0509 - val_loss: 0.1047\n",
      "Epoch 25/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0525 - val_loss: 0.0710\n",
      "Epoch 26/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0460 - val_loss: 0.0715\n",
      "Epoch 27/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0446 - val_loss: 0.0734\n",
      "Epoch 28/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0411 - val_loss: 0.0653\n",
      "Epoch 29/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0394 - val_loss: 0.0673\n",
      "Epoch 30/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0439 - val_loss: 0.0686\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step\n"
     ]
    }
   ],
   "source": [
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "from tensorflow.keras import layers, models, Model\n",
    "\n",
    "input_dim = X_train_if.shape[1]\n",
    "reconstruction_error = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "x = layers.Dense(64, activation='relu')(input_layer)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "\n",
    "x = layers.Dense(32, activation='relu')(latent)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "output = layers.Dense(input_dim, activation='linear')(x)\n",
    "\n",
    "autoencoder = Model(input_layer, output)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "autoencoder.fit(\n",
    "    X_train_if[y_train == 0], X_train_if[y_train == 0],\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "# 4️⃣ Predict reconstruction for test set\n",
    "X_test_pred = autoencoder.predict(X_test_if)\n",
    "\n",
    "# 5️⃣ Compute reconstruction error\n",
    "reconstruction_error = np.mean(np.square(X_test_if - X_test_pred), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "52760e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 405us/step\n",
      "\n",
      "=== AUTOENCODER ===\n",
      "Accuracy: 0.8121212121212121\n",
      "ROC-AUC: 0.6165344609401524\n",
      "Precision: 0.4118866620594333\n",
      "Recall: 0.16464088397790055\n",
      "F1-score: 0.23524768107361357\n",
      "Confusion Matrix:\n",
      " [[16154   851]\n",
      " [ 3024   596]]\n"
     ]
    }
   ],
   "source": [
    "X_test_pred = autoencoder.predict(X_test_if)\n",
    "test_rec_err = np.mean((X_test_if - X_test_pred)**2, axis=1)\n",
    "\n",
    "X_train_pred = autoencoder.predict(X_train_if[y_train == 0])\n",
    "train_rec_err = np.mean((X_train_if[y_train == 0] - X_train_pred)**2, axis=1)\n",
    "\n",
    "threshold_ae = np.percentile(train_rec_err, 95)\n",
    "ae_pred = (test_rec_err > threshold_ae).astype(int)\n",
    "print(\"\\n=== AUTOENCODER ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, ae_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, test_rec_err))\n",
    "print(\"Precision:\", precision_score(y_test, ae_pred))\n",
    "print(\"Recall:\", recall_score(y_test, ae_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, ae_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, ae_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eb04ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Deep SVDD implementation in PyTorch\n",
    "# --- Encoder network (small MLP) ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Deep SVDD trainer ---\n",
    "class DeepSVDD:\n",
    "    def __init__(self, input_dim, c=None, nu=0.1, lr=1e-3):\n",
    "        self.encoder = Encoder(input_dim).to(device)\n",
    "        self.c = c  # center (torch tensor) or None -> init from data\n",
    "        self.nu = nu\n",
    "        self.optimizer = optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "        self.criterion = lambda z, c: ((z - c)**2).sum(dim=1)  # per-sample squared dist\n",
    "\n",
    "    def init_center_c(self, loader):\n",
    "        # initialize center as mean of encoder outputs on normal data\n",
    "        self.encoder.eval()\n",
    "        n = 0\n",
    "        c_sum = None\n",
    "        with torch.no_grad():\n",
    "            for x in loader:\n",
    "                x = x[0].to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                if c_sum is None:\n",
    "                    c_sum = z.sum(dim=0)\n",
    "                else:\n",
    "                    c_sum += z.sum(dim=0)\n",
    "                n += z.size(0)\n",
    "        c = c_sum / n\n",
    "        # avoid components too close to zero\n",
    "        c[(abs(c) < 1e-6)] = 1e-6\n",
    "        self.c = c\n",
    "\n",
    "    def train(self, loader, epochs=50):\n",
    "        if self.c is None:\n",
    "            self.init_center_c(loader)\n",
    "        self.encoder.train()\n",
    "        for ep in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for x, in loader:\n",
    "                x = x.to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                dist = self.criterion(z, self.c)\n",
    "                loss = dist.mean()  # minimize avg distance of normal samples\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item() * x.size(0)\n",
    "            # print progress\n",
    "            if ep % 5 == 0:\n",
    "                print(f\"DeepSVDD epoch {ep}/{epochs} loss: {epoch_loss/len(loader.dataset):.6f}\")\n",
    "\n",
    "    def score(self, X):  # X: numpy array\n",
    "        self.encoder.eval()\n",
    "        ds = TensorDataset(torch.from_numpy(X).float())\n",
    "        loader = DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                x = batch[0].to(device)\n",
    "                z = self.encoder(x)\n",
    "                dist = ((z - self.c) ** 2).sum(dim=1)  # per-sample\n",
    "                scores.append(dist.cpu().numpy())\n",
    "        return np.concatenate(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e819807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSVDD epoch 0/50 loss: 0.024755\n",
      "DeepSVDD epoch 5/50 loss: 0.000100\n",
      "DeepSVDD epoch 10/50 loss: 0.000029\n",
      "DeepSVDD epoch 15/50 loss: 0.000013\n",
      "DeepSVDD epoch 20/50 loss: 0.000008\n",
      "DeepSVDD epoch 25/50 loss: 0.000006\n",
      "DeepSVDD epoch 30/50 loss: 0.000005\n",
      "DeepSVDD epoch 35/50 loss: 0.000004\n",
      "DeepSVDD epoch 40/50 loss: 0.000003\n",
      "DeepSVDD epoch 45/50 loss: 0.000002\n"
     ]
    }
   ],
   "source": [
    "# Train DeepSVDD on *normal* training samples only (y_train==0)\n",
    "# Convert DataFrame → NumPy → Tensor\n",
    "X_train_normal_np = X_train[y_train == 0].to_numpy().astype(\"float32\")\n",
    "X_test_np = X_test.to_numpy().astype(\"float32\")\n",
    "\n",
    "\n",
    "ds = TensorDataset(torch.from_numpy(X_train_normal_np).float())\n",
    "loader = DataLoader(ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "svdd = DeepSVDD(input_dim=X_train.shape[1])\n",
    "svdd.init_center_c(DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024))\n",
    "svdd.train(loader, epochs=50)\n",
    "\n",
    "# get scores on test set (higher = more anomalous)\n",
    "# Convert X_test and X_train to NumPy arrays\n",
    "X_test_np = X_test.to_numpy().astype(\"float32\")\n",
    "X_train_np = X_train.to_numpy().astype(\"float32\")\n",
    "\n",
    "# Then score\n",
    "svdd_scores_test = svdd.score(X_test_np)\n",
    "svdd_scores_train = svdd.score(X_train_np)  # optional for thresholding\n",
    " # optional for thresholding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "26629e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DeepSVDD ===\n",
      "Accuracy: 0.8067\n",
      "ROC-AUC: 0.5375\n",
      "Precision: 0.3761\n",
      "Recall: 0.1539\n",
      "F1-score: 0.2184\n",
      "Confusion Matrix:\n",
      "[[16081   924]\n",
      " [ 3063   557]]\n"
     ]
    }
   ],
   "source": [
    "# Threshold for binary prediction\n",
    "# Usually you can set it based on 95th percentile of training normal scores\n",
    "threshold = np.percentile(svdd_scores_train[y_train == 0], 95)\n",
    "svdd_pred = (svdd_scores_test > threshold).astype(int)\n",
    "\n",
    "# Evaluation metrics\n",
    "y_test_binary = (y_test != 0).astype(int)  # treat all non-normal as anomaly\n",
    "print(\"=== DeepSVDD ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_binary, svdd_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test_binary, svdd_scores_test):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_binary, svdd_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_binary, svdd_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test_binary, svdd_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_binary, svdd_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b346d66f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(TensorDataset(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(X_train_normal_np)\u001b[38;5;241m.\u001b[39mfloat()), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m svdd\u001b[38;5;241m.\u001b[39minit_center_c(loader)\n\u001b[1;32m---> 11\u001b[0m \u001b[43msvdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# --- Step 2: Compute base model scores on test set ---\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# IF scores\u001b[39;00m\n\u001b[0;32m     15\u001b[0m if_scores_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mif_model\u001b[38;5;241m.\u001b[39mdecision_function(X_test_if)\n",
      "Cell \u001b[1;32mIn[104], line 117\u001b[0m, in \u001b[0;36mDeepSVDD.train\u001b[1;34m(self, loader, epochs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m    116\u001b[0m     loss_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x,_ \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m    118\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    119\u001b[0m         z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# --- Step 1: Ensure DeepSVDD uses correct input dimension ---\n",
    "input_dim = X_train_if.shape[1]  # include IF score if added\n",
    "svdd = DeepSVDD(input_dim=input_dim)\n",
    "X_train_normal_np = X_train_if[y_train == 0].astype(\"float32\")\n",
    "loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "svdd.init_center_c(loader)\n",
    "svdd.train(loader, epochs=50)\n",
    "\n",
    "# --- Step 2: Compute base model scores on test set ---\n",
    "# IF scores\n",
    "if_scores_test = -if_model.decision_function(X_test_if)\n",
    "\n",
    "# Autoencoder reconstruction error\n",
    "X_test_pred = autoencoder.predict(X_test_if)\n",
    "reconstruction_error = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "# DeepSVDD scores\n",
    "X_test_np = X_test_if.astype(\"float32\")\n",
    "svdd_scores_test = svdd.score(X_test_np)\n",
    "\n",
    "# --- Step 3: Stack as meta-features ---\n",
    "X_meta_test = np.vstack([if_scores_test, reconstruction_error, svdd_scores_test]).T\n",
    "y_meta_test = y_test  # binary labels\n",
    "\n",
    "# --- Step 4: Train meta-classifier ---\n",
    "meta_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# For proper training, compute meta-features on the training set as well:\n",
    "if_scores_train = -if_model.decision_function(X_train_if)\n",
    "X_train_pred = autoencoder.predict(X_train_if)\n",
    "reconstruction_error_train = np.mean(np.square(X_train_if - X_train_pred), axis=1)\n",
    "X_train_np = X_train_if.astype(\"float32\")\n",
    "svdd_scores_train = svdd.score(X_train_np)\n",
    "\n",
    "X_meta_train = np.vstack([if_scores_train, reconstruction_error_train, svdd_scores_train]).T\n",
    "y_meta_train = y_train\n",
    "\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# --- Step 5: Evaluate stacking ensemble ---\n",
    "meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "print(\"=== Stacking Ensemble (Random Forest) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, meta_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cd8b29f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5301 - val_loss: 0.3875\n",
      "Epoch 2/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2390 - val_loss: 0.2016\n",
      "Epoch 3/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1792 - val_loss: 0.1800\n",
      "Epoch 4/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1532 - val_loss: 0.1555\n",
      "Epoch 5/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1262 - val_loss: 0.1799\n",
      "Epoch 6/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1233 - val_loss: 0.1379\n",
      "Epoch 7/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1077 - val_loss: 0.1748\n",
      "Epoch 8/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1106 - val_loss: 0.1154\n",
      "Epoch 9/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0997 - val_loss: 0.1124\n",
      "Epoch 10/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0849 - val_loss: 0.1509\n",
      "Epoch 11/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0830 - val_loss: 0.1030\n",
      "Epoch 12/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0712 - val_loss: 0.1012\n",
      "Epoch 13/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0661 - val_loss: 0.1049\n",
      "Epoch 14/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0775 - val_loss: 0.0960\n",
      "Epoch 15/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0645 - val_loss: 0.0983\n",
      "Epoch 16/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0624 - val_loss: 0.0819\n",
      "Epoch 17/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0566 - val_loss: 0.1307\n",
      "Epoch 18/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0577 - val_loss: 0.0853\n",
      "Epoch 19/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0578 - val_loss: 0.0798\n",
      "Epoch 20/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499 - val_loss: 0.0819\n",
      "Epoch 21/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0553 - val_loss: 0.0749\n",
      "Epoch 22/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0567 - val_loss: 0.0735\n",
      "Epoch 23/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0544 - val_loss: 0.0811\n",
      "Epoch 24/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0478 - val_loss: 0.0712\n",
      "Epoch 25/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0500 - val_loss: 0.0689\n",
      "Epoch 26/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0503 - val_loss: 0.0798\n",
      "Epoch 27/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0489 - val_loss: 0.0901\n",
      "Epoch 28/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0475 - val_loss: 0.0613\n",
      "Epoch 29/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0542 - val_loss: 0.0735\n",
      "Epoch 30/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0460 - val_loss: 0.0634\n",
      "Epoch 31/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0367 - val_loss: 0.0717\n",
      "Epoch 32/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0405 - val_loss: 0.0732\n",
      "Epoch 33/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0391 - val_loss: 0.0603\n",
      "Epoch 34/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0444 - val_loss: 0.0595\n",
      "Epoch 35/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0389 - val_loss: 0.0593\n",
      "Epoch 36/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0467 - val_loss: 0.0662\n",
      "Epoch 37/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0320 - val_loss: 0.0582\n",
      "Epoch 38/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0337 - val_loss: 0.0524\n",
      "Epoch 39/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0325 - val_loss: 0.0500\n",
      "Epoch 40/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0342 - val_loss: 0.0580\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 500us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step\n",
      "Epoch 1/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.9974 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 0.0029\n",
      "Epoch 2/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.9551e-06 - val_accuracy: 1.0000 - val_loss: 2.7116e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.6419e-06 - val_accuracy: 1.0000 - val_loss: 1.7147e-06\n",
      "Epoch 4/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.2951e-06 - val_accuracy: 1.0000 - val_loss: 9.3555e-07\n",
      "Epoch 5/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.0352e-06 - val_accuracy: 1.0000 - val_loss: 7.6194e-07\n",
      "Epoch 6/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.9614e-07 - val_accuracy: 1.0000 - val_loss: 5.3316e-07\n",
      "Epoch 7/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 6.9478e-07 - val_accuracy: 1.0000 - val_loss: 3.9595e-07\n",
      "Epoch 8/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.5199e-07 - val_accuracy: 1.0000 - val_loss: 3.0881e-07\n",
      "Epoch 9/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.0213e-07 - val_accuracy: 1.0000 - val_loss: 3.0058e-07\n",
      "Epoch 10/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.9243e-07 - val_accuracy: 1.0000 - val_loss: 2.4511e-07\n",
      "Epoch 11/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.5197e-07 - val_accuracy: 1.0000 - val_loss: 2.3657e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.8428e-07 - val_accuracy: 1.0000 - val_loss: 1.5650e-07\n",
      "Epoch 13/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.4266e-07 - val_accuracy: 1.0000 - val_loss: 1.3884e-07\n",
      "Epoch 14/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.9337e-07 - val_accuracy: 1.0000 - val_loss: 1.1523e-07\n",
      "Epoch 15/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.7470e-07 - val_accuracy: 1.0000 - val_loss: 1.1671e-07\n",
      "Epoch 16/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.4674e-07 - val_accuracy: 1.0000 - val_loss: 7.3257e-08\n",
      "Epoch 17/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.3264e-07 - val_accuracy: 1.0000 - val_loss: 9.6660e-08\n",
      "Epoch 18/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.0656e-07 - val_accuracy: 1.0000 - val_loss: 5.4172e-08\n",
      "Epoch 19/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.6650e-08 - val_accuracy: 1.0000 - val_loss: 4.9754e-08\n",
      "Epoch 20/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.8366e-08 - val_accuracy: 1.0000 - val_loss: 3.9571e-08\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 875us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 885us/step\n",
      "DeepSVDD epoch 0/50 loss: 0.026542\n",
      "DeepSVDD epoch 5/50 loss: 0.000173\n",
      "DeepSVDD epoch 10/50 loss: 0.000050\n",
      "DeepSVDD epoch 15/50 loss: 0.000022\n",
      "DeepSVDD epoch 20/50 loss: 0.000010\n",
      "DeepSVDD epoch 25/50 loss: 0.000000\n",
      "DeepSVDD epoch 30/50 loss: 0.000000\n",
      "DeepSVDD epoch 35/50 loss: 0.000000\n",
      "DeepSVDD epoch 40/50 loss: 0.000000\n",
      "DeepSVDD epoch 45/50 loss: 0.000000\n",
      "=== Stacking Ensemble (Random Forest) ===\n",
      "Accuracy: 0.9525\n",
      "Precision: 0.8800\n",
      "Recall: 0.8448\n",
      "F1-score: 0.8620\n",
      "Confusion Matrix:\n",
      " [[16588   417]\n",
      " [  562  3058]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tensorflow.keras import layers, Model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# =========================\n",
    "# 1️⃣ Base model: Isolation Forest\n",
    "# =========================\n",
    "if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "if_model.fit(X_train[y_train == 0])  # train only on normal samples\n",
    "\n",
    "# anomaly scores (more negative = more anomalous)\n",
    "if_scores_train = -if_model.decision_function(X_train)\n",
    "if_scores_test  = -if_model.decision_function(X_test)\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ Base model: Autoencoder\n",
    "# =========================\n",
    "# Add IF score as extra feature\n",
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if  = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "input_dim = X_train_if.shape[1]\n",
    "\n",
    "# Define AE\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "x = layers.Dense(64, activation='relu')(input_layer)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(latent)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "output = layers.Dense(input_dim, activation='linear')(x)\n",
    "autoencoder = Model(input_layer, output)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train AE only on normal samples\n",
    "autoencoder.fit(\n",
    "    X_train_if[y_train == 0], X_train_if[y_train == 0],\n",
    "    epochs=40,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# AE reconstruction error\n",
    "X_train_pred = autoencoder.predict(X_train_if)\n",
    "X_test_pred  = autoencoder.predict(X_test_if)\n",
    "reconstruction_error_train = np.mean(np.square(X_train_if - X_train_pred), axis=1)\n",
    "reconstruction_error_test  = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "# CNN based Autoencoder can also be used for better performance\n",
    "import numpy as np\n",
    "\n",
    "# Convert to 3D tensor for 1D CNN: (samples, timesteps, channels)\n",
    "X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "\n",
    "y_train_cnn = y_train\n",
    "y_test_cnn  = y_test\n",
    "\n",
    "input_layer = layers.Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))\n",
    "x = layers.Conv1D(32, kernel_size=3, activation='relu')(input_layer)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Conv1D(16, kernel_size=3, activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "output = layers.Dense(1, activation='sigmoid')(latent)\n",
    "\n",
    "cnn_model = models.Model(input_layer, output)\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "cnn_model = Model(input_layer, output)\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "cnn_model.fit(\n",
    "    X_train_cnn[y_train==0], y_train_cnn[y_train==0],  # semi-supervised if you want\n",
    "    epochs=20,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "cnn_scores_train = cnn_model.predict(X_train_cnn).flatten()\n",
    "cnn_scores_test  = cnn_model.predict(X_test_cnn).flatten()\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ Base model: Deep SVDD\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "        super().__init__()\n",
    "        layers_list = []\n",
    "        prev = input_dim\n",
    "        for h in hidden:\n",
    "            layers_list.append(torch.nn.Linear(prev, h))\n",
    "            layers_list.append(torch.nn.ReLU())\n",
    "            prev = h\n",
    "        layers_list.append(torch.nn.Linear(prev, out_dim))\n",
    "        self.net = torch.nn.Sequential(*layers_list)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DeepSVDD:\n",
    "    def __init__(self, input_dim, c=None, nu=0.1, lr=1e-3):\n",
    "        self.encoder = Encoder(input_dim).to(device)\n",
    "        self.c = c\n",
    "        self.nu = nu\n",
    "        self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "        self.criterion = lambda z, c: ((z - c)**2).sum(dim=1)\n",
    "\n",
    "    def init_center_c(self, loader):\n",
    "        self.encoder.eval()\n",
    "        n = 0\n",
    "        c_sum = None\n",
    "        with torch.no_grad():\n",
    "            for x in loader:\n",
    "                x = x[0].to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                c_sum = z.sum(dim=0) if c_sum is None else c_sum + z.sum(dim=0)\n",
    "                n += z.size(0)\n",
    "        c = c_sum / n\n",
    "        c[(abs(c) < 1e-6)] = 1e-6\n",
    "        self.c = c\n",
    "\n",
    "    def train(self, loader, epochs=50):\n",
    "        if self.c is None:\n",
    "            self.init_center_c(loader)\n",
    "        self.encoder.train()\n",
    "        for ep in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for x, in loader:\n",
    "                x = x.to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                dist = self.criterion(z, self.c)\n",
    "                loss = dist.mean()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item() * x.size(0)\n",
    "            if ep % 5 == 0:\n",
    "                print(f\"DeepSVDD epoch {ep}/{epochs} loss: {epoch_loss/len(loader.dataset):.6f}\")\n",
    "\n",
    "    def score(self, X):  # X must be np.ndarray\n",
    "        self.encoder.eval()\n",
    "        ds = TensorDataset(torch.from_numpy(X).float())\n",
    "        loader = DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                x = batch[0].to(device)\n",
    "                z = self.encoder(x)\n",
    "                dist = ((z - self.c)**2).sum(dim=1)\n",
    "                scores.append(dist.cpu().numpy())\n",
    "        return np.concatenate(scores)\n",
    "\n",
    "# Train DeepSVDD\n",
    "X_train_normal_np = X_train_if[y_train == 0].astype(\"float32\")\n",
    "loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "svdd = DeepSVDD(input_dim=input_dim)\n",
    "svdd.init_center_c(loader)\n",
    "svdd.train(loader, epochs=50)\n",
    "\n",
    "svdd_scores_train = svdd.score(X_train_if.astype(\"float32\"))\n",
    "svdd_scores_test  = svdd.score(X_test_if.astype(\"float32\"))\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Z-score normalization\n",
    "if_scores_train_z = zscore(if_scores_train)\n",
    "reconstruction_error_train_z = zscore(reconstruction_error_train)\n",
    "if_scores_test_z = zscore(if_scores_test)\n",
    "reconstruction_error_test_z = zscore(reconstruction_error_test)\n",
    "\n",
    "# Stack meta-features\n",
    "X_meta_train = np.vstack([\n",
    "    if_scores_train,\n",
    "    if_scores_train_z,\n",
    "    reconstruction_error_train,\n",
    "    reconstruction_error_train_z,\n",
    "    svdd_scores_train\n",
    "]).T\n",
    "\n",
    "X_meta_test = np.vstack([\n",
    "    if_scores_test,\n",
    "    if_scores_test_z,\n",
    "    reconstruction_error_test,\n",
    "    reconstruction_error_test_z,\n",
    "    svdd_scores_test\n",
    "]).T\n",
    "\n",
    "y_meta_train = y_train  # binary labels\n",
    "y_meta_test  = y_test\n",
    "\n",
    "# =========================\n",
    "# 4️⃣ Stacking ensemble\n",
    "# =========================\n",
    "X_meta_train = np.vstack([if_scores_train, reconstruction_error_train, svdd_scores_train, cnn_scores_train]).T\n",
    "X_meta_test  = np.vstack([if_scores_test, reconstruction_error_test, svdd_scores_test, cnn_scores_test]).T\n",
    "y_meta_train = y_train\n",
    "y_meta_test  = y_test\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "\n",
    "meta_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "print(\"=== Stacking Ensemble (Random Forest) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, meta_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f700a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stacking Ensemble (XGBoost on meta-features) ===\n",
      "Accuracy: 0.9114\n",
      "Precision: 0.8238\n",
      "Recall: 0.6301\n",
      "F1-score: 0.7140\n",
      "Confusion Matrix:\n",
      " [[16517   488]\n",
      " [ 1339  2281]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Gradient Boosting meta-classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Option 1: Train XGBoost on the same meta-features\n",
    "xgb_model.fit(X_meta_train, y_meta_train)\n",
    "xgb_pred = xgb_model.predict(X_meta_test)\n",
    "\n",
    "print(\"=== Stacking Ensemble (XGBoost on meta-features) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_meta_test, xgb_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_meta_test, xgb_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_meta_test, xgb_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_meta_test, xgb_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, xgb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e6ba116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5074 - val_loss: 0.3378\n",
      "Epoch 2/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2330 - val_loss: 0.2100\n",
      "Epoch 3/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1740 - val_loss: 0.1726\n",
      "Epoch 4/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1539 - val_loss: 0.1623\n",
      "Epoch 5/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1265 - val_loss: 0.1399\n",
      "Epoch 6/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1119 - val_loss: 0.1356\n",
      "Epoch 7/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1016 - val_loss: 0.1140\n",
      "Epoch 8/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0929 - val_loss: 0.1061\n",
      "Epoch 9/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0827 - val_loss: 0.0932\n",
      "Epoch 10/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0794 - val_loss: 0.1086\n",
      "Epoch 11/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0864 - val_loss: 0.0996\n",
      "Epoch 12/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0747 - val_loss: 0.0892\n",
      "Epoch 13/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0705 - val_loss: 0.0835\n",
      "Epoch 14/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0696 - val_loss: 0.0944\n",
      "Epoch 15/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0681 - val_loss: 0.0970\n",
      "Epoch 16/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0665 - val_loss: 0.0823\n",
      "Epoch 17/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0568 - val_loss: 0.0782\n",
      "Epoch 18/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0617 - val_loss: 0.0769\n",
      "Epoch 19/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0559 - val_loss: 0.0738\n",
      "Epoch 20/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0519 - val_loss: 0.0883\n",
      "Epoch 21/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0499 - val_loss: 0.0698\n",
      "Epoch 22/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0514 - val_loss: 0.1129\n",
      "Epoch 23/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0507 - val_loss: 0.0822\n",
      "Epoch 24/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0539 - val_loss: 0.0605\n",
      "Epoch 25/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0534 - val_loss: 0.0586\n",
      "Epoch 26/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0421 - val_loss: 0.0664\n",
      "Epoch 27/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0397 - val_loss: 0.0776\n",
      "Epoch 28/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0431 - val_loss: 0.0620\n",
      "Epoch 29/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0410 - val_loss: 0.0600\n",
      "Epoch 30/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0422 - val_loss: 0.0649\n",
      "Epoch 31/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0415 - val_loss: 0.0639\n",
      "Epoch 32/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0418 - val_loss: 0.0563\n",
      "Epoch 33/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0382 - val_loss: 0.0613\n",
      "Epoch 34/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0381 - val_loss: 0.0660\n",
      "Epoch 35/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0411 - val_loss: 0.0592\n",
      "Epoch 36/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0572 - val_loss: 0.1094\n",
      "Epoch 37/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0442 - val_loss: 0.0638\n",
      "Epoch 38/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0332 - val_loss: 0.0619\n",
      "Epoch 39/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0327 - val_loss: 0.0515\n",
      "Epoch 40/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0337 - val_loss: 0.0603\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 450us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step\n",
      "Epoch 1/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.9977 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 0.0092\n",
      "Epoch 2/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 6.0354e-06 - val_accuracy: 1.0000 - val_loss: 1.0053e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 4.1414e-06 - val_accuracy: 1.0000 - val_loss: 8.1064e-06\n",
      "Epoch 4/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 3.1538e-06 - val_accuracy: 1.0000 - val_loss: 3.3039e-06\n",
      "Epoch 5/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.4165e-06 - val_accuracy: 1.0000 - val_loss: 2.5121e-06\n",
      "Epoch 6/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.7389e-06 - val_accuracy: 1.0000 - val_loss: 1.8270e-06\n",
      "Epoch 7/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.3943e-06 - val_accuracy: 1.0000 - val_loss: 1.3413e-06\n",
      "Epoch 8/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.1307e-06 - val_accuracy: 1.0000 - val_loss: 1.1753e-06\n",
      "Epoch 9/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 9.0774e-07 - val_accuracy: 1.0000 - val_loss: 9.2735e-07\n",
      "Epoch 10/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 7.3588e-07 - val_accuracy: 1.0000 - val_loss: 8.4034e-07\n",
      "Epoch 11/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 6.3200e-07 - val_accuracy: 1.0000 - val_loss: 6.3632e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 4.9561e-07 - val_accuracy: 1.0000 - val_loss: 5.0954e-07\n",
      "Epoch 13/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 4.1591e-07 - val_accuracy: 1.0000 - val_loss: 3.3597e-07\n",
      "Epoch 14/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 3.5385e-07 - val_accuracy: 1.0000 - val_loss: 3.5306e-07\n",
      "Epoch 15/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.1389e-07 - val_accuracy: 1.0000 - val_loss: 2.4493e-07\n",
      "Epoch 16/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 2.6698e-07 - val_accuracy: 1.0000 - val_loss: 2.5394e-07\n",
      "Epoch 17/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.3087e-07 - val_accuracy: 1.0000 - val_loss: 2.7447e-07\n",
      "Epoch 18/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 1.8822e-07 - val_accuracy: 1.0000 - val_loss: 1.7249e-07\n",
      "Epoch 19/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 1.6955e-07 - val_accuracy: 1.0000 - val_loss: 1.6972e-07\n",
      "Epoch 20/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 1.4967e-07 - val_accuracy: 1.0000 - val_loss: 1.5103e-07\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "DeepSVDD epoch 0/50 loss: 0.027364\n",
      "DeepSVDD epoch 5/50 loss: 0.000161\n",
      "DeepSVDD epoch 10/50 loss: 0.000057\n",
      "DeepSVDD epoch 15/50 loss: 0.000035\n",
      "DeepSVDD epoch 20/50 loss: 0.000021\n",
      "DeepSVDD epoch 25/50 loss: 0.000015\n",
      "DeepSVDD epoch 30/50 loss: 0.000010\n",
      "DeepSVDD epoch 35/50 loss: 0.000006\n",
      "DeepSVDD epoch 40/50 loss: 0.000005\n",
      "DeepSVDD epoch 45/50 loss: 0.000004\n",
      "Optimal threshold: 0.530\n",
      "=== Stacking Ensemble + Threshold Tuning ===\n",
      "Accuracy: 0.9516\n",
      "Precision: 0.8842\n",
      "Recall: 0.8331\n",
      "F1-score: 0.8579\n",
      "Confusion Matrix:\n",
      " [[16610   395]\n",
      " [  604  3016]]\n",
      "=== Two-Level Stacking Ensemble (RF + XGBoost) ===\n",
      "Accuracy: 0.9506\n",
      "Precision: 0.8669\n",
      "Recall: 0.8489\n",
      "F1-score: 0.8578\n",
      "Confusion Matrix:\n",
      " [[16533   472]\n",
      " [  547  3073]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tensorflow.keras import layers, Model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import xgboost as xgb\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# =========================\n",
    "# 1️⃣ Base model: Isolation Forest\n",
    "# =========================\n",
    "if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "if_model.fit(X_train[y_train == 0])  # train only on normal samples\n",
    "\n",
    "# anomaly scores\n",
    "if_scores_train = -if_model.decision_function(X_train)\n",
    "if_scores_test  = -if_model.decision_function(X_test)\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ Base model: Autoencoder\n",
    "# =========================\n",
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if  = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "input_dim = X_train_if.shape[1]\n",
    "\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "x = layers.Dense(64, activation='relu')(input_layer)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(latent)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "output = layers.Dense(input_dim, activation='linear')(x)\n",
    "autoencoder = Model(input_layer, output)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "autoencoder.fit(\n",
    "    X_train_if[y_train == 0], X_train_if[y_train == 0],\n",
    "    epochs=40, batch_size=256, validation_split=0.1, verbose=1\n",
    ")\n",
    "\n",
    "X_train_pred = autoencoder.predict(X_train_if)\n",
    "X_test_pred  = autoencoder.predict(X_test_if)\n",
    "reconstruction_error_train = np.mean(np.square(X_train_if - X_train_pred), axis=1)\n",
    "reconstruction_error_test  = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ Base model: CNN\n",
    "# =========================\n",
    "X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "\n",
    "input_layer = layers.Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))\n",
    "x = layers.Conv1D(32, kernel_size=3, activation='relu')(input_layer)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Conv1D(16, kernel_size=3, activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "output = layers.Dense(1, activation='sigmoid')(latent)\n",
    "\n",
    "cnn_model = Model(input_layer, output)\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.fit(\n",
    "    X_train_cnn[y_train==0], y_train[y_train==0],\n",
    "    epochs=20, batch_size=256, validation_split=0.1, verbose=1\n",
    ")\n",
    "\n",
    "cnn_scores_train = cnn_model.predict(X_train_cnn).flatten()\n",
    "cnn_scores_test  = cnn_model.predict(X_test_cnn).flatten()\n",
    "\n",
    "# =========================\n",
    "# 4️⃣ Base model: Deep SVDD\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "        super().__init__()\n",
    "        layers_list = []\n",
    "        prev = input_dim\n",
    "        for h in hidden:\n",
    "            layers_list.append(torch.nn.Linear(prev, h))\n",
    "            layers_list.append(torch.nn.ReLU())\n",
    "            prev = h\n",
    "        layers_list.append(torch.nn.Linear(prev, out_dim))\n",
    "        self.net = torch.nn.Sequential(*layers_list)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DeepSVDD:\n",
    "    def __init__(self, input_dim, c=None, nu=0.1, lr=1e-3):\n",
    "        self.encoder = Encoder(input_dim).to(device)\n",
    "        self.c = c\n",
    "        self.nu = nu\n",
    "        self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "        self.criterion = lambda z, c: ((z - c)**2).sum(dim=1)\n",
    "    def init_center_c(self, loader):\n",
    "        self.encoder.eval()\n",
    "        n = 0\n",
    "        c_sum = None\n",
    "        with torch.no_grad():\n",
    "            for x in loader:\n",
    "                x = x[0].to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                c_sum = z.sum(dim=0) if c_sum is None else c_sum + z.sum(dim=0)\n",
    "                n += z.size(0)\n",
    "        c = c_sum / n\n",
    "        c[(abs(c) < 1e-6)] = 1e-6\n",
    "        self.c = c\n",
    "    def train(self, loader, epochs=50):\n",
    "        if self.c is None:\n",
    "            self.init_center_c(loader)\n",
    "        self.encoder.train()\n",
    "        for ep in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for x, in loader:\n",
    "                x = x.to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                dist = self.criterion(z, self.c)\n",
    "                loss = dist.mean()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item() * x.size(0)\n",
    "            if ep % 5 == 0:\n",
    "                print(f\"DeepSVDD epoch {ep}/{epochs} loss: {epoch_loss/len(loader.dataset):.6f}\")\n",
    "    def score(self, X):\n",
    "        self.encoder.eval()\n",
    "        ds = TensorDataset(torch.from_numpy(X).float())\n",
    "        loader = DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                x = batch[0].to(device)\n",
    "                z = self.encoder(x)\n",
    "                dist = ((z - self.c)**2).sum(dim=1)\n",
    "                scores.append(dist.cpu().numpy())\n",
    "        return np.concatenate(scores)\n",
    "\n",
    "X_train_normal_np = X_train_if[y_train==0].astype(\"float32\")\n",
    "loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "svdd = DeepSVDD(input_dim=input_dim)\n",
    "svdd.init_center_c(loader)\n",
    "svdd.train(loader, epochs=50)\n",
    "\n",
    "svdd_scores_train = svdd.score(X_train_if.astype(\"float32\"))\n",
    "svdd_scores_test  = svdd.score(X_test_if.astype(\"float32\"))\n",
    "\n",
    "# =========================\n",
    "# 5️⃣ Stack meta-features\n",
    "# =========================\n",
    "X_meta_train = np.vstack([\n",
    "    if_scores_train,\n",
    "    reconstruction_error_train,\n",
    "    svdd_scores_train,\n",
    "    cnn_scores_train\n",
    "]).T\n",
    "X_meta_test = np.vstack([\n",
    "    if_scores_test,\n",
    "    reconstruction_error_test,\n",
    "    svdd_scores_test,\n",
    "    cnn_scores_test\n",
    "]).T\n",
    "\n",
    "\n",
    "\n",
    "# Train meta-model\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# --- Step 1: Get probabilities ---\n",
    "probs = meta_model.predict_proba(X_meta_test)[:, 1]\n",
    "\n",
    "# --- Step 2: Find best threshold ---\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "precision, recall, thresholds = precision_recall_curve(y_meta_test, probs)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "best_idx = f1_scores.argmax()\n",
    "best_thresh = thresholds[best_idx]\n",
    "print(f\"Optimal threshold: {best_thresh:.3f}\")\n",
    "\n",
    "# --- Step 3: Apply threshold ---\n",
    "meta_pred_adjusted = (probs >= best_thresh).astype(int)\n",
    "\n",
    "# --- Step 4: Evaluate ---\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "print(\"=== Stacking Ensemble + Threshold Tuning ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, meta_pred_adjusted))\n",
    "\n",
    "# =========================\n",
    "# 6️⃣ First-level meta-classifier: Random Forest\n",
    "# =========================\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_meta_train, y_train)\n",
    "rf_probs_train = rf_model.predict_proba(X_meta_train)[:,1]\n",
    "rf_probs_test  = rf_model.predict_proba(X_meta_test)[:,1]\n",
    "\n",
    "# =========================\n",
    "# 7️⃣ Second-level meta-classifier: XGBoost\n",
    "# =========================\n",
    "# Use RF probabilities as extra feature\n",
    "X_meta_train_xgb = np.vstack([X_meta_train.T, rf_probs_train]).T\n",
    "X_meta_test_xgb  = np.vstack([X_meta_test.T, rf_probs_test]).T\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.3,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_meta_train_xgb, y_train)\n",
    "xgb_pred = xgb_model.predict(X_meta_test_xgb)\n",
    "\n",
    "# =========================\n",
    "# 8️⃣ Evaluate final stacked ensemble\n",
    "# =========================\n",
    "print(\"=== Two-Level Stacking Ensemble (RF + XGBoost) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, xgb_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, xgb_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, xgb_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test, xgb_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, xgb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5a665996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.4948 - val_loss: 0.2806\n",
      "Epoch 2/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2180 - val_loss: 0.2298\n",
      "Epoch 3/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1859 - val_loss: 0.1835\n",
      "Epoch 4/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1590 - val_loss: 0.1761\n",
      "Epoch 5/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1432 - val_loss: 0.1493\n",
      "Epoch 6/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1297 - val_loss: 0.1340\n",
      "Epoch 7/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1184 - val_loss: 0.1260\n",
      "Epoch 8/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1132 - val_loss: 0.1152\n",
      "Epoch 9/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0982 - val_loss: 0.1332\n",
      "Epoch 10/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1100 - val_loss: 0.1186\n",
      "Epoch 11/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0929 - val_loss: 0.1027\n",
      "Epoch 12/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0745 - val_loss: 0.1264\n",
      "Epoch 13/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0715 - val_loss: 0.0869\n",
      "Epoch 14/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0645 - val_loss: 0.0922\n",
      "Epoch 15/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0620 - val_loss: 0.0934\n",
      "Epoch 16/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0647 - val_loss: 0.0784\n",
      "Epoch 17/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0647 - val_loss: 0.1038\n",
      "Epoch 18/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0572 - val_loss: 0.0759\n",
      "Epoch 19/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0555 - val_loss: 0.0821\n",
      "Epoch 20/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0544 - val_loss: 0.0875\n",
      "Epoch 21/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0507 - val_loss: 0.0906\n",
      "Epoch 22/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0521 - val_loss: 0.0740\n",
      "Epoch 23/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0486 - val_loss: 0.0811\n",
      "Epoch 24/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0621 - val_loss: 0.0685\n",
      "Epoch 25/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0445 - val_loss: 0.0644\n",
      "Epoch 26/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0492 - val_loss: 0.0637\n",
      "Epoch 27/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0438 - val_loss: 0.0849\n",
      "Epoch 28/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0474 - val_loss: 0.0661\n",
      "Epoch 29/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0403 - val_loss: 0.0653\n",
      "Epoch 30/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0425 - val_loss: 0.0715\n",
      "Epoch 31/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0410 - val_loss: 0.0554\n",
      "Epoch 32/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0405 - val_loss: 0.0944\n",
      "Epoch 33/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0453 - val_loss: 0.0571\n",
      "Epoch 34/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0360 - val_loss: 0.0501\n",
      "Epoch 35/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0314 - val_loss: 0.0594\n",
      "Epoch 36/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0366 - val_loss: 0.0530\n",
      "Epoch 37/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0388 - val_loss: 0.0522\n",
      "Epoch 38/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0345 - val_loss: 0.0510\n",
      "Epoch 39/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0340 - val_loss: 0.0506\n",
      "Epoch 40/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0351 - val_loss: 0.0502\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 584us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step\n",
      "Epoch 1/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9972 - loss: 0.0068 - val_accuracy: 1.0000 - val_loss: 0.0134\n",
      "Epoch 2/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.2121e-05 - val_accuracy: 1.0000 - val_loss: 1.9856e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.5825e-06 - val_accuracy: 1.0000 - val_loss: 1.2441e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.2523e-06 - val_accuracy: 1.0000 - val_loss: 5.7919e-06\n",
      "Epoch 5/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.6878e-06 - val_accuracy: 1.0000 - val_loss: 3.6685e-06\n",
      "Epoch 6/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 2.6404e-06 - val_accuracy: 1.0000 - val_loss: 2.5699e-06\n",
      "Epoch 7/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 1.9859e-06 - val_accuracy: 1.0000 - val_loss: 1.7727e-06\n",
      "Epoch 8/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.5595e-06 - val_accuracy: 1.0000 - val_loss: 1.8104e-06\n",
      "Epoch 9/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.2276e-06 - val_accuracy: 1.0000 - val_loss: 9.3954e-07\n",
      "Epoch 10/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 9.9163e-07 - val_accuracy: 1.0000 - val_loss: 7.2372e-07\n",
      "Epoch 11/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 8.0515e-07 - val_accuracy: 1.0000 - val_loss: 6.3146e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 6.6725e-07 - val_accuracy: 1.0000 - val_loss: 4.9743e-07\n",
      "Epoch 13/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 5.4044e-07 - val_accuracy: 1.0000 - val_loss: 4.6978e-07\n",
      "Epoch 14/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 4.5804e-07 - val_accuracy: 1.0000 - val_loss: 4.4845e-07\n",
      "Epoch 15/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.8357e-07 - val_accuracy: 1.0000 - val_loss: 2.7617e-07\n",
      "Epoch 16/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 3.1983e-07 - val_accuracy: 1.0000 - val_loss: 2.6742e-07\n",
      "Epoch 17/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 2.7266e-07 - val_accuracy: 1.0000 - val_loss: 2.0211e-07\n",
      "Epoch 18/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 2.2543e-07 - val_accuracy: 1.0000 - val_loss: 1.8309e-07\n",
      "Epoch 19/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.9361e-07 - val_accuracy: 1.0000 - val_loss: 1.4209e-07\n",
      "Epoch 20/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.6730e-07 - val_accuracy: 1.0000 - val_loss: 1.2086e-07\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "DeepSVDD epoch 0/50 loss: 0.021284\n",
      "DeepSVDD epoch 5/50 loss: 0.000167\n",
      "DeepSVDD epoch 10/50 loss: 0.000056\n",
      "DeepSVDD epoch 15/50 loss: 0.000026\n",
      "DeepSVDD epoch 20/50 loss: 0.000015\n",
      "DeepSVDD epoch 25/50 loss: 0.000010\n",
      "DeepSVDD epoch 30/50 loss: 0.000007\n",
      "DeepSVDD epoch 35/50 loss: 0.000005\n",
      "DeepSVDD epoch 40/50 loss: 0.000003\n",
      "DeepSVDD epoch 45/50 loss: 0.000002\n",
      "Optimal threshold: 0.525\n",
      "=== Stacking Ensemble + Threshold Tuning ===\n",
      "Accuracy: 0.9530\n",
      "Precision: 0.8892\n",
      "Recall: 0.8362\n",
      "F1-score: 0.8619\n",
      "Confusion Matrix:\n",
      " [[16628   377]\n",
      " [  593  3027]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve\n",
    "from tensorflow.keras import layers, Model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# =========================\n",
    "# 1️⃣ Base model: Isolation Forest\n",
    "# =========================\n",
    "if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "if_model.fit(X_train[y_train == 0])  # train only on normal samples\n",
    "\n",
    "if_scores_train = -if_model.decision_function(X_train)\n",
    "if_scores_test  = -if_model.decision_function(X_test)\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ Base model: Autoencoder\n",
    "# =========================\n",
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if  = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "input_dim = X_train_if.shape[1]\n",
    "\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "x = layers.Dense(64, activation='relu')(input_layer)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(latent)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "output = layers.Dense(input_dim, activation='linear')(x)\n",
    "autoencoder = Model(input_layer, output)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "autoencoder.fit(\n",
    "    X_train_if[y_train == 0], X_train_if[y_train == 0],\n",
    "    epochs=40,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "X_train_pred = autoencoder.predict(X_train_if)\n",
    "X_test_pred  = autoencoder.predict(X_test_if)\n",
    "reconstruction_error_train = np.mean(np.square(X_train_if - X_train_pred), axis=1)\n",
    "reconstruction_error_test  = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ Base model: CNN (optional)\n",
    "# =========================\n",
    "X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "\n",
    "input_layer = layers.Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))\n",
    "x = layers.Conv1D(32, kernel_size=3, activation='relu')(input_layer)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Conv1D(16, kernel_size=3, activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "output = layers.Dense(1, activation='sigmoid')(latent)\n",
    "\n",
    "cnn_model = Model(input_layer, output)\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "cnn_model.fit(\n",
    "    X_train_cnn[y_train==0], y_train[y_train==0],\n",
    "    epochs=20,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cnn_scores_train = cnn_model.predict(X_train_cnn).flatten()\n",
    "cnn_scores_test  = cnn_model.predict(X_test_cnn).flatten()\n",
    "\n",
    "# =========================\n",
    "# 4️⃣ Base model: Deep SVDD\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "        super().__init__()\n",
    "        layers_list = []\n",
    "        prev = input_dim\n",
    "        for h in hidden:\n",
    "            layers_list.append(torch.nn.Linear(prev, h))\n",
    "            layers_list.append(torch.nn.ReLU())\n",
    "            prev = h\n",
    "        layers_list.append(torch.nn.Linear(prev, out_dim))\n",
    "        self.net = torch.nn.Sequential(*layers_list)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DeepSVDD:\n",
    "    def __init__(self, input_dim, c=None, nu=0.1, lr=1e-3):\n",
    "        self.encoder = Encoder(input_dim).to(device)\n",
    "        self.c = c\n",
    "        self.nu = nu\n",
    "        self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "        self.criterion = lambda z, c: ((z - c)**2).sum(dim=1)\n",
    "\n",
    "    def init_center_c(self, loader):\n",
    "        self.encoder.eval()\n",
    "        n = 0\n",
    "        c_sum = None\n",
    "        with torch.no_grad():\n",
    "            for x in loader:\n",
    "                x = x[0].to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                c_sum = z.sum(dim=0) if c_sum is None else c_sum + z.sum(dim=0)\n",
    "                n += z.size(0)\n",
    "        c = c_sum / n\n",
    "        c[(abs(c) < 1e-6)] = 1e-6\n",
    "        self.c = c\n",
    "\n",
    "    def train(self, loader, epochs=50):\n",
    "        if self.c is None:\n",
    "            self.init_center_c(loader)\n",
    "        self.encoder.train()\n",
    "        for ep in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for x, in loader:\n",
    "                x = x.to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                dist = self.criterion(z, self.c)\n",
    "                loss = dist.mean()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item() * x.size(0)\n",
    "            if ep % 5 == 0:\n",
    "                print(f\"DeepSVDD epoch {ep}/{epochs} loss: {epoch_loss/len(loader.dataset):.6f}\")\n",
    "\n",
    "    def score(self, X):\n",
    "        self.encoder.eval()\n",
    "        ds = TensorDataset(torch.from_numpy(X).float())\n",
    "        loader = DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                x = batch[0].to(device)\n",
    "                z = self.encoder(x)\n",
    "                dist = ((z - self.c)**2).sum(dim=1)\n",
    "                scores.append(dist.cpu().numpy())\n",
    "        return np.concatenate(scores)\n",
    "\n",
    "X_train_normal_np = X_train_if[y_train==0].astype(\"float32\")\n",
    "loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "\n",
    "svdd = DeepSVDD(input_dim=input_dim)\n",
    "svdd.init_center_c(loader)\n",
    "svdd.train(loader, epochs=50)\n",
    "\n",
    "svdd_scores_train = svdd.score(X_train_if.astype(\"float32\"))\n",
    "svdd_scores_test  = svdd.score(X_test_if.astype(\"float32\"))\n",
    "\n",
    "# =========================\n",
    "# 5️⃣ Stack meta-features\n",
    "# =========================\n",
    "X_meta_train = np.vstack([if_scores_train, reconstruction_error_train, svdd_scores_train, cnn_scores_train]).T\n",
    "X_meta_test  = np.vstack([if_scores_test, reconstruction_error_test, svdd_scores_test, cnn_scores_test]).T\n",
    "y_meta_train = y_train\n",
    "y_meta_test  = y_test\n",
    "\n",
    "# =========================\n",
    "# 6️⃣ Train meta-classifier with threshold tuning\n",
    "# =========================\n",
    "meta_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# --- Get probabilities ---\n",
    "probs = meta_model.predict_proba(X_meta_test)[:,1]\n",
    "\n",
    "# --- Precision-recall threshold tuning ---\n",
    "precision, recall, thresholds = precision_recall_curve(y_meta_test, probs)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "best_idx = f1_scores.argmax()\n",
    "best_thresh = thresholds[best_idx]\n",
    "print(f\"Optimal threshold: {best_thresh:.3f}\")\n",
    "\n",
    "# --- Apply threshold ---\n",
    "meta_pred_adjusted = (probs >= best_thresh).astype(int)\n",
    "\n",
    "# --- Evaluate ---\n",
    "print(\"=== Stacking Ensemble + Threshold Tuning ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, meta_pred_adjusted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "45a52522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4973357\ttotal: 174ms\tremaining: 1m 26s\n",
      "100:\tlearn: 0.6078141\ttotal: 2.02s\tremaining: 7.96s\n",
      "200:\tlearn: 0.6646870\ttotal: 4.04s\tremaining: 6.01s\n",
      "300:\tlearn: 0.6849326\ttotal: 5.73s\tremaining: 3.79s\n",
      "400:\tlearn: 0.7126731\ttotal: 7.47s\tremaining: 1.84s\n",
      "499:\tlearn: 0.7346923\ttotal: 9.04s\tremaining: 0us\n",
      "=== Stacking Ensemble (CatBoost) ===\n",
      "Accuracy: 0.9193\n",
      "Precision: 0.8493\n",
      "Recall: 0.6569\n",
      "F1-score: 0.7408\n",
      "Confusion Matrix:\n",
      " [[16583   422]\n",
      " [ 1242  2378]]\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Initialize CatBoost\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    depth=4,\n",
    "    learning_rate=0.05,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='F1',\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Train on meta-features\n",
    "cat_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Predict\n",
    "cat_pred = cat_model.predict(X_meta_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"=== Stacking Ensemble (CatBoost) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_meta_test, cat_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_meta_test, cat_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_meta_test, cat_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_meta_test, cat_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, cat_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6477f525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stacking Ensemble (Random Forest) ===\n",
      "Accuracy: 0.9525\n",
      "Precision: 0.8826\n",
      "Recall: 0.8412\n",
      "F1-score: 0.8614\n",
      "Confusion Matrix:\n",
      " [[16600   405]\n",
      " [  575  3045]]\n",
      "0:\tlearn: 0.4973357\ttotal: 14.9ms\tremaining: 7.41s\n",
      "100:\tlearn: 0.6078141\ttotal: 1.35s\tremaining: 5.35s\n",
      "200:\tlearn: 0.6646870\ttotal: 2.69s\tremaining: 4s\n",
      "300:\tlearn: 0.6849326\ttotal: 4.08s\tremaining: 2.69s\n",
      "400:\tlearn: 0.7126731\ttotal: 5.46s\tremaining: 1.35s\n",
      "499:\tlearn: 0.7346923\ttotal: 7.01s\tremaining: 0us\n",
      "=== Stacking Ensemble (CatBoost) ===\n",
      "Accuracy: 0.9193\n",
      "Precision: 0.8493\n",
      "Recall: 0.6569\n",
      "F1-score: 0.7408\n",
      "Confusion Matrix:\n",
      " [[16583   422]\n",
      " [ 1242  2378]]\n",
      "=== Ensemble RF + CatBoost ===\n",
      "Accuracy: 0.9280\n",
      "Precision: 0.9406\n",
      "Recall: 0.6296\n",
      "F1-score: 0.7543\n",
      "Confusion Matrix:\n",
      " [[16861   144]\n",
      " [ 1341  2279]]\n"
     ]
    }
   ],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# # --- Random Forest ---\n",
    "# rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "# rf_model.fit(X_meta_train, y_meta_train)\n",
    "# rf_pred = rf_model.predict(X_meta_test)\n",
    "\n",
    "# print(\"=== Stacking Ensemble (Random Forest) ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, rf_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, rf_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, rf_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, rf_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, rf_pred))\n",
    "\n",
    "\n",
    "# # --- CatBoost ---\n",
    "# cat_model = CatBoostClassifier(\n",
    "#     iterations=500,\n",
    "#     depth=4,\n",
    "#     learning_rate=0.05,\n",
    "#     loss_function='Logloss',\n",
    "#     eval_metric='F1',\n",
    "#     random_seed=42,\n",
    "#     verbose=100\n",
    "# )\n",
    "# cat_model.fit(X_meta_train, y_meta_train)\n",
    "# cat_pred = cat_model.predict(X_meta_test)\n",
    "\n",
    "# print(\"=== Stacking Ensemble (CatBoost) ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, cat_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, cat_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, cat_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, cat_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, cat_pred))\n",
    "\n",
    "\n",
    "# # Optional: combine RF + CatBoost predictions (majority vote)\n",
    "# import numpy as np\n",
    "# ensemble_pred = np.round((rf_pred + cat_pred)/2).astype(int)\n",
    "# print(\"=== Ensemble RF + CatBoost ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, ensemble_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef7548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 454us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 974us/step\n",
      "=== Weighted Ensemble (RF + CatBoost) ===\n",
      "Accuracy: 0.9531\n",
      "Precision: 0.8778\n",
      "Recall: 0.8514\n",
      "F1-score: 0.8644\n",
      "Confusion Matrix:\n",
      " [[16576   429]\n",
      " [  538  3082]]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# from tensorflow.keras import layers, Model\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from scipy.stats import zscore\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# # =========================\n",
    "# # 1️⃣ Base model: Isolation Forest\n",
    "# # =========================\n",
    "# if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "# if_model.fit(X_train[y_train == 0])  # train only on normal samples\n",
    "\n",
    "# if_scores_train = -if_model.decision_function(X_train)\n",
    "# if_scores_test  = -if_model.decision_function(X_test)\n",
    "\n",
    "# # =========================\n",
    "# # 2️⃣ Base model: Autoencoder\n",
    "# # =========================\n",
    "# X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "# X_test_if  = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "# input_dim = X_train_if.shape[1]\n",
    "\n",
    "# # Define AE\n",
    "# input_layer = layers.Input(shape=(input_dim,))\n",
    "# x = layers.Dense(64, activation='relu')(input_layer)\n",
    "# x = layers.Dense(32, activation='relu')(x)\n",
    "# latent = layers.Dense(16, activation='relu')(x)\n",
    "# x = layers.Dense(32, activation='relu')(latent)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "# output = layers.Dense(input_dim, activation='linear')(x)\n",
    "# autoencoder = Model(input_layer, output)\n",
    "# autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Train AE only on normal samples\n",
    "# autoencoder.fit(\n",
    "#     X_train_if[y_train == 0], X_train_if[y_train == 0],\n",
    "#     epochs=40, batch_size=256, validation_split=0.1, verbose=0\n",
    "# )\n",
    "\n",
    "# X_train_pred = autoencoder.predict(X_train_if)\n",
    "# X_test_pred  = autoencoder.predict(X_test_if)\n",
    "# reconstruction_error_train = np.mean(np.square(X_train_if - X_train_pred), axis=1)\n",
    "# reconstruction_error_test  = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "# # =========================\n",
    "# # 3️⃣ Base model: CNN\n",
    "# # =========================\n",
    "# X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "# X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "\n",
    "# input_layer = layers.Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))\n",
    "# x = layers.Conv1D(32, kernel_size=3, activation='relu')(input_layer)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "# x = layers.Conv1D(16, kernel_size=3, activation='relu')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# latent = layers.Dense(16, activation='relu')(x)\n",
    "# output = layers.Dense(1, activation='sigmoid')(latent)\n",
    "\n",
    "# cnn_model = Model(input_layer, output)\n",
    "# cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# cnn_model.fit(\n",
    "#     X_train_cnn[y_train==0], y_train[y_train==0],\n",
    "#     epochs=20, batch_size=256, validation_split=0.1, verbose=0\n",
    "# )\n",
    "\n",
    "# cnn_scores_train = cnn_model.predict(X_train_cnn).flatten()\n",
    "# cnn_scores_test  = cnn_model.predict(X_test_cnn).flatten()\n",
    "\n",
    "# # =========================\n",
    "# # 4️⃣ Base model: Deep SVDD\n",
    "# # =========================\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# class Encoder(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "#         super().__init__()\n",
    "#         layers_list = []\n",
    "#         prev = input_dim\n",
    "#         for h in hidden:\n",
    "#             layers_list.append(torch.nn.Linear(prev, h))\n",
    "#             layers_list.append(torch.nn.ReLU())\n",
    "#             prev = h\n",
    "#         layers_list.append(torch.nn.Linear(prev, out_dim))\n",
    "#         self.net = torch.nn.Sequential(*layers_list)\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# class DeepSVDD:\n",
    "#     def __init__(self, input_dim, c=None, nu=0.1, lr=1e-3):\n",
    "#         self.encoder = Encoder(input_dim).to(device)\n",
    "#         self.c = c\n",
    "#         self.nu = nu\n",
    "#         self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "#         self.criterion = lambda z, c: ((z - c)**2).sum(dim=1)\n",
    "\n",
    "#     def init_center_c(self, loader):\n",
    "#         self.encoder.eval()\n",
    "#         n = 0\n",
    "#         c_sum = None\n",
    "#         with torch.no_grad():\n",
    "#             for x in loader:\n",
    "#                 x = x[0].to(device).float()\n",
    "#                 z = self.encoder(x)\n",
    "#                 c_sum = z.sum(dim=0) if c_sum is None else c_sum + z.sum(dim=0)\n",
    "#                 n += z.size(0)\n",
    "#         c = c_sum / n\n",
    "#         c[(abs(c) < 1e-6)] = 1e-6\n",
    "#         self.c = c\n",
    "\n",
    "#     def train(self, loader, epochs=50):\n",
    "#         if self.c is None:\n",
    "#             self.init_center_c(loader)\n",
    "#         self.encoder.train()\n",
    "#         for ep in range(epochs):\n",
    "#             epoch_loss = 0.0\n",
    "#             for x, in loader:\n",
    "#                 x = x.to(device).float()\n",
    "#                 z = self.encoder(x)\n",
    "#                 dist = self.criterion(z, self.c)\n",
    "#                 loss = dist.mean()\n",
    "#                 self.optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 self.optimizer.step()\n",
    "#                 epoch_loss += loss.item() * x.size(0)\n",
    "\n",
    "#     def score(self, X):\n",
    "#         self.encoder.eval()\n",
    "#         ds = TensorDataset(torch.from_numpy(X).float())\n",
    "#         loader = DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "#         scores = []\n",
    "#         with torch.no_grad():\n",
    "#             for batch in loader:\n",
    "#                 x = batch[0].to(device)\n",
    "#                 z = self.encoder(x)\n",
    "#                 dist = ((z - self.c)**2).sum(dim=1)\n",
    "#                 scores.append(dist.cpu().numpy())\n",
    "#         return np.concatenate(scores)\n",
    "\n",
    "# X_train_normal_np = X_train_if[y_train==0].astype(\"float32\")\n",
    "# loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "# svdd = DeepSVDD(input_dim=input_dim)\n",
    "# svdd.init_center_c(loader)\n",
    "# svdd.train(loader, epochs=50)\n",
    "\n",
    "# svdd_scores_train = svdd.score(X_train_if.astype(\"float32\"))\n",
    "# svdd_scores_test  = svdd.score(X_test_if.astype(\"float32\"))\n",
    "\n",
    "# # =========================\n",
    "# # 5️⃣ Stack meta-features\n",
    "# # =========================\n",
    "# X_meta_train = np.vstack([if_scores_train, reconstruction_error_train, svdd_scores_train, cnn_scores_train]).T\n",
    "# X_meta_test  = np.vstack([if_scores_test, reconstruction_error_test, svdd_scores_test, cnn_scores_test]).T\n",
    "\n",
    "# y_meta_train = y_train\n",
    "# y_meta_test  = y_test\n",
    "\n",
    "# # =========================\n",
    "# # 6️⃣ Train meta-models: RF + CatBoost\n",
    "# # =========================\n",
    "# rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "# cat_model = CatBoostClassifier(n_estimators=500, learning_rate=0.1, verbose=0, random_state=42)\n",
    "\n",
    "# rf_model.fit(X_meta_train, y_meta_train)\n",
    "# cat_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# # =========================\n",
    "# # 7️⃣ Weighted ensemble + threshold tuning\n",
    "# # =========================\n",
    "# rf_probs  = rf_model.predict_proba(X_meta_test)[:,1]\n",
    "# cat_probs = cat_model.predict_proba(X_meta_test)[:,1]\n",
    "\n",
    "# # Weighting\n",
    "# w_rf, w_cat = 0.6, 0.4\n",
    "# ensemble_probs = w_rf * rf_probs + w_cat * cat_probs\n",
    "\n",
    "# # Threshold tuning\n",
    "# precision, recall, thresholds = precision_recall_curve(y_meta_test, ensemble_probs)\n",
    "# f1_scores = 2 * (precision*recall)/(precision+recall+1e-8)\n",
    "# best_thresh = thresholds[f1_scores.argmax()]\n",
    "\n",
    "# ensemble_pred = (ensemble_probs >= best_thresh).astype(int)\n",
    "\n",
    "# # =========================\n",
    "# # 8️⃣ Evaluate\n",
    "# # =========================\n",
    "# print(\"=== Weighted Ensemble (RF + CatBoost) ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, ensemble_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "28249f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 408us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439us/step\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 868us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 943us/step\n",
      "DeepSVDD epoch 0/50 loss: 0.022014\n",
      "DeepSVDD epoch 5/50 loss: 0.000211\n",
      "DeepSVDD epoch 10/50 loss: 0.000068\n",
      "DeepSVDD epoch 15/50 loss: 0.000029\n",
      "DeepSVDD epoch 20/50 loss: 0.000016\n",
      "DeepSVDD epoch 25/50 loss: 0.000009\n",
      "DeepSVDD epoch 30/50 loss: 0.000006\n",
      "DeepSVDD epoch 35/50 loss: 0.000004\n",
      "DeepSVDD epoch 40/50 loss: 0.000003\n",
      "DeepSVDD epoch 45/50 loss: 0.000003\n",
      "=== Weighted XGBoost Stacked Ensemble ===\n",
      "Accuracy: 0.9737\n",
      "Precision: 0.9126\n",
      "Recall: 0.9403\n",
      "F1-score: 0.9263\n",
      "Confusion Matrix:\n",
      " [[16679   326]\n",
      " [  216  3404]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import xgboost as xgb\n",
    "\n",
    "# =========================\n",
    "# 1️⃣ Base models: Isolation Forest + Autoencoder\n",
    "# =========================\n",
    "\n",
    "# Isolation Forest\n",
    "if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "if_model.fit(X_train[y_train==0])\n",
    "if_scores_train = -if_model.decision_function(X_train)\n",
    "if_scores_test  = -if_model.decision_function(X_test)\n",
    "\n",
    "# Autoencoder (MLP)\n",
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if  = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "input_dim = X_train_if.shape[1]\n",
    "\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "x = layers.Dense(64, activation='relu')(input_layer)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(latent)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "output = layers.Dense(input_dim, activation='linear')(x)\n",
    "autoencoder = Model(input_layer, output)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_train_if[y_train==0], X_train_if[y_train==0], epochs=40, batch_size=256, validation_split=0.1, verbose=0)\n",
    "\n",
    "X_train_pred = autoencoder.predict(X_train_if)\n",
    "X_test_pred  = autoencoder.predict(X_test_if)\n",
    "recon_error_train = np.mean((X_train_if - X_train_pred)**2, axis=1)\n",
    "recon_error_test  = np.mean((X_test_if - X_test_pred)**2, axis=1)\n",
    "\n",
    "# Z-score normalization\n",
    "if_scores_train_z = (if_scores_train - if_scores_train.mean()) / if_scores_train.std()\n",
    "recon_error_train_z = (recon_error_train - recon_error_train.mean()) / recon_error_train.std()\n",
    "if_scores_test_z  = (if_scores_test - if_scores_test.mean()) / if_scores_test.std()\n",
    "recon_error_test_z  = (recon_error_test - recon_error_test.mean()) / recon_error_test.std()\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ CNN base model\n",
    "# =========================\n",
    "X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "y_train_cnn = y_train\n",
    "y_test_cnn  = y_test\n",
    "\n",
    "input_layer = layers.Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))\n",
    "x = layers.Conv1D(32, 3, activation='relu')(input_layer)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Conv1D(16, 3, activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "output = layers.Dense(1, activation='sigmoid')(latent)\n",
    "cnn_model = Model(input_layer, output)\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.fit(X_train_cnn[y_train==0], y_train_cnn[y_train==0], epochs=20, batch_size=256, validation_split=0.1, verbose=0)\n",
    "\n",
    "cnn_scores_train = cnn_model.predict(X_train_cnn).flatten()\n",
    "cnn_scores_test  = cnn_model.predict(X_test_cnn).flatten()\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ DeepSVDD\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "        super().__init__()\n",
    "        layers_list = []\n",
    "        prev = input_dim\n",
    "        for h in hidden:\n",
    "            layers_list.append(torch.nn.Linear(prev,h))\n",
    "            layers_list.append(torch.nn.ReLU())\n",
    "            prev=h\n",
    "        layers_list.append(torch.nn.Linear(prev,out_dim))\n",
    "        self.net = torch.nn.Sequential(*layers_list)\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DeepSVDD:\n",
    "    def __init__(self, input_dim, c=None, lr=1e-3):\n",
    "        self.encoder = Encoder(input_dim).to(device)\n",
    "        self.c = c\n",
    "        self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "        self.criterion = lambda z,c: ((z-c)**2).sum(dim=1)\n",
    "    def init_center_c(self, loader):\n",
    "        self.encoder.eval()\n",
    "        n = 0\n",
    "        c_sum=None\n",
    "        with torch.no_grad():\n",
    "            for x in loader:\n",
    "                x=x[0].to(device).float()\n",
    "                z=self.encoder(x)\n",
    "                c_sum = z.sum(dim=0) if c_sum is None else c_sum+z.sum(dim=0)\n",
    "                n+=z.size(0)\n",
    "        c = c_sum/n\n",
    "        c[abs(c)<1e-6]=1e-6\n",
    "        self.c=c\n",
    "    def train(self, loader, epochs=50):\n",
    "        if self.c is None: self.init_center_c(loader)\n",
    "        self.encoder.train()\n",
    "        for ep in range(epochs):\n",
    "            epoch_loss=0\n",
    "            for x, in loader:\n",
    "                x=x.to(device).float()\n",
    "                z=self.encoder(x)\n",
    "                dist = self.criterion(z,self.c)\n",
    "                loss=dist.mean()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()*x.size(0)\n",
    "            if ep%5==0:\n",
    "                print(f\"DeepSVDD epoch {ep}/{epochs} loss: {epoch_loss/len(loader.dataset):.6f}\")\n",
    "    def score(self,X):\n",
    "        self.encoder.eval()\n",
    "        ds=TensorDataset(torch.from_numpy(X).float())\n",
    "        loader=DataLoader(ds,batch_size=1024,shuffle=False)\n",
    "        scores=[]\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                x=batch[0].to(device)\n",
    "                z=self.encoder(x)\n",
    "                dist=((z-self.c)**2).sum(dim=1)\n",
    "                scores.append(dist.cpu().numpy())\n",
    "        return np.concatenate(scores)\n",
    "\n",
    "X_train_normal_np = X_train_if[y_train==0].astype(\"float32\")\n",
    "loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "svdd = DeepSVDD(input_dim=input_dim)\n",
    "svdd.init_center_c(loader)\n",
    "svdd.train(loader, epochs=50)\n",
    "\n",
    "svdd_scores_train = svdd.score(X_train_if.astype(\"float32\"))\n",
    "svdd_scores_test  = svdd.score(X_test_if.astype(\"float32\"))\n",
    "\n",
    "# =========================\n",
    "# 4️⃣ Tree-based base models: RF + CatBoost\n",
    "# =========================\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_scores_train = rf_model.predict_proba(X_train)[:,1]\n",
    "rf_scores_test  = rf_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "cb_model = CatBoostClassifier(n_estimators=500, learning_rate=0.1, depth=4, verbose=0)\n",
    "cb_model.fit(X_train, y_train)\n",
    "cb_scores_train = cb_model.predict_proba(X_train)[:,1]\n",
    "cb_scores_test  = cb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# =========================\n",
    "# 5️⃣ Stack all base model outputs\n",
    "# =========================\n",
    "X_meta_train = np.vstack([\n",
    "    if_scores_train_z, recon_error_train_z, cnn_scores_train,\n",
    "    svdd_scores_train, rf_scores_train, cb_scores_train\n",
    "]).T\n",
    "\n",
    "X_meta_test = np.vstack([\n",
    "    if_scores_test_z, recon_error_test_z, cnn_scores_test,\n",
    "    svdd_scores_test, rf_scores_test, cb_scores_test\n",
    "]).T\n",
    "\n",
    "# =========================\n",
    "# 6️⃣ Weighted XGBoost as meta-layer\n",
    "# =========================\n",
    "meta_model = xgb.XGBClassifier(\n",
    "    n_estimators=300, max_depth=3, learning_rate=0.1, random_state=42,\n",
    "    scale_pos_weight=(y_train==0).sum()/(y_train==1).sum()\n",
    ")\n",
    "meta_model.fit(X_meta_train, y_train)\n",
    "meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "print(\"=== Weighted XGBoost Stacked Ensemble ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, meta_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, meta_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, meta_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test, meta_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, meta_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a0935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
