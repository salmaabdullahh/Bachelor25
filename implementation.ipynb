{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "131a7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                           f1_score, roc_auc_score, average_precision_score,\n",
    "                           classification_report, confusion_matrix)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b155f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0f10007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2150cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103121 entries, 0 to 103120\n",
      "Data columns (total 79 columns):\n",
      " #   Column                      Non-Null Count   Dtype   \n",
      "---  ------                      --------------   -----   \n",
      " 0   Protocol                    103121 non-null  int8    \n",
      " 1   Flow Duration               103121 non-null  int32   \n",
      " 2   Total Fwd Packet            103121 non-null  int32   \n",
      " 3   Total Bwd packets           103121 non-null  int32   \n",
      " 4   Total Length of Fwd Packet  103121 non-null  int32   \n",
      " 5   Total Length of Bwd Packet  103121 non-null  int32   \n",
      " 6   Fwd Packet Length Max       103121 non-null  int32   \n",
      " 7   Fwd Packet Length Min       103121 non-null  int16   \n",
      " 8   Fwd Packet Length Mean      103121 non-null  float32 \n",
      " 9   Fwd Packet Length Std       103121 non-null  float32 \n",
      " 10  Bwd Packet Length Max       103121 non-null  int32   \n",
      " 11  Bwd Packet Length Min       103121 non-null  int16   \n",
      " 12  Bwd Packet Length Mean      103121 non-null  float32 \n",
      " 13  Bwd Packet Length Std       103121 non-null  float32 \n",
      " 14  Flow Bytes/s                103121 non-null  float64 \n",
      " 15  Flow Packets/s              103121 non-null  float64 \n",
      " 16  Flow IAT Mean               103121 non-null  float32 \n",
      " 17  Flow IAT Std                103121 non-null  float32 \n",
      " 18  Flow IAT Max                103121 non-null  int32   \n",
      " 19  Flow IAT Min                103121 non-null  int32   \n",
      " 20  Fwd IAT Total               103121 non-null  int32   \n",
      " 21  Fwd IAT Mean                103121 non-null  float32 \n",
      " 22  Fwd IAT Std                 103121 non-null  float32 \n",
      " 23  Fwd IAT Max                 103121 non-null  int32   \n",
      " 24  Fwd IAT Min                 103121 non-null  int32   \n",
      " 25  Bwd IAT Total               103121 non-null  int32   \n",
      " 26  Bwd IAT Mean                103121 non-null  float32 \n",
      " 27  Bwd IAT Std                 103121 non-null  float32 \n",
      " 28  Bwd IAT Max                 103121 non-null  int32   \n",
      " 29  Bwd IAT Min                 103121 non-null  int32   \n",
      " 30  Fwd PSH Flags               103121 non-null  int8    \n",
      " 31  Bwd PSH Flags               103121 non-null  int8    \n",
      " 32  Fwd URG Flags               103121 non-null  int8    \n",
      " 33  Bwd URG Flags               103121 non-null  int8    \n",
      " 34  Fwd Header Length           103121 non-null  int32   \n",
      " 35  Bwd Header Length           103121 non-null  int32   \n",
      " 36  Fwd Packets/s               103121 non-null  float32 \n",
      " 37  Bwd Packets/s               103121 non-null  float32 \n",
      " 38  Packet Length Min           103121 non-null  int16   \n",
      " 39  Packet Length Max           103121 non-null  int32   \n",
      " 40  Packet Length Mean          103121 non-null  float32 \n",
      " 41  Packet Length Std           103121 non-null  float32 \n",
      " 42  Packet Length Variance      103121 non-null  float32 \n",
      " 43  FIN Flag Count              103121 non-null  int8    \n",
      " 44  SYN Flag Count              103121 non-null  int8    \n",
      " 45  RST Flag Count              103121 non-null  int8    \n",
      " 46  PSH Flag Count              103121 non-null  int32   \n",
      " 47  ACK Flag Count              103121 non-null  int32   \n",
      " 48  URG Flag Count              103121 non-null  int8    \n",
      " 49  CWE Flag Count              103121 non-null  int8    \n",
      " 50  ECE Flag Count              103121 non-null  int8    \n",
      " 51  Down/Up Ratio               103121 non-null  int16   \n",
      " 52  Avg Packet Size             103121 non-null  float32 \n",
      " 53  Fwd Segment Size Avg        103121 non-null  float32 \n",
      " 54  Bwd Segment Size Avg        103121 non-null  float32 \n",
      " 55  Fwd Bytes/Bulk Avg          103121 non-null  int8    \n",
      " 56  Fwd Packet/Bulk Avg         103121 non-null  int8    \n",
      " 57  Fwd Bulk Rate Avg           103121 non-null  int8    \n",
      " 58  Bwd Bytes/Bulk Avg          103121 non-null  int8    \n",
      " 59  Bwd Packet/Bulk Avg         103121 non-null  int32   \n",
      " 60  Bwd Bulk Rate Avg           103121 non-null  int32   \n",
      " 61  Subflow Fwd Packets         103121 non-null  int8    \n",
      " 62  Subflow Fwd Bytes           103121 non-null  int16   \n",
      " 63  Subflow Bwd Packets         103121 non-null  int8    \n",
      " 64  Subflow Bwd Bytes           103121 non-null  int16   \n",
      " 65  FWD Init Win Bytes          103121 non-null  int32   \n",
      " 66  Bwd Init Win Bytes          103121 non-null  int32   \n",
      " 67  Fwd Act Data Packets        103121 non-null  int32   \n",
      " 68  Fwd Seg Size Min            103121 non-null  int8    \n",
      " 69  Active Mean                 103121 non-null  int8    \n",
      " 70  Active Std                  103121 non-null  int8    \n",
      " 71  Active Max                  103121 non-null  int8    \n",
      " 72  Active Min                  103121 non-null  int8    \n",
      " 73  Idle Mean                   103121 non-null  float32 \n",
      " 74  Idle Std                    103121 non-null  float32 \n",
      " 75  Idle Max                    103121 non-null  float32 \n",
      " 76  Idle Min                    103121 non-null  float32 \n",
      " 77  Label                       103121 non-null  category\n",
      " 78  Label.1                     103121 non-null  category\n",
      "dtypes: category(2), float32(22), float64(2), int16(6), int32(25), int8(22)\n",
      "memory usage: 23.6 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Label\n",
       "Non-Tor    64804\n",
       "NonVPN     20216\n",
       "VPN        16922\n",
       "Tor         1179\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"cicdarknet2020.parquet\", engine=\"fastparquet\")\n",
    "df.info()\n",
    "df['Label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c1ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INITIAL DATA INSPECTION ===\n",
      "DataFrame shape: (103121, 79)\n",
      "Memory usage: 23.60 MB\n",
      "\n",
      "=== DATA TYPES DETAILED ===\n",
      "Label column info:\n",
      "  dtype: category\n",
      "  type of dtype: <class 'pandas.core.dtypes.dtypes.CategoricalDtype'>\n",
      "  is categorical? True\n",
      "  is string? True\n",
      "\n",
      "Label.1 column info:\n",
      "  dtype: category\n",
      "  type of dtype: <class 'pandas.core.dtypes.dtypes.CategoricalDtype'>\n",
      "  is categorical? True\n",
      "  is string? True\n",
      "\n",
      "=== DECIDING WHICH LABEL TO USE ===\n",
      "Based on your output:\n",
      "1. Label column: Has values 0, 1, 2, 3 (4 classes)\n",
      "2. Label.1 column: Has actual names like 'Browsing', 'P2P', etc.\n",
      "\n",
      "Checking if Label is already encoded...\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: NonVPN -> Label.1: Chat\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Tor -> Label.1: VOIP\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: NonVPN -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Email\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Email\n",
      "Label: VPN -> Label.1: VOIP\n",
      "Label: Non-Tor -> Label.1: Video-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: VOIP\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: VPN -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Video-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Video-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: NonVPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Email\n",
      "Label: NonVPN -> Label.1: Chat\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: File-Transfer\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: VPN -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Email\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: NonVPN -> Label.1: Email\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: NonVPN -> Label.1: Email\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: AUDIO-STREAMING\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: NonVPN -> Label.1: Video-Streaming\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: NonVPN -> Label.1: File-transfer\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: NonVPN -> Label.1: Video-Streaming\n",
      "Label: NonVPN -> Label.1: File-Transfer\n",
      "Label: VPN -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: AUDIO-STREAMING\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Video-Streaming\n",
      "\n",
      "=== CREATING PROPER LABEL MAPPING ===\n",
      "             most_common  num_unique  \\\n",
      "Label                                  \n",
      "Non-Tor  AUDIO-STREAMING           8   \n",
      "NonVPN              Chat           8   \n",
      "Tor      Audio-Streaming           8   \n",
      "VPN        File-Transfer           6   \n",
      "\n",
      "                                             sample_values  \n",
      "Label                                                       \n",
      "Non-Tor  [AUDIO-STREAMING, Browsing, Chat, Email, File-...  \n",
      "NonVPN   [Chat, Audio-Streaming, Email, File-Transfer, ...  \n",
      "Tor      [Audio-Streaming, Browsing, Chat, File-Transfe...  \n",
      "VPN      [File-Transfer, Chat, Audio-Streaming, Email, ...  \n",
      "\n",
      "Based on analysis, I recommend using Label.1 as it has the actual class names\n",
      "\n",
      "=== DATA CLEANING ===\n",
      "Checking for duplicate columns...\n",
      "Duplicate columns: []\n",
      "Constant columns: ['Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Bwd Bytes/Bulk Avg', 'Subflow Bwd Packets', 'Active Mean', 'Active Std', 'Active Max', 'Active Min']\n",
      "Dropped constant columns: ['Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Bwd Bytes/Bulk Avg', 'Subflow Bwd Packets', 'Active Mean', 'Active Std', 'Active Max', 'Active Min']\n",
      "\n",
      "=== HANDLING MISSING/INFINITE VALUES ===\n",
      "\n",
      "=== LABEL PROCESSING ===\n",
      "\n",
      "Cleaned label distribution:\n",
      "  'browsing': 29862 samples (28.96%)\n",
      "  'p2p': 23404 samples (22.70%)\n",
      "  'audio-streaming': 11328 samples (10.99%)\n",
      "  'file-transfer': 10647 samples (10.32%)\n",
      "  'chat': 10365 samples (10.05%)\n",
      "  'video-streaming': 9012 samples (8.74%)\n",
      "  'email': 5442 samples (5.28%)\n",
      "  'voip': 3061 samples (2.97%)\n",
      "\n",
      "=== FINAL LABEL ENCODING ===\n",
      "Class mapping:\n",
      "  0: 'audio-streaming' - 11328 samples (10.99%)\n",
      "  1: 'browsing' - 29862 samples (28.96%)\n",
      "  2: 'chat' - 10365 samples (10.05%)\n",
      "  3: 'email' - 5442 samples (5.28%)\n",
      "  4: 'file-transfer' - 10647 samples (10.32%)\n",
      "  5: 'p2p' - 23404 samples (22.70%)\n",
      "  6: 'video-streaming' - 9012 samples (8.74%)\n",
      "  7: 'voip' - 3061 samples (2.97%)\n",
      "\n",
      "=== FEATURE PREPARATION ===\n",
      "Features shape: (103121, 62)\n",
      "Labels shape: (103121,)\n",
      "\n",
      "=== FEATURE SCALING ===\n",
      "Scaled 62 numeric features\n",
      "\n",
      "=== DATA SPLITTING ===\n",
      "Class distribution:\n",
      "  1: 'browsing' - 29862 samples (28.96%)\n",
      "  5: 'p2p' - 23404 samples (22.70%)\n",
      "  0: 'audio-streaming' - 11328 samples (10.99%)\n",
      "  4: 'file-transfer' - 10647 samples (10.32%)\n",
      "  2: 'chat' - 10365 samples (10.05%)\n",
      "  6: 'video-streaming' - 9012 samples (8.74%)\n",
      "  3: 'email' - 5442 samples (5.28%)\n",
      "  7: 'voip' - 3061 samples (2.97%)\n",
      "\n",
      "Using stratified split\n",
      "\n",
      "Training set: 82496 samples\n",
      "Testing set: 20625 samples\n",
      "\n",
      "=== SAVING DATA ===\n",
      "Saved to: cicdarknet_preprocessed_20251210_164345.pkl\n",
      "\n",
      "=== SUMMARY ===\n",
      "Original data: (103121, 67)\n",
      "Features: 62\n",
      "Classes: 8\n",
      "Training samples: 82496\n",
      "Test samples: 20625\n",
      "\n",
      "Data preprocessing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(\"=== INITIAL DATA INSPECTION ===\")\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage().sum() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Let's check the actual dtypes more carefully\n",
    "print(\"\\n=== DATA TYPES DETAILED ===\")\n",
    "print(\"Label column info:\")\n",
    "print(f\"  dtype: {df['Label'].dtype}\")\n",
    "print(f\"  type of dtype: {type(df['Label'].dtype)}\")\n",
    "print(f\"  is categorical? {pd.api.types.is_categorical_dtype(df['Label'])}\")\n",
    "print(f\"  is string? {pd.api.types.is_string_dtype(df['Label'])}\")\n",
    "\n",
    "print(\"\\nLabel.1 column info:\")\n",
    "print(f\"  dtype: {df['Label.1'].dtype}\")\n",
    "print(f\"  type of dtype: {type(df['Label.1'].dtype)}\")\n",
    "print(f\"  is categorical? {pd.api.types.is_categorical_dtype(df['Label.1'])}\")\n",
    "print(f\"  is string? {pd.api.types.is_string_dtype(df['Label.1'])}\")\n",
    "\n",
    "# DECISION TIME: Which label to use?\n",
    "print(\"\\n=== DECIDING WHICH LABEL TO USE ===\")\n",
    "print(\"Based on your output:\")\n",
    "print(\"1. Label column: Has values 0, 1, 2, 3 (4 classes)\")\n",
    "print(\"2. Label.1 column: Has actual names like 'Browsing', 'P2P', etc.\")\n",
    "\n",
    "# Let me check if Label might be encoded already\n",
    "print(\"\\nChecking if Label is already encoded...\")\n",
    "# Get a mapping by sampling\n",
    "sample_size = min(100, len(df))\n",
    "sample = df[['Label', 'Label.1']].sample(sample_size)\n",
    "for _, row in sample.iterrows():\n",
    "    print(f\"Label: {row['Label']} -> Label.1: {row['Label.1']}\")\n",
    "\n",
    "\n",
    "\n",
    "# Let's create a proper mapping\n",
    "print(\"\\n=== CREATING PROPER LABEL MAPPING ===\")\n",
    "# Group by Label and see what Label.1 values correspond\n",
    "label_mapping_df = df.groupby('Label')['Label.1'].agg(['first', 'nunique', lambda x: list(x.unique())[:5]])\n",
    "label_mapping_df.columns = ['most_common', 'num_unique', 'sample_values']\n",
    "print(label_mapping_df)\n",
    "\n",
    "# If Label is already encoded and Label.1 has the names, use Label.1\n",
    "print(\"\\nBased on analysis, I recommend using Label.1 as it has the actual class names\")\n",
    "\n",
    "# Clean up the data\n",
    "print(\"\\n=== DATA CLEANING ===\")\n",
    "print(\"Checking for duplicate columns...\")\n",
    "duplicate_columns = df.columns[df.columns.duplicated()]\n",
    "print(f\"Duplicate columns: {list(duplicate_columns)}\")\n",
    "\n",
    "# Check for constant columns\n",
    "constant_columns = [col for col in df.columns if df[col].nunique() == 1]\n",
    "print(f\"Constant columns: {constant_columns}\")\n",
    "\n",
    "if constant_columns:\n",
    "    df = df.drop(columns=constant_columns)\n",
    "    print(f\"Dropped constant columns: {constant_columns}\")\n",
    "\n",
    "# Handle missing/infinite values\n",
    "print(\"\\n=== HANDLING MISSING/INFINITE VALUES ===\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "nan_counts = df.isnull().sum()\n",
    "if nan_counts.any():\n",
    "    nan_cols = nan_counts[nan_counts > 0].index.tolist()\n",
    "    print(f\"Columns with NaN: {nan_cols}\")\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Now process the labels\n",
    "print(\"\\n=== LABEL PROCESSING ===\")\n",
    "# Use Label.1 since it has the actual names\n",
    "df['Label_original'] = df['Label.1'].astype(str)\n",
    "\n",
    "# Clean the labels\n",
    "df['Label_cleaned'] = df['Label_original'].str.lower().str.strip()\n",
    "\n",
    "# Check cleaned labels\n",
    "print(\"\\nCleaned label distribution:\")\n",
    "cleaned_counts = df['Label_cleaned'].value_counts()\n",
    "for label, count in cleaned_counts.items():\n",
    "    proportion = count / len(df) * 100\n",
    "    print(f\"  '{label}': {count} samples ({proportion:.2f}%)\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['Label_encoded'] = label_encoder.fit_transform(df['Label_cleaned'])\n",
    "\n",
    "print(\"\\n=== FINAL LABEL ENCODING ===\")\n",
    "print(\"Class mapping:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    count = (df['Label_cleaned'] == label).sum()\n",
    "    proportion = count / len(df) * 100\n",
    "    print(f\"  {i}: '{label}' - {count} samples ({proportion:.2f}%)\")\n",
    "\n",
    "label_mapping = dict(zip(range(len(label_encoder.classes_)), label_encoder.classes_))\n",
    "\n",
    "# Prepare features\n",
    "print(\"\\n=== FEATURE PREPARATION ===\")\n",
    "# Exclude all label-related columns\n",
    "label_cols = ['Label', 'Label.1', 'Label_original', 'Label_cleaned', 'Label_encoded']\n",
    "exclude_cols = [col for col in label_cols if col in df.columns]\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['Label_encoded']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "# Scale features\n",
    "print(\"\\n=== FEATURE SCALING ===\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "if len(numeric_features) > 0:\n",
    "    X_scaled[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "    print(f\"Scaled {len(numeric_features)} numeric features\")\n",
    "else:\n",
    "    print(\"No numeric features to scale\")\n",
    "\n",
    "# Split data\n",
    "print(\"\\n=== DATA SPLITTING ===\")\n",
    "class_counts = y.value_counts()\n",
    "print(\"Class distribution:\")\n",
    "for class_id, count in class_counts.items():\n",
    "    class_name = label_encoder.inverse_transform([class_id])[0]\n",
    "    proportion = count / len(y) * 100\n",
    "    print(f\"  {class_id}: '{class_name}' - {count} samples ({proportion:.2f}%)\")\n",
    "\n",
    "# Use stratification if possible\n",
    "if class_counts.min() >= 2:\n",
    "    print(\"\\nUsing stratified split\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nUsing random split (some classes have < 2 samples)\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Save processed data\n",
    "print(\"\\n=== SAVING DATA ===\")\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "preprocessed_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'scaler': scaler,\n",
    "    'label_encoder': label_encoder,\n",
    "    'feature_names': X_train.columns.tolist(),\n",
    "    'label_mapping': label_mapping,\n",
    "    'num_classes': len(label_encoder.classes_)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Quick Summary \n",
    "print(\"\\nData preprocessing completed successfully.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea64c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_encrypted'] = df['Label'].apply(lambda x: 1 if x in ['Tor','VPN'] else 0)\n",
    "df['is_encrypted'] = df['is_encrypted'].astype(int)\n",
    "df['is_encrypted'].value_counts()\n",
    "X = df.drop(columns=['Label', 'is_encrypted'])\n",
    "y_multiclass = df['Label']         # for multiclass classification\n",
    "y_binary = df['is_encrypted']      # for encrypted vs non-encrypted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "091c982f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTE:\n",
      "  Class 0: 8987 samples\n",
      "  Class 1: 23732 samples\n",
      "  Class 2: 8321 samples\n",
      "  Class 3: 4405 samples\n",
      "  Class 4: 8603 samples\n",
      "  Class 5: 18782 samples\n",
      "  Class 6: 7206 samples\n",
      "  Class 7: 2460 samples\n",
      "\n",
      "Class distribution after SMOTE:\n",
      "  Class 0: 23732 samples\n",
      "  Class 1: 23732 samples\n",
      "  Class 2: 23732 samples\n",
      "  Class 3: 23732 samples\n",
      "  Class 4: 23732 samples\n",
      "  Class 5: 23732 samples\n",
      "  Class 6: 23732 samples\n",
      "  Class 7: 23732 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df['Label_encoded'] exists\n",
    "X = X_scaled  # scaled numeric features\n",
    "y = df['Label_encoded'].values  # integer-encoded labels\n",
    "\n",
    "# Split train/test (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "print(\"Class distribution before SMOTE:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  Class {u}: {c} samples\")\n",
    "\n",
    "# Apply SMOTE to training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "unique_res, counts_res = np.unique(y_train_res, return_counts=True)\n",
    "for u, c in zip(unique_res, counts_res):\n",
    "    print(f\"  Class {u}: {c} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5c9669b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTE (binary):\n",
      "  Non-encrypted: 85020 samples\n",
      "  Encrypted: 18101 samples\n",
      "\n",
      "Class distribution after SMOTE (binary):\n",
      "  Non-encrypted: 68015 samples\n",
      "  Encrypted: 68015 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Binary target\n",
    "y_binary = df['is_encrypted'].values  # 0 = non-encrypted, 1 = Tor/VPN\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_binary, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_binary\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Class distribution before SMOTE (binary):\")\n",
    "unique, counts = np.unique(y_binary, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    label_name = \"Encrypted\" if u == 1 else \"Non-encrypted\"\n",
    "    print(f\"  {label_name}: {c} samples\")\n",
    "\n",
    "# Apply SMOTE to training set\n",
    "smote_bin = SMOTE(random_state=42)\n",
    "X_train_bin_res, y_train_bin_res = smote_bin.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE (binary):\")\n",
    "unique_res, counts_res = np.unique(y_train_bin_res, return_counts=True)\n",
    "for u, c in zip(unique_res, counts_res):\n",
    "    label_name = \"Encrypted\" if u == 1 else \"Non-encrypted\"\n",
    "    print(f\"  {label_name}: {c} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4f27b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train IF on all training data (or only normal traffic if you have labels)\n",
    "if_model = IsolationForest(n_estimators=200, contamination=0.03, random_state=42)\n",
    "if_model.fit(X_train)\n",
    "\n",
    "# Get anomaly scores (more negative = more anomalous)\n",
    "if_scores_train = if_model.decision_function(X_train)\n",
    "if_scores_test = if_model.decision_function(X_test)\n",
    "\n",
    "# Add IF score as an additional feature\n",
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0407832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train encrypted count: 14481\n",
      "Test encrypted count: 3620\n",
      "Unique values in y_binary: [0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_binary, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_binary\n",
    ")\n",
    "\n",
    "print(\"Train encrypted count:\", sum(y_train))\n",
    "print(\"Test encrypted count:\", sum(y_test))\n",
    "\n",
    "print(\"Unique values in y_binary:\", np.unique(y_binary))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "10c83f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ISOLATION FOREST ===\n",
      "Accuracy: 0.8000484848484849\n",
      "ROC-AUC: 0.6621479220443776\n",
      "Precision: 0.28716216216216217\n",
      "Recall: 0.09392265193370165\n",
      "F1-score: 0.14154870940882597\n",
      "Confusion Matrix:\n",
      " [[16161   844]\n",
      " [ 3280   340]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Train IF only on normal samples (y_train == 0)\n",
    "if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "if_model.fit(X_train[y_train == 0])\n",
    "\n",
    "# anomaly scores (more negative = more anomalous)\n",
    "if_scores_train = -if_model.decision_function(X_train)\n",
    "if_scores_test = -if_model.decision_function(X_test)\n",
    "\n",
    "# pick threshold = 95th percentile of normal scores\n",
    "threshold_if = np.percentile(if_scores_train[y_train == 0], 95)\n",
    "if_pred = (if_scores_test > threshold_if).astype(int)\n",
    "\n",
    "print(\"\\n=== ISOLATION FOREST ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, if_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, if_scores_test))\n",
    "print(\"Precision:\", precision_score(y_test, if_pred))\n",
    "print(\"Recall:\", recall_score(y_test, if_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, if_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, if_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7647701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5313 - val_loss: 0.4068\n",
      "Epoch 2/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2600 - val_loss: 0.2274\n",
      "Epoch 3/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1893 - val_loss: 0.1830\n",
      "Epoch 4/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1575 - val_loss: 0.2465\n",
      "Epoch 5/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1372 - val_loss: 0.1539\n",
      "Epoch 6/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1186 - val_loss: 0.1726\n",
      "Epoch 7/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1102 - val_loss: 0.1333\n",
      "Epoch 8/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1135 - val_loss: 0.1205\n",
      "Epoch 9/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0946 - val_loss: 0.1224\n",
      "Epoch 10/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0781 - val_loss: 0.1410\n",
      "Epoch 11/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0867 - val_loss: 0.1082\n",
      "Epoch 12/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0795 - val_loss: 0.1026\n",
      "Epoch 13/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0726 - val_loss: 0.1269\n",
      "Epoch 14/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1256 - val_loss: 0.1011\n",
      "Epoch 15/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0735 - val_loss: 0.0911\n",
      "Epoch 16/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0627 - val_loss: 0.0950\n",
      "Epoch 17/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0596 - val_loss: 0.0818\n",
      "Epoch 18/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0562 - val_loss: 0.0732\n",
      "Epoch 19/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0532 - val_loss: 0.1056\n",
      "Epoch 20/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1054 - val_loss: 0.1054\n",
      "Epoch 21/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0628 - val_loss: 0.0919\n",
      "Epoch 22/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0534 - val_loss: 0.0779\n",
      "Epoch 23/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0504 - val_loss: 0.0730\n",
      "Epoch 24/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0509 - val_loss: 0.1047\n",
      "Epoch 25/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0525 - val_loss: 0.0710\n",
      "Epoch 26/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0460 - val_loss: 0.0715\n",
      "Epoch 27/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0446 - val_loss: 0.0734\n",
      "Epoch 28/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0411 - val_loss: 0.0653\n",
      "Epoch 29/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0394 - val_loss: 0.0673\n",
      "Epoch 30/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0439 - val_loss: 0.0686\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step\n"
     ]
    }
   ],
   "source": [
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "from tensorflow.keras import layers, models, Model\n",
    "\n",
    "input_dim = X_train_if.shape[1]\n",
    "reconstruction_error = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "x = layers.Dense(64, activation='relu')(input_layer)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "\n",
    "x = layers.Dense(32, activation='relu')(latent)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "output = layers.Dense(input_dim, activation='linear')(x)\n",
    "\n",
    "autoencoder = Model(input_layer, output)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "autoencoder.fit(\n",
    "    X_train_if[y_train == 0], X_train_if[y_train == 0],\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "# 4️⃣ Predict reconstruction for test set\n",
    "X_test_pred = autoencoder.predict(X_test_if)\n",
    "\n",
    "# 5️⃣ Compute reconstruction error\n",
    "reconstruction_error = np.mean(np.square(X_test_if - X_test_pred), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "52760e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 405us/step\n",
      "\n",
      "=== AUTOENCODER ===\n",
      "Accuracy: 0.8121212121212121\n",
      "ROC-AUC: 0.6165344609401524\n",
      "Precision: 0.4118866620594333\n",
      "Recall: 0.16464088397790055\n",
      "F1-score: 0.23524768107361357\n",
      "Confusion Matrix:\n",
      " [[16154   851]\n",
      " [ 3024   596]]\n"
     ]
    }
   ],
   "source": [
    "X_test_pred = autoencoder.predict(X_test_if)\n",
    "test_rec_err = np.mean((X_test_if - X_test_pred)**2, axis=1)\n",
    "\n",
    "X_train_pred = autoencoder.predict(X_train_if[y_train == 0])\n",
    "train_rec_err = np.mean((X_train_if[y_train == 0] - X_train_pred)**2, axis=1)\n",
    "\n",
    "threshold_ae = np.percentile(train_rec_err, 95)\n",
    "ae_pred = (test_rec_err > threshold_ae).astype(int)\n",
    "print(\"\\n=== AUTOENCODER ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, ae_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, test_rec_err))\n",
    "print(\"Precision:\", precision_score(y_test, ae_pred))\n",
    "print(\"Recall:\", recall_score(y_test, ae_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, ae_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, ae_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eb04ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Deep SVDD implementation in PyTorch\n",
    "# --- Encoder network (small MLP) ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Deep SVDD trainer ---\n",
    "class DeepSVDD:\n",
    "    def __init__(self, input_dim, c=None, nu=0.1, lr=1e-3):\n",
    "        self.encoder = Encoder(input_dim).to(device)\n",
    "        self.c = c  # center (torch tensor) or None -> init from data\n",
    "        self.nu = nu\n",
    "        self.optimizer = optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "        self.criterion = lambda z, c: ((z - c)**2).sum(dim=1)  # per-sample squared dist\n",
    "\n",
    "    def init_center_c(self, loader):\n",
    "        # initialize center as mean of encoder outputs on normal data\n",
    "        self.encoder.eval()\n",
    "        n = 0\n",
    "        c_sum = None\n",
    "        with torch.no_grad():\n",
    "            for x in loader:\n",
    "                x = x[0].to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                if c_sum is None:\n",
    "                    c_sum = z.sum(dim=0)\n",
    "                else:\n",
    "                    c_sum += z.sum(dim=0)\n",
    "                n += z.size(0)\n",
    "        c = c_sum / n\n",
    "        # avoid components too close to zero\n",
    "        c[(abs(c) < 1e-6)] = 1e-6\n",
    "        self.c = c\n",
    "\n",
    "    def train(self, loader, epochs=50):\n",
    "        if self.c is None:\n",
    "            self.init_center_c(loader)\n",
    "        self.encoder.train()\n",
    "        for ep in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for x, in loader:\n",
    "                x = x.to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                dist = self.criterion(z, self.c)\n",
    "                loss = dist.mean()  # minimize avg distance of normal samples\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item() * x.size(0)\n",
    "            # print progress\n",
    "            if ep % 5 == 0:\n",
    "                print(f\"DeepSVDD epoch {ep}/{epochs} loss: {epoch_loss/len(loader.dataset):.6f}\")\n",
    "\n",
    "    def score(self, X):  # X: numpy array\n",
    "        self.encoder.eval()\n",
    "        ds = TensorDataset(torch.from_numpy(X).float())\n",
    "        loader = DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                x = batch[0].to(device)\n",
    "                z = self.encoder(x)\n",
    "                dist = ((z - self.c) ** 2).sum(dim=1)  # per-sample\n",
    "                scores.append(dist.cpu().numpy())\n",
    "        return np.concatenate(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e819807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSVDD epoch 0/50 loss: 0.024755\n",
      "DeepSVDD epoch 5/50 loss: 0.000100\n",
      "DeepSVDD epoch 10/50 loss: 0.000029\n",
      "DeepSVDD epoch 15/50 loss: 0.000013\n",
      "DeepSVDD epoch 20/50 loss: 0.000008\n",
      "DeepSVDD epoch 25/50 loss: 0.000006\n",
      "DeepSVDD epoch 30/50 loss: 0.000005\n",
      "DeepSVDD epoch 35/50 loss: 0.000004\n",
      "DeepSVDD epoch 40/50 loss: 0.000003\n",
      "DeepSVDD epoch 45/50 loss: 0.000002\n"
     ]
    }
   ],
   "source": [
    "# Train DeepSVDD on *normal* training samples only (y_train==0)\n",
    "# Convert DataFrame → NumPy → Tensor\n",
    "X_train_normal_np = X_train[y_train == 0].to_numpy().astype(\"float32\")\n",
    "X_test_np = X_test.to_numpy().astype(\"float32\")\n",
    "\n",
    "\n",
    "ds = TensorDataset(torch.from_numpy(X_train_normal_np).float())\n",
    "loader = DataLoader(ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "svdd = DeepSVDD(input_dim=X_train.shape[1])\n",
    "svdd.init_center_c(DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024))\n",
    "svdd.train(loader, epochs=50)\n",
    "\n",
    "# get scores on test set (higher = more anomalous)\n",
    "# Convert X_test and X_train to NumPy arrays\n",
    "X_test_np = X_test.to_numpy().astype(\"float32\")\n",
    "X_train_np = X_train.to_numpy().astype(\"float32\")\n",
    "\n",
    "# Then score\n",
    "svdd_scores_test = svdd.score(X_test_np)\n",
    "svdd_scores_train = svdd.score(X_train_np)  # optional for thresholding\n",
    " # optional for thresholding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "26629e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DeepSVDD ===\n",
      "Accuracy: 0.8067\n",
      "ROC-AUC: 0.5375\n",
      "Precision: 0.3761\n",
      "Recall: 0.1539\n",
      "F1-score: 0.2184\n",
      "Confusion Matrix:\n",
      "[[16081   924]\n",
      " [ 3063   557]]\n"
     ]
    }
   ],
   "source": [
    "# Threshold for binary prediction\n",
    "# Usually you can set it based on 95th percentile of training normal scores\n",
    "threshold = np.percentile(svdd_scores_train[y_train == 0], 95)\n",
    "svdd_pred = (svdd_scores_test > threshold).astype(int)\n",
    "\n",
    "# Evaluation metrics\n",
    "y_test_binary = (y_test != 0).astype(int)  # treat all non-normal as anomaly\n",
    "print(\"=== DeepSVDD ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_binary, svdd_pred):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test_binary, svdd_scores_test):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_binary, svdd_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_binary, svdd_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test_binary, svdd_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_binary, svdd_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b346d66f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(TensorDataset(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(X_train_normal_np)\u001b[38;5;241m.\u001b[39mfloat()), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m svdd\u001b[38;5;241m.\u001b[39minit_center_c(loader)\n\u001b[1;32m---> 11\u001b[0m \u001b[43msvdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# --- Step 2: Compute base model scores on test set ---\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# IF scores\u001b[39;00m\n\u001b[0;32m     15\u001b[0m if_scores_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mif_model\u001b[38;5;241m.\u001b[39mdecision_function(X_test_if)\n",
      "Cell \u001b[1;32mIn[104], line 117\u001b[0m, in \u001b[0;36mDeepSVDD.train\u001b[1;34m(self, loader, epochs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m    116\u001b[0m     loss_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x,_ \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m    118\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    119\u001b[0m         z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# --- Step 1: Ensure DeepSVDD uses correct input dimension ---\n",
    "input_dim = X_train_if.shape[1]  # include IF score if added\n",
    "svdd = DeepSVDD(input_dim=input_dim)\n",
    "X_train_normal_np = X_train_if[y_train == 0].astype(\"float32\")\n",
    "loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "svdd.init_center_c(loader)\n",
    "svdd.train(loader, epochs=50)\n",
    "\n",
    "# --- Step 2: Compute base model scores on test set ---\n",
    "# IF scores\n",
    "if_scores_test = -if_model.decision_function(X_test_if)\n",
    "\n",
    "# Autoencoder reconstruction error\n",
    "X_test_pred = autoencoder.predict(X_test_if)\n",
    "reconstruction_error = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "# DeepSVDD scores\n",
    "X_test_np = X_test_if.astype(\"float32\")\n",
    "svdd_scores_test = svdd.score(X_test_np)\n",
    "\n",
    "# --- Step 3: Stack as meta-features ---\n",
    "X_meta_test = np.vstack([if_scores_test, reconstruction_error, svdd_scores_test]).T\n",
    "y_meta_test = y_test  # binary labels\n",
    "\n",
    "# --- Step 4: Train meta-classifier ---\n",
    "meta_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# For proper training, compute meta-features on the training set as well:\n",
    "if_scores_train = -if_model.decision_function(X_train_if)\n",
    "X_train_pred = autoencoder.predict(X_train_if)\n",
    "reconstruction_error_train = np.mean(np.square(X_train_if - X_train_pred), axis=1)\n",
    "X_train_np = X_train_if.astype(\"float32\")\n",
    "svdd_scores_train = svdd.score(X_train_np)\n",
    "\n",
    "X_meta_train = np.vstack([if_scores_train, reconstruction_error_train, svdd_scores_train]).T\n",
    "y_meta_train = y_train\n",
    "\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# --- Step 5: Evaluate stacking ensemble ---\n",
    "meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "print(\"=== Stacking Ensemble (Random Forest) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, meta_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cd8b29f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5301 - val_loss: 0.3875\n",
      "Epoch 2/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2390 - val_loss: 0.2016\n",
      "Epoch 3/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1792 - val_loss: 0.1800\n",
      "Epoch 4/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1532 - val_loss: 0.1555\n",
      "Epoch 5/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1262 - val_loss: 0.1799\n",
      "Epoch 6/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1233 - val_loss: 0.1379\n",
      "Epoch 7/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1077 - val_loss: 0.1748\n",
      "Epoch 8/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1106 - val_loss: 0.1154\n",
      "Epoch 9/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0997 - val_loss: 0.1124\n",
      "Epoch 10/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0849 - val_loss: 0.1509\n",
      "Epoch 11/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0830 - val_loss: 0.1030\n",
      "Epoch 12/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0712 - val_loss: 0.1012\n",
      "Epoch 13/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0661 - val_loss: 0.1049\n",
      "Epoch 14/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0775 - val_loss: 0.0960\n",
      "Epoch 15/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0645 - val_loss: 0.0983\n",
      "Epoch 16/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0624 - val_loss: 0.0819\n",
      "Epoch 17/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0566 - val_loss: 0.1307\n",
      "Epoch 18/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0577 - val_loss: 0.0853\n",
      "Epoch 19/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0578 - val_loss: 0.0798\n",
      "Epoch 20/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499 - val_loss: 0.0819\n",
      "Epoch 21/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0553 - val_loss: 0.0749\n",
      "Epoch 22/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0567 - val_loss: 0.0735\n",
      "Epoch 23/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0544 - val_loss: 0.0811\n",
      "Epoch 24/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0478 - val_loss: 0.0712\n",
      "Epoch 25/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0500 - val_loss: 0.0689\n",
      "Epoch 26/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0503 - val_loss: 0.0798\n",
      "Epoch 27/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0489 - val_loss: 0.0901\n",
      "Epoch 28/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0475 - val_loss: 0.0613\n",
      "Epoch 29/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0542 - val_loss: 0.0735\n",
      "Epoch 30/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0460 - val_loss: 0.0634\n",
      "Epoch 31/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0367 - val_loss: 0.0717\n",
      "Epoch 32/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0405 - val_loss: 0.0732\n",
      "Epoch 33/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0391 - val_loss: 0.0603\n",
      "Epoch 34/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0444 - val_loss: 0.0595\n",
      "Epoch 35/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0389 - val_loss: 0.0593\n",
      "Epoch 36/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0467 - val_loss: 0.0662\n",
      "Epoch 37/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0320 - val_loss: 0.0582\n",
      "Epoch 38/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0337 - val_loss: 0.0524\n",
      "Epoch 39/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0325 - val_loss: 0.0500\n",
      "Epoch 40/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0342 - val_loss: 0.0580\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 500us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step\n",
      "Epoch 1/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.9974 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 0.0029\n",
      "Epoch 2/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.9551e-06 - val_accuracy: 1.0000 - val_loss: 2.7116e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.6419e-06 - val_accuracy: 1.0000 - val_loss: 1.7147e-06\n",
      "Epoch 4/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.2951e-06 - val_accuracy: 1.0000 - val_loss: 9.3555e-07\n",
      "Epoch 5/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.0352e-06 - val_accuracy: 1.0000 - val_loss: 7.6194e-07\n",
      "Epoch 6/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.9614e-07 - val_accuracy: 1.0000 - val_loss: 5.3316e-07\n",
      "Epoch 7/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 6.9478e-07 - val_accuracy: 1.0000 - val_loss: 3.9595e-07\n",
      "Epoch 8/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.5199e-07 - val_accuracy: 1.0000 - val_loss: 3.0881e-07\n",
      "Epoch 9/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.0213e-07 - val_accuracy: 1.0000 - val_loss: 3.0058e-07\n",
      "Epoch 10/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.9243e-07 - val_accuracy: 1.0000 - val_loss: 2.4511e-07\n",
      "Epoch 11/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.5197e-07 - val_accuracy: 1.0000 - val_loss: 2.3657e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.8428e-07 - val_accuracy: 1.0000 - val_loss: 1.5650e-07\n",
      "Epoch 13/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.4266e-07 - val_accuracy: 1.0000 - val_loss: 1.3884e-07\n",
      "Epoch 14/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.9337e-07 - val_accuracy: 1.0000 - val_loss: 1.1523e-07\n",
      "Epoch 15/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.7470e-07 - val_accuracy: 1.0000 - val_loss: 1.1671e-07\n",
      "Epoch 16/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.4674e-07 - val_accuracy: 1.0000 - val_loss: 7.3257e-08\n",
      "Epoch 17/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.3264e-07 - val_accuracy: 1.0000 - val_loss: 9.6660e-08\n",
      "Epoch 18/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.0656e-07 - val_accuracy: 1.0000 - val_loss: 5.4172e-08\n",
      "Epoch 19/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.6650e-08 - val_accuracy: 1.0000 - val_loss: 4.9754e-08\n",
      "Epoch 20/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.8366e-08 - val_accuracy: 1.0000 - val_loss: 3.9571e-08\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 875us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 885us/step\n",
      "DeepSVDD epoch 0/50 loss: 0.026542\n",
      "DeepSVDD epoch 5/50 loss: 0.000173\n",
      "DeepSVDD epoch 10/50 loss: 0.000050\n",
      "DeepSVDD epoch 15/50 loss: 0.000022\n",
      "DeepSVDD epoch 20/50 loss: 0.000010\n",
      "DeepSVDD epoch 25/50 loss: 0.000000\n",
      "DeepSVDD epoch 30/50 loss: 0.000000\n",
      "DeepSVDD epoch 35/50 loss: 0.000000\n",
      "DeepSVDD epoch 40/50 loss: 0.000000\n",
      "DeepSVDD epoch 45/50 loss: 0.000000\n",
      "=== Stacking Ensemble (Random Forest) ===\n",
      "Accuracy: 0.9525\n",
      "Precision: 0.8800\n",
      "Recall: 0.8448\n",
      "F1-score: 0.8620\n",
      "Confusion Matrix:\n",
      " [[16588   417]\n",
      " [  562  3058]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tensorflow.keras import layers, Model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# =========================\n",
    "# 1️⃣ Base model: Isolation Forest\n",
    "# =========================\n",
    "if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "if_model.fit(X_train[y_train == 0])  # train only on normal samples\n",
    "\n",
    "# anomaly scores (more negative = more anomalous)\n",
    "if_scores_train = -if_model.decision_function(X_train)\n",
    "if_scores_test  = -if_model.decision_function(X_test)\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ Base model: Autoencoder\n",
    "# =========================\n",
    "# Add IF score as extra feature\n",
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if  = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "input_dim = X_train_if.shape[1]\n",
    "\n",
    "# Define AE\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "x = layers.Dense(64, activation='relu')(input_layer)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(latent)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "output = layers.Dense(input_dim, activation='linear')(x)\n",
    "autoencoder = Model(input_layer, output)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train AE only on normal samples\n",
    "autoencoder.fit(\n",
    "    X_train_if[y_train == 0], X_train_if[y_train == 0],\n",
    "    epochs=40,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# AE reconstruction error\n",
    "X_train_pred = autoencoder.predict(X_train_if)\n",
    "X_test_pred  = autoencoder.predict(X_test_if)\n",
    "reconstruction_error_train = np.mean(np.square(X_train_if - X_train_pred), axis=1)\n",
    "reconstruction_error_test  = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "# CNN based Autoencoder can also be used for better performance\n",
    "import numpy as np\n",
    "\n",
    "# Convert to 3D tensor for 1D CNN: (samples, timesteps, channels)\n",
    "X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "\n",
    "y_train_cnn = y_train\n",
    "y_test_cnn  = y_test\n",
    "\n",
    "input_layer = layers.Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))\n",
    "x = layers.Conv1D(32, kernel_size=3, activation='relu')(input_layer)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Conv1D(16, kernel_size=3, activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "output = layers.Dense(1, activation='sigmoid')(latent)\n",
    "\n",
    "cnn_model = models.Model(input_layer, output)\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "cnn_model = Model(input_layer, output)\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "cnn_model.fit(\n",
    "    X_train_cnn[y_train==0], y_train_cnn[y_train==0],  # semi-supervised if you want\n",
    "    epochs=20,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "cnn_scores_train = cnn_model.predict(X_train_cnn).flatten()\n",
    "cnn_scores_test  = cnn_model.predict(X_test_cnn).flatten()\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ Base model: Deep SVDD\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "        super().__init__()\n",
    "        layers_list = []\n",
    "        prev = input_dim\n",
    "        for h in hidden:\n",
    "            layers_list.append(torch.nn.Linear(prev, h))\n",
    "            layers_list.append(torch.nn.ReLU())\n",
    "            prev = h\n",
    "        layers_list.append(torch.nn.Linear(prev, out_dim))\n",
    "        self.net = torch.nn.Sequential(*layers_list)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DeepSVDD:\n",
    "    def __init__(self, input_dim, c=None, nu=0.1, lr=1e-3):\n",
    "        self.encoder = Encoder(input_dim).to(device)\n",
    "        self.c = c\n",
    "        self.nu = nu\n",
    "        self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "        self.criterion = lambda z, c: ((z - c)**2).sum(dim=1)\n",
    "\n",
    "    def init_center_c(self, loader):\n",
    "        self.encoder.eval()\n",
    "        n = 0\n",
    "        c_sum = None\n",
    "        with torch.no_grad():\n",
    "            for x in loader:\n",
    "                x = x[0].to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                c_sum = z.sum(dim=0) if c_sum is None else c_sum + z.sum(dim=0)\n",
    "                n += z.size(0)\n",
    "        c = c_sum / n\n",
    "        c[(abs(c) < 1e-6)] = 1e-6\n",
    "        self.c = c\n",
    "\n",
    "    def train(self, loader, epochs=50):\n",
    "        if self.c is None:\n",
    "            self.init_center_c(loader)\n",
    "        self.encoder.train()\n",
    "        for ep in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for x, in loader:\n",
    "                x = x.to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                dist = self.criterion(z, self.c)\n",
    "                loss = dist.mean()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item() * x.size(0)\n",
    "            if ep % 5 == 0:\n",
    "                print(f\"DeepSVDD epoch {ep}/{epochs} loss: {epoch_loss/len(loader.dataset):.6f}\")\n",
    "\n",
    "    def score(self, X):  # X must be np.ndarray\n",
    "        self.encoder.eval()\n",
    "        ds = TensorDataset(torch.from_numpy(X).float())\n",
    "        loader = DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                x = batch[0].to(device)\n",
    "                z = self.encoder(x)\n",
    "                dist = ((z - self.c)**2).sum(dim=1)\n",
    "                scores.append(dist.cpu().numpy())\n",
    "        return np.concatenate(scores)\n",
    "\n",
    "# Train DeepSVDD\n",
    "X_train_normal_np = X_train_if[y_train == 0].astype(\"float32\")\n",
    "loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "svdd = DeepSVDD(input_dim=input_dim)\n",
    "svdd.init_center_c(loader)\n",
    "svdd.train(loader, epochs=50)\n",
    "\n",
    "svdd_scores_train = svdd.score(X_train_if.astype(\"float32\"))\n",
    "svdd_scores_test  = svdd.score(X_test_if.astype(\"float32\"))\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Z-score normalization\n",
    "if_scores_train_z = zscore(if_scores_train)\n",
    "reconstruction_error_train_z = zscore(reconstruction_error_train)\n",
    "if_scores_test_z = zscore(if_scores_test)\n",
    "reconstruction_error_test_z = zscore(reconstruction_error_test)\n",
    "\n",
    "# Stack meta-features\n",
    "X_meta_train = np.vstack([\n",
    "    if_scores_train,\n",
    "    if_scores_train_z,\n",
    "    reconstruction_error_train,\n",
    "    reconstruction_error_train_z,\n",
    "    svdd_scores_train\n",
    "]).T\n",
    "\n",
    "X_meta_test = np.vstack([\n",
    "    if_scores_test,\n",
    "    if_scores_test_z,\n",
    "    reconstruction_error_test,\n",
    "    reconstruction_error_test_z,\n",
    "    svdd_scores_test\n",
    "]).T\n",
    "\n",
    "y_meta_train = y_train  # binary labels\n",
    "y_meta_test  = y_test\n",
    "\n",
    "# =========================\n",
    "# 4️⃣ Stacking ensemble\n",
    "# =========================\n",
    "X_meta_train = np.vstack([if_scores_train, reconstruction_error_train, svdd_scores_train, cnn_scores_train]).T\n",
    "X_meta_test  = np.vstack([if_scores_test, reconstruction_error_test, svdd_scores_test, cnn_scores_test]).T\n",
    "y_meta_train = y_train\n",
    "y_meta_test  = y_test\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "\n",
    "meta_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "print(\"=== Stacking Ensemble (Random Forest) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_meta_test, meta_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, meta_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f700a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stacking Ensemble (XGBoost on meta-features) ===\n",
      "Accuracy: 0.9114\n",
      "Precision: 0.8238\n",
      "Recall: 0.6301\n",
      "F1-score: 0.7140\n",
      "Confusion Matrix:\n",
      " [[16517   488]\n",
      " [ 1339  2281]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Gradient Boosting meta-classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Option 1: Train XGBoost on the same meta-features\n",
    "xgb_model.fit(X_meta_train, y_meta_train)\n",
    "xgb_pred = xgb_model.predict(X_meta_test)\n",
    "\n",
    "print(\"=== Stacking Ensemble (XGBoost on meta-features) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_meta_test, xgb_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_meta_test, xgb_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_meta_test, xgb_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_meta_test, xgb_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, xgb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e6ba116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5074 - val_loss: 0.3378\n",
      "Epoch 2/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2330 - val_loss: 0.2100\n",
      "Epoch 3/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1740 - val_loss: 0.1726\n",
      "Epoch 4/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1539 - val_loss: 0.1623\n",
      "Epoch 5/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1265 - val_loss: 0.1399\n",
      "Epoch 6/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1119 - val_loss: 0.1356\n",
      "Epoch 7/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1016 - val_loss: 0.1140\n",
      "Epoch 8/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0929 - val_loss: 0.1061\n",
      "Epoch 9/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0827 - val_loss: 0.0932\n",
      "Epoch 10/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0794 - val_loss: 0.1086\n",
      "Epoch 11/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0864 - val_loss: 0.0996\n",
      "Epoch 12/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0747 - val_loss: 0.0892\n",
      "Epoch 13/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0705 - val_loss: 0.0835\n",
      "Epoch 14/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0696 - val_loss: 0.0944\n",
      "Epoch 15/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0681 - val_loss: 0.0970\n",
      "Epoch 16/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0665 - val_loss: 0.0823\n",
      "Epoch 17/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0568 - val_loss: 0.0782\n",
      "Epoch 18/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0617 - val_loss: 0.0769\n",
      "Epoch 19/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0559 - val_loss: 0.0738\n",
      "Epoch 20/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0519 - val_loss: 0.0883\n",
      "Epoch 21/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0499 - val_loss: 0.0698\n",
      "Epoch 22/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0514 - val_loss: 0.1129\n",
      "Epoch 23/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0507 - val_loss: 0.0822\n",
      "Epoch 24/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0539 - val_loss: 0.0605\n",
      "Epoch 25/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0534 - val_loss: 0.0586\n",
      "Epoch 26/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0421 - val_loss: 0.0664\n",
      "Epoch 27/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0397 - val_loss: 0.0776\n",
      "Epoch 28/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0431 - val_loss: 0.0620\n",
      "Epoch 29/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0410 - val_loss: 0.0600\n",
      "Epoch 30/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0422 - val_loss: 0.0649\n",
      "Epoch 31/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0415 - val_loss: 0.0639\n",
      "Epoch 32/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0418 - val_loss: 0.0563\n",
      "Epoch 33/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0382 - val_loss: 0.0613\n",
      "Epoch 34/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0381 - val_loss: 0.0660\n",
      "Epoch 35/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0411 - val_loss: 0.0592\n",
      "Epoch 36/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0572 - val_loss: 0.1094\n",
      "Epoch 37/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0442 - val_loss: 0.0638\n",
      "Epoch 38/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0332 - val_loss: 0.0619\n",
      "Epoch 39/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0327 - val_loss: 0.0515\n",
      "Epoch 40/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0337 - val_loss: 0.0603\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 450us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step\n",
      "Epoch 1/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.9977 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 0.0092\n",
      "Epoch 2/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 6.0354e-06 - val_accuracy: 1.0000 - val_loss: 1.0053e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 4.1414e-06 - val_accuracy: 1.0000 - val_loss: 8.1064e-06\n",
      "Epoch 4/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 3.1538e-06 - val_accuracy: 1.0000 - val_loss: 3.3039e-06\n",
      "Epoch 5/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.4165e-06 - val_accuracy: 1.0000 - val_loss: 2.5121e-06\n",
      "Epoch 6/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.7389e-06 - val_accuracy: 1.0000 - val_loss: 1.8270e-06\n",
      "Epoch 7/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.3943e-06 - val_accuracy: 1.0000 - val_loss: 1.3413e-06\n",
      "Epoch 8/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.1307e-06 - val_accuracy: 1.0000 - val_loss: 1.1753e-06\n",
      "Epoch 9/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 9.0774e-07 - val_accuracy: 1.0000 - val_loss: 9.2735e-07\n",
      "Epoch 10/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 7.3588e-07 - val_accuracy: 1.0000 - val_loss: 8.4034e-07\n",
      "Epoch 11/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 6.3200e-07 - val_accuracy: 1.0000 - val_loss: 6.3632e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 4.9561e-07 - val_accuracy: 1.0000 - val_loss: 5.0954e-07\n",
      "Epoch 13/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 4.1591e-07 - val_accuracy: 1.0000 - val_loss: 3.3597e-07\n",
      "Epoch 14/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 3.5385e-07 - val_accuracy: 1.0000 - val_loss: 3.5306e-07\n",
      "Epoch 15/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.1389e-07 - val_accuracy: 1.0000 - val_loss: 2.4493e-07\n",
      "Epoch 16/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 2.6698e-07 - val_accuracy: 1.0000 - val_loss: 2.5394e-07\n",
      "Epoch 17/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.3087e-07 - val_accuracy: 1.0000 - val_loss: 2.7447e-07\n",
      "Epoch 18/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 1.8822e-07 - val_accuracy: 1.0000 - val_loss: 1.7249e-07\n",
      "Epoch 19/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 1.6955e-07 - val_accuracy: 1.0000 - val_loss: 1.6972e-07\n",
      "Epoch 20/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 1.4967e-07 - val_accuracy: 1.0000 - val_loss: 1.5103e-07\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "DeepSVDD epoch 0/50 loss: 0.027364\n",
      "DeepSVDD epoch 5/50 loss: 0.000161\n",
      "DeepSVDD epoch 10/50 loss: 0.000057\n",
      "DeepSVDD epoch 15/50 loss: 0.000035\n",
      "DeepSVDD epoch 20/50 loss: 0.000021\n",
      "DeepSVDD epoch 25/50 loss: 0.000015\n",
      "DeepSVDD epoch 30/50 loss: 0.000010\n",
      "DeepSVDD epoch 35/50 loss: 0.000006\n",
      "DeepSVDD epoch 40/50 loss: 0.000005\n",
      "DeepSVDD epoch 45/50 loss: 0.000004\n",
      "Optimal threshold: 0.530\n",
      "=== Stacking Ensemble + Threshold Tuning ===\n",
      "Accuracy: 0.9516\n",
      "Precision: 0.8842\n",
      "Recall: 0.8331\n",
      "F1-score: 0.8579\n",
      "Confusion Matrix:\n",
      " [[16610   395]\n",
      " [  604  3016]]\n",
      "=== Two-Level Stacking Ensemble (RF + XGBoost) ===\n",
      "Accuracy: 0.9506\n",
      "Precision: 0.8669\n",
      "Recall: 0.8489\n",
      "F1-score: 0.8578\n",
      "Confusion Matrix:\n",
      " [[16533   472]\n",
      " [  547  3073]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tensorflow.keras import layers, Model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import xgboost as xgb\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# =========================\n",
    "# 1️⃣ Base model: Isolation Forest\n",
    "# =========================\n",
    "if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "if_model.fit(X_train[y_train == 0])  # train only on normal samples\n",
    "\n",
    "# anomaly scores\n",
    "if_scores_train = -if_model.decision_function(X_train)\n",
    "if_scores_test  = -if_model.decision_function(X_test)\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ Base model: Autoencoder\n",
    "# =========================\n",
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if  = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "input_dim = X_train_if.shape[1]\n",
    "\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "x = layers.Dense(64, activation='relu')(input_layer)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(latent)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "output = layers.Dense(input_dim, activation='linear')(x)\n",
    "autoencoder = Model(input_layer, output)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "autoencoder.fit(\n",
    "    X_train_if[y_train == 0], X_train_if[y_train == 0],\n",
    "    epochs=40, batch_size=256, validation_split=0.1, verbose=1\n",
    ")\n",
    "\n",
    "X_train_pred = autoencoder.predict(X_train_if)\n",
    "X_test_pred  = autoencoder.predict(X_test_if)\n",
    "reconstruction_error_train = np.mean(np.square(X_train_if - X_train_pred), axis=1)\n",
    "reconstruction_error_test  = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ Base model: CNN\n",
    "# =========================\n",
    "X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "\n",
    "input_layer = layers.Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))\n",
    "x = layers.Conv1D(32, kernel_size=3, activation='relu')(input_layer)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Conv1D(16, kernel_size=3, activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "output = layers.Dense(1, activation='sigmoid')(latent)\n",
    "\n",
    "cnn_model = Model(input_layer, output)\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.fit(\n",
    "    X_train_cnn[y_train==0], y_train[y_train==0],\n",
    "    epochs=20, batch_size=256, validation_split=0.1, verbose=1\n",
    ")\n",
    "\n",
    "cnn_scores_train = cnn_model.predict(X_train_cnn).flatten()\n",
    "cnn_scores_test  = cnn_model.predict(X_test_cnn).flatten()\n",
    "\n",
    "# =========================\n",
    "# 4️⃣ Base model: Deep SVDD\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "        super().__init__()\n",
    "        layers_list = []\n",
    "        prev = input_dim\n",
    "        for h in hidden:\n",
    "            layers_list.append(torch.nn.Linear(prev, h))\n",
    "            layers_list.append(torch.nn.ReLU())\n",
    "            prev = h\n",
    "        layers_list.append(torch.nn.Linear(prev, out_dim))\n",
    "        self.net = torch.nn.Sequential(*layers_list)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DeepSVDD:\n",
    "    def __init__(self, input_dim, c=None, nu=0.1, lr=1e-3):\n",
    "        self.encoder = Encoder(input_dim).to(device)\n",
    "        self.c = c\n",
    "        self.nu = nu\n",
    "        self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "        self.criterion = lambda z, c: ((z - c)**2).sum(dim=1)\n",
    "    def init_center_c(self, loader):\n",
    "        self.encoder.eval()\n",
    "        n = 0\n",
    "        c_sum = None\n",
    "        with torch.no_grad():\n",
    "            for x in loader:\n",
    "                x = x[0].to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                c_sum = z.sum(dim=0) if c_sum is None else c_sum + z.sum(dim=0)\n",
    "                n += z.size(0)\n",
    "        c = c_sum / n\n",
    "        c[(abs(c) < 1e-6)] = 1e-6\n",
    "        self.c = c\n",
    "    def train(self, loader, epochs=50):\n",
    "        if self.c is None:\n",
    "            self.init_center_c(loader)\n",
    "        self.encoder.train()\n",
    "        for ep in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for x, in loader:\n",
    "                x = x.to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                dist = self.criterion(z, self.c)\n",
    "                loss = dist.mean()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item() * x.size(0)\n",
    "            if ep % 5 == 0:\n",
    "                print(f\"DeepSVDD epoch {ep}/{epochs} loss: {epoch_loss/len(loader.dataset):.6f}\")\n",
    "    def score(self, X):\n",
    "        self.encoder.eval()\n",
    "        ds = TensorDataset(torch.from_numpy(X).float())\n",
    "        loader = DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                x = batch[0].to(device)\n",
    "                z = self.encoder(x)\n",
    "                dist = ((z - self.c)**2).sum(dim=1)\n",
    "                scores.append(dist.cpu().numpy())\n",
    "        return np.concatenate(scores)\n",
    "\n",
    "X_train_normal_np = X_train_if[y_train==0].astype(\"float32\")\n",
    "loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "svdd = DeepSVDD(input_dim=input_dim)\n",
    "svdd.init_center_c(loader)\n",
    "svdd.train(loader, epochs=50)\n",
    "\n",
    "svdd_scores_train = svdd.score(X_train_if.astype(\"float32\"))\n",
    "svdd_scores_test  = svdd.score(X_test_if.astype(\"float32\"))\n",
    "\n",
    "# =========================\n",
    "# 5️⃣ Stack meta-features\n",
    "# =========================\n",
    "X_meta_train = np.vstack([\n",
    "    if_scores_train,\n",
    "    reconstruction_error_train,\n",
    "    svdd_scores_train,\n",
    "    cnn_scores_train\n",
    "]).T\n",
    "X_meta_test = np.vstack([\n",
    "    if_scores_test,\n",
    "    reconstruction_error_test,\n",
    "    svdd_scores_test,\n",
    "    cnn_scores_test\n",
    "]).T\n",
    "\n",
    "\n",
    "\n",
    "# Train meta-model\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# --- Step 1: Get probabilities ---\n",
    "probs = meta_model.predict_proba(X_meta_test)[:, 1]\n",
    "\n",
    "# --- Step 2: Find best threshold ---\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "precision, recall, thresholds = precision_recall_curve(y_meta_test, probs)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "best_idx = f1_scores.argmax()\n",
    "best_thresh = thresholds[best_idx]\n",
    "print(f\"Optimal threshold: {best_thresh:.3f}\")\n",
    "\n",
    "# --- Step 3: Apply threshold ---\n",
    "meta_pred_adjusted = (probs >= best_thresh).astype(int)\n",
    "\n",
    "# --- Step 4: Evaluate ---\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "print(\"=== Stacking Ensemble + Threshold Tuning ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, meta_pred_adjusted))\n",
    "\n",
    "# =========================\n",
    "# 6️⃣ First-level meta-classifier: Random Forest\n",
    "# =========================\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_meta_train, y_train)\n",
    "rf_probs_train = rf_model.predict_proba(X_meta_train)[:,1]\n",
    "rf_probs_test  = rf_model.predict_proba(X_meta_test)[:,1]\n",
    "\n",
    "# =========================\n",
    "# 7️⃣ Second-level meta-classifier: XGBoost\n",
    "# =========================\n",
    "# Use RF probabilities as extra feature\n",
    "X_meta_train_xgb = np.vstack([X_meta_train.T, rf_probs_train]).T\n",
    "X_meta_test_xgb  = np.vstack([X_meta_test.T, rf_probs_test]).T\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.3,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_meta_train_xgb, y_train)\n",
    "xgb_pred = xgb_model.predict(X_meta_test_xgb)\n",
    "\n",
    "# =========================\n",
    "# 8️⃣ Evaluate final stacked ensemble\n",
    "# =========================\n",
    "print(\"=== Two-Level Stacking Ensemble (RF + XGBoost) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, xgb_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, xgb_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, xgb_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test, xgb_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, xgb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5a665996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.4948 - val_loss: 0.2806\n",
      "Epoch 2/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2180 - val_loss: 0.2298\n",
      "Epoch 3/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1859 - val_loss: 0.1835\n",
      "Epoch 4/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1590 - val_loss: 0.1761\n",
      "Epoch 5/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1432 - val_loss: 0.1493\n",
      "Epoch 6/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1297 - val_loss: 0.1340\n",
      "Epoch 7/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1184 - val_loss: 0.1260\n",
      "Epoch 8/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1132 - val_loss: 0.1152\n",
      "Epoch 9/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0982 - val_loss: 0.1332\n",
      "Epoch 10/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1100 - val_loss: 0.1186\n",
      "Epoch 11/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0929 - val_loss: 0.1027\n",
      "Epoch 12/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0745 - val_loss: 0.1264\n",
      "Epoch 13/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0715 - val_loss: 0.0869\n",
      "Epoch 14/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0645 - val_loss: 0.0922\n",
      "Epoch 15/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0620 - val_loss: 0.0934\n",
      "Epoch 16/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0647 - val_loss: 0.0784\n",
      "Epoch 17/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0647 - val_loss: 0.1038\n",
      "Epoch 18/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0572 - val_loss: 0.0759\n",
      "Epoch 19/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0555 - val_loss: 0.0821\n",
      "Epoch 20/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0544 - val_loss: 0.0875\n",
      "Epoch 21/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0507 - val_loss: 0.0906\n",
      "Epoch 22/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0521 - val_loss: 0.0740\n",
      "Epoch 23/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0486 - val_loss: 0.0811\n",
      "Epoch 24/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0621 - val_loss: 0.0685\n",
      "Epoch 25/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0445 - val_loss: 0.0644\n",
      "Epoch 26/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0492 - val_loss: 0.0637\n",
      "Epoch 27/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0438 - val_loss: 0.0849\n",
      "Epoch 28/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0474 - val_loss: 0.0661\n",
      "Epoch 29/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0403 - val_loss: 0.0653\n",
      "Epoch 30/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0425 - val_loss: 0.0715\n",
      "Epoch 31/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0410 - val_loss: 0.0554\n",
      "Epoch 32/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0405 - val_loss: 0.0944\n",
      "Epoch 33/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0453 - val_loss: 0.0571\n",
      "Epoch 34/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0360 - val_loss: 0.0501\n",
      "Epoch 35/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0314 - val_loss: 0.0594\n",
      "Epoch 36/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0366 - val_loss: 0.0530\n",
      "Epoch 37/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0388 - val_loss: 0.0522\n",
      "Epoch 38/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0345 - val_loss: 0.0510\n",
      "Epoch 39/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0340 - val_loss: 0.0506\n",
      "Epoch 40/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0351 - val_loss: 0.0502\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 584us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step\n",
      "Epoch 1/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9972 - loss: 0.0068 - val_accuracy: 1.0000 - val_loss: 0.0134\n",
      "Epoch 2/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.2121e-05 - val_accuracy: 1.0000 - val_loss: 1.9856e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.5825e-06 - val_accuracy: 1.0000 - val_loss: 1.2441e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.2523e-06 - val_accuracy: 1.0000 - val_loss: 5.7919e-06\n",
      "Epoch 5/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.6878e-06 - val_accuracy: 1.0000 - val_loss: 3.6685e-06\n",
      "Epoch 6/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 2.6404e-06 - val_accuracy: 1.0000 - val_loss: 2.5699e-06\n",
      "Epoch 7/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 1.9859e-06 - val_accuracy: 1.0000 - val_loss: 1.7727e-06\n",
      "Epoch 8/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.5595e-06 - val_accuracy: 1.0000 - val_loss: 1.8104e-06\n",
      "Epoch 9/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.2276e-06 - val_accuracy: 1.0000 - val_loss: 9.3954e-07\n",
      "Epoch 10/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 9.9163e-07 - val_accuracy: 1.0000 - val_loss: 7.2372e-07\n",
      "Epoch 11/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 8.0515e-07 - val_accuracy: 1.0000 - val_loss: 6.3146e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 6.6725e-07 - val_accuracy: 1.0000 - val_loss: 4.9743e-07\n",
      "Epoch 13/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 5.4044e-07 - val_accuracy: 1.0000 - val_loss: 4.6978e-07\n",
      "Epoch 14/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 4.5804e-07 - val_accuracy: 1.0000 - val_loss: 4.4845e-07\n",
      "Epoch 15/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.8357e-07 - val_accuracy: 1.0000 - val_loss: 2.7617e-07\n",
      "Epoch 16/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 3.1983e-07 - val_accuracy: 1.0000 - val_loss: 2.6742e-07\n",
      "Epoch 17/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 2.7266e-07 - val_accuracy: 1.0000 - val_loss: 2.0211e-07\n",
      "Epoch 18/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 2.2543e-07 - val_accuracy: 1.0000 - val_loss: 1.8309e-07\n",
      "Epoch 19/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.9361e-07 - val_accuracy: 1.0000 - val_loss: 1.4209e-07\n",
      "Epoch 20/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.6730e-07 - val_accuracy: 1.0000 - val_loss: 1.2086e-07\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "DeepSVDD epoch 0/50 loss: 0.021284\n",
      "DeepSVDD epoch 5/50 loss: 0.000167\n",
      "DeepSVDD epoch 10/50 loss: 0.000056\n",
      "DeepSVDD epoch 15/50 loss: 0.000026\n",
      "DeepSVDD epoch 20/50 loss: 0.000015\n",
      "DeepSVDD epoch 25/50 loss: 0.000010\n",
      "DeepSVDD epoch 30/50 loss: 0.000007\n",
      "DeepSVDD epoch 35/50 loss: 0.000005\n",
      "DeepSVDD epoch 40/50 loss: 0.000003\n",
      "DeepSVDD epoch 45/50 loss: 0.000002\n",
      "Optimal threshold: 0.525\n",
      "=== Stacking Ensemble + Threshold Tuning ===\n",
      "Accuracy: 0.9530\n",
      "Precision: 0.8892\n",
      "Recall: 0.8362\n",
      "F1-score: 0.8619\n",
      "Confusion Matrix:\n",
      " [[16628   377]\n",
      " [  593  3027]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve\n",
    "from tensorflow.keras import layers, Model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# =========================\n",
    "# 1️⃣ Base model: Isolation Forest\n",
    "# =========================\n",
    "if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "if_model.fit(X_train[y_train == 0])  # train only on normal samples\n",
    "\n",
    "if_scores_train = -if_model.decision_function(X_train)\n",
    "if_scores_test  = -if_model.decision_function(X_test)\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ Base model: Autoencoder\n",
    "# =========================\n",
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if  = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "input_dim = X_train_if.shape[1]\n",
    "\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "x = layers.Dense(64, activation='relu')(input_layer)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(latent)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "output = layers.Dense(input_dim, activation='linear')(x)\n",
    "autoencoder = Model(input_layer, output)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "autoencoder.fit(\n",
    "    X_train_if[y_train == 0], X_train_if[y_train == 0],\n",
    "    epochs=40,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "X_train_pred = autoencoder.predict(X_train_if)\n",
    "X_test_pred  = autoencoder.predict(X_test_if)\n",
    "reconstruction_error_train = np.mean(np.square(X_train_if - X_train_pred), axis=1)\n",
    "reconstruction_error_test  = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ Base model: CNN (optional)\n",
    "# =========================\n",
    "X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "\n",
    "input_layer = layers.Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))\n",
    "x = layers.Conv1D(32, kernel_size=3, activation='relu')(input_layer)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Conv1D(16, kernel_size=3, activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "output = layers.Dense(1, activation='sigmoid')(latent)\n",
    "\n",
    "cnn_model = Model(input_layer, output)\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "cnn_model.fit(\n",
    "    X_train_cnn[y_train==0], y_train[y_train==0],\n",
    "    epochs=20,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cnn_scores_train = cnn_model.predict(X_train_cnn).flatten()\n",
    "cnn_scores_test  = cnn_model.predict(X_test_cnn).flatten()\n",
    "\n",
    "# =========================\n",
    "# 4️⃣ Base model: Deep SVDD\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "        super().__init__()\n",
    "        layers_list = []\n",
    "        prev = input_dim\n",
    "        for h in hidden:\n",
    "            layers_list.append(torch.nn.Linear(prev, h))\n",
    "            layers_list.append(torch.nn.ReLU())\n",
    "            prev = h\n",
    "        layers_list.append(torch.nn.Linear(prev, out_dim))\n",
    "        self.net = torch.nn.Sequential(*layers_list)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DeepSVDD:\n",
    "    def __init__(self, input_dim, c=None, nu=0.1, lr=1e-3):\n",
    "        self.encoder = Encoder(input_dim).to(device)\n",
    "        self.c = c\n",
    "        self.nu = nu\n",
    "        self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "        self.criterion = lambda z, c: ((z - c)**2).sum(dim=1)\n",
    "\n",
    "    def init_center_c(self, loader):\n",
    "        self.encoder.eval()\n",
    "        n = 0\n",
    "        c_sum = None\n",
    "        with torch.no_grad():\n",
    "            for x in loader:\n",
    "                x = x[0].to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                c_sum = z.sum(dim=0) if c_sum is None else c_sum + z.sum(dim=0)\n",
    "                n += z.size(0)\n",
    "        c = c_sum / n\n",
    "        c[(abs(c) < 1e-6)] = 1e-6\n",
    "        self.c = c\n",
    "\n",
    "    def train(self, loader, epochs=50):\n",
    "        if self.c is None:\n",
    "            self.init_center_c(loader)\n",
    "        self.encoder.train()\n",
    "        for ep in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for x, in loader:\n",
    "                x = x.to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                dist = self.criterion(z, self.c)\n",
    "                loss = dist.mean()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item() * x.size(0)\n",
    "            if ep % 5 == 0:\n",
    "                print(f\"DeepSVDD epoch {ep}/{epochs} loss: {epoch_loss/len(loader.dataset):.6f}\")\n",
    "\n",
    "    def score(self, X):\n",
    "        self.encoder.eval()\n",
    "        ds = TensorDataset(torch.from_numpy(X).float())\n",
    "        loader = DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                x = batch[0].to(device)\n",
    "                z = self.encoder(x)\n",
    "                dist = ((z - self.c)**2).sum(dim=1)\n",
    "                scores.append(dist.cpu().numpy())\n",
    "        return np.concatenate(scores)\n",
    "\n",
    "X_train_normal_np = X_train_if[y_train==0].astype(\"float32\")\n",
    "loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "\n",
    "svdd = DeepSVDD(input_dim=input_dim)\n",
    "svdd.init_center_c(loader)\n",
    "svdd.train(loader, epochs=50)\n",
    "\n",
    "svdd_scores_train = svdd.score(X_train_if.astype(\"float32\"))\n",
    "svdd_scores_test  = svdd.score(X_test_if.astype(\"float32\"))\n",
    "\n",
    "# =========================\n",
    "# 5️⃣ Stack meta-features\n",
    "# =========================\n",
    "X_meta_train = np.vstack([if_scores_train, reconstruction_error_train, svdd_scores_train, cnn_scores_train]).T\n",
    "X_meta_test  = np.vstack([if_scores_test, reconstruction_error_test, svdd_scores_test, cnn_scores_test]).T\n",
    "y_meta_train = y_train\n",
    "y_meta_test  = y_test\n",
    "\n",
    "# =========================\n",
    "# 6️⃣ Train meta-classifier with threshold tuning\n",
    "# =========================\n",
    "meta_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# --- Get probabilities ---\n",
    "probs = meta_model.predict_proba(X_meta_test)[:,1]\n",
    "\n",
    "# --- Precision-recall threshold tuning ---\n",
    "precision, recall, thresholds = precision_recall_curve(y_meta_test, probs)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "best_idx = f1_scores.argmax()\n",
    "best_thresh = thresholds[best_idx]\n",
    "print(f\"Optimal threshold: {best_thresh:.3f}\")\n",
    "\n",
    "# --- Apply threshold ---\n",
    "meta_pred_adjusted = (probs >= best_thresh).astype(int)\n",
    "\n",
    "# --- Evaluate ---\n",
    "print(\"=== Stacking Ensemble + Threshold Tuning ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, meta_pred_adjusted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "45a52522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4973357\ttotal: 174ms\tremaining: 1m 26s\n",
      "100:\tlearn: 0.6078141\ttotal: 2.02s\tremaining: 7.96s\n",
      "200:\tlearn: 0.6646870\ttotal: 4.04s\tremaining: 6.01s\n",
      "300:\tlearn: 0.6849326\ttotal: 5.73s\tremaining: 3.79s\n",
      "400:\tlearn: 0.7126731\ttotal: 7.47s\tremaining: 1.84s\n",
      "499:\tlearn: 0.7346923\ttotal: 9.04s\tremaining: 0us\n",
      "=== Stacking Ensemble (CatBoost) ===\n",
      "Accuracy: 0.9193\n",
      "Precision: 0.8493\n",
      "Recall: 0.6569\n",
      "F1-score: 0.7408\n",
      "Confusion Matrix:\n",
      " [[16583   422]\n",
      " [ 1242  2378]]\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Initialize CatBoost\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    depth=4,\n",
    "    learning_rate=0.05,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='F1',\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Train on meta-features\n",
    "cat_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Predict\n",
    "cat_pred = cat_model.predict(X_meta_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"=== Stacking Ensemble (CatBoost) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_meta_test, cat_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_meta_test, cat_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_meta_test, cat_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_meta_test, cat_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, cat_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6477f525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stacking Ensemble (Random Forest) ===\n",
      "Accuracy: 0.9525\n",
      "Precision: 0.8826\n",
      "Recall: 0.8412\n",
      "F1-score: 0.8614\n",
      "Confusion Matrix:\n",
      " [[16600   405]\n",
      " [  575  3045]]\n",
      "0:\tlearn: 0.4973357\ttotal: 14.9ms\tremaining: 7.41s\n",
      "100:\tlearn: 0.6078141\ttotal: 1.35s\tremaining: 5.35s\n",
      "200:\tlearn: 0.6646870\ttotal: 2.69s\tremaining: 4s\n",
      "300:\tlearn: 0.6849326\ttotal: 4.08s\tremaining: 2.69s\n",
      "400:\tlearn: 0.7126731\ttotal: 5.46s\tremaining: 1.35s\n",
      "499:\tlearn: 0.7346923\ttotal: 7.01s\tremaining: 0us\n",
      "=== Stacking Ensemble (CatBoost) ===\n",
      "Accuracy: 0.9193\n",
      "Precision: 0.8493\n",
      "Recall: 0.6569\n",
      "F1-score: 0.7408\n",
      "Confusion Matrix:\n",
      " [[16583   422]\n",
      " [ 1242  2378]]\n",
      "=== Ensemble RF + CatBoost ===\n",
      "Accuracy: 0.9280\n",
      "Precision: 0.9406\n",
      "Recall: 0.6296\n",
      "F1-score: 0.7543\n",
      "Confusion Matrix:\n",
      " [[16861   144]\n",
      " [ 1341  2279]]\n"
     ]
    }
   ],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# # --- Random Forest ---\n",
    "# rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "# rf_model.fit(X_meta_train, y_meta_train)\n",
    "# rf_pred = rf_model.predict(X_meta_test)\n",
    "\n",
    "# print(\"=== Stacking Ensemble (Random Forest) ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, rf_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, rf_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, rf_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, rf_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, rf_pred))\n",
    "\n",
    "\n",
    "# # --- CatBoost ---\n",
    "# cat_model = CatBoostClassifier(\n",
    "#     iterations=500,\n",
    "#     depth=4,\n",
    "#     learning_rate=0.05,\n",
    "#     loss_function='Logloss',\n",
    "#     eval_metric='F1',\n",
    "#     random_seed=42,\n",
    "#     verbose=100\n",
    "# )\n",
    "# cat_model.fit(X_meta_train, y_meta_train)\n",
    "# cat_pred = cat_model.predict(X_meta_test)\n",
    "\n",
    "# print(\"=== Stacking Ensemble (CatBoost) ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, cat_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, cat_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, cat_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, cat_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, cat_pred))\n",
    "\n",
    "\n",
    "# # Optional: combine RF + CatBoost predictions (majority vote)\n",
    "# import numpy as np\n",
    "# ensemble_pred = np.round((rf_pred + cat_pred)/2).astype(int)\n",
    "# print(\"=== Ensemble RF + CatBoost ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, ensemble_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef7548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 454us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 974us/step\n",
      "=== Weighted Ensemble (RF + CatBoost) ===\n",
      "Accuracy: 0.9531\n",
      "Precision: 0.8778\n",
      "Recall: 0.8514\n",
      "F1-score: 0.8644\n",
      "Confusion Matrix:\n",
      " [[16576   429]\n",
      " [  538  3082]]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# from tensorflow.keras import layers, Model\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from scipy.stats import zscore\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# # =========================\n",
    "# # 1️⃣ Base model: Isolation Forest\n",
    "# # =========================\n",
    "# if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "# if_model.fit(X_train[y_train == 0])  # train only on normal samples\n",
    "\n",
    "# if_scores_train = -if_model.decision_function(X_train)\n",
    "# if_scores_test  = -if_model.decision_function(X_test)\n",
    "\n",
    "# # =========================\n",
    "# # 2️⃣ Base model: Autoencoder\n",
    "# # =========================\n",
    "# X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "# X_test_if  = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "# input_dim = X_train_if.shape[1]\n",
    "\n",
    "# # Define AE\n",
    "# input_layer = layers.Input(shape=(input_dim,))\n",
    "# x = layers.Dense(64, activation='relu')(input_layer)\n",
    "# x = layers.Dense(32, activation='relu')(x)\n",
    "# latent = layers.Dense(16, activation='relu')(x)\n",
    "# x = layers.Dense(32, activation='relu')(latent)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "# output = layers.Dense(input_dim, activation='linear')(x)\n",
    "# autoencoder = Model(input_layer, output)\n",
    "# autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Train AE only on normal samples\n",
    "# autoencoder.fit(\n",
    "#     X_train_if[y_train == 0], X_train_if[y_train == 0],\n",
    "#     epochs=40, batch_size=256, validation_split=0.1, verbose=0\n",
    "# )\n",
    "\n",
    "# X_train_pred = autoencoder.predict(X_train_if)\n",
    "# X_test_pred  = autoencoder.predict(X_test_if)\n",
    "# reconstruction_error_train = np.mean(np.square(X_train_if - X_train_pred), axis=1)\n",
    "# reconstruction_error_test  = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "# # =========================\n",
    "# # 3️⃣ Base model: CNN\n",
    "# # =========================\n",
    "# X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "# X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "\n",
    "# input_layer = layers.Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))\n",
    "# x = layers.Conv1D(32, kernel_size=3, activation='relu')(input_layer)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "# x = layers.Conv1D(16, kernel_size=3, activation='relu')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# latent = layers.Dense(16, activation='relu')(x)\n",
    "# output = layers.Dense(1, activation='sigmoid')(latent)\n",
    "\n",
    "# cnn_model = Model(input_layer, output)\n",
    "# cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# cnn_model.fit(\n",
    "#     X_train_cnn[y_train==0], y_train[y_train==0],\n",
    "#     epochs=20, batch_size=256, validation_split=0.1, verbose=0\n",
    "# )\n",
    "\n",
    "# cnn_scores_train = cnn_model.predict(X_train_cnn).flatten()\n",
    "# cnn_scores_test  = cnn_model.predict(X_test_cnn).flatten()\n",
    "\n",
    "# # =========================\n",
    "# # 4️⃣ Base model: Deep SVDD\n",
    "# # =========================\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# class Encoder(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "#         super().__init__()\n",
    "#         layers_list = []\n",
    "#         prev = input_dim\n",
    "#         for h in hidden:\n",
    "#             layers_list.append(torch.nn.Linear(prev, h))\n",
    "#             layers_list.append(torch.nn.ReLU())\n",
    "#             prev = h\n",
    "#         layers_list.append(torch.nn.Linear(prev, out_dim))\n",
    "#         self.net = torch.nn.Sequential(*layers_list)\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# class DeepSVDD:\n",
    "#     def __init__(self, input_dim, c=None, nu=0.1, lr=1e-3):\n",
    "#         self.encoder = Encoder(input_dim).to(device)\n",
    "#         self.c = c\n",
    "#         self.nu = nu\n",
    "#         self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "#         self.criterion = lambda z, c: ((z - c)**2).sum(dim=1)\n",
    "\n",
    "#     def init_center_c(self, loader):\n",
    "#         self.encoder.eval()\n",
    "#         n = 0\n",
    "#         c_sum = None\n",
    "#         with torch.no_grad():\n",
    "#             for x in loader:\n",
    "#                 x = x[0].to(device).float()\n",
    "#                 z = self.encoder(x)\n",
    "#                 c_sum = z.sum(dim=0) if c_sum is None else c_sum + z.sum(dim=0)\n",
    "#                 n += z.size(0)\n",
    "#         c = c_sum / n\n",
    "#         c[(abs(c) < 1e-6)] = 1e-6\n",
    "#         self.c = c\n",
    "\n",
    "#     def train(self, loader, epochs=50):\n",
    "#         if self.c is None:\n",
    "#             self.init_center_c(loader)\n",
    "#         self.encoder.train()\n",
    "#         for ep in range(epochs):\n",
    "#             epoch_loss = 0.0\n",
    "#             for x, in loader:\n",
    "#                 x = x.to(device).float()\n",
    "#                 z = self.encoder(x)\n",
    "#                 dist = self.criterion(z, self.c)\n",
    "#                 loss = dist.mean()\n",
    "#                 self.optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 self.optimizer.step()\n",
    "#                 epoch_loss += loss.item() * x.size(0)\n",
    "\n",
    "#     def score(self, X):\n",
    "#         self.encoder.eval()\n",
    "#         ds = TensorDataset(torch.from_numpy(X).float())\n",
    "#         loader = DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "#         scores = []\n",
    "#         with torch.no_grad():\n",
    "#             for batch in loader:\n",
    "#                 x = batch[0].to(device)\n",
    "#                 z = self.encoder(x)\n",
    "#                 dist = ((z - self.c)**2).sum(dim=1)\n",
    "#                 scores.append(dist.cpu().numpy())\n",
    "#         return np.concatenate(scores)\n",
    "\n",
    "# X_train_normal_np = X_train_if[y_train==0].astype(\"float32\")\n",
    "# loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "# svdd = DeepSVDD(input_dim=input_dim)\n",
    "# svdd.init_center_c(loader)\n",
    "# svdd.train(loader, epochs=50)\n",
    "\n",
    "# svdd_scores_train = svdd.score(X_train_if.astype(\"float32\"))\n",
    "# svdd_scores_test  = svdd.score(X_test_if.astype(\"float32\"))\n",
    "\n",
    "# # =========================\n",
    "# # 5️⃣ Stack meta-features\n",
    "# # =========================\n",
    "# X_meta_train = np.vstack([if_scores_train, reconstruction_error_train, svdd_scores_train, cnn_scores_train]).T\n",
    "# X_meta_test  = np.vstack([if_scores_test, reconstruction_error_test, svdd_scores_test, cnn_scores_test]).T\n",
    "\n",
    "# y_meta_train = y_train\n",
    "# y_meta_test  = y_test\n",
    "\n",
    "# # =========================\n",
    "# # 6️⃣ Train meta-models: RF + CatBoost\n",
    "# # =========================\n",
    "# rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "# cat_model = CatBoostClassifier(n_estimators=500, learning_rate=0.1, verbose=0, random_state=42)\n",
    "\n",
    "# rf_model.fit(X_meta_train, y_meta_train)\n",
    "# cat_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# # =========================\n",
    "# # 7️⃣ Weighted ensemble + threshold tuning\n",
    "# # =========================\n",
    "# rf_probs  = rf_model.predict_proba(X_meta_test)[:,1]\n",
    "# cat_probs = cat_model.predict_proba(X_meta_test)[:,1]\n",
    "\n",
    "# # Weighting\n",
    "# w_rf, w_cat = 0.6, 0.4\n",
    "# ensemble_probs = w_rf * rf_probs + w_cat * cat_probs\n",
    "\n",
    "# # Threshold tuning\n",
    "# precision, recall, thresholds = precision_recall_curve(y_meta_test, ensemble_probs)\n",
    "# f1_scores = 2 * (precision*recall)/(precision+recall+1e-8)\n",
    "# best_thresh = thresholds[f1_scores.argmax()]\n",
    "\n",
    "# ensemble_pred = (ensemble_probs >= best_thresh).astype(int)\n",
    "\n",
    "# # =========================\n",
    "# # 8️⃣ Evaluate\n",
    "# # =========================\n",
    "# print(\"=== Weighted Ensemble (RF + CatBoost) ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, ensemble_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "28249f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 408us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439us/step\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 868us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 943us/step\n",
      "DeepSVDD epoch 0/50 loss: 0.022014\n",
      "DeepSVDD epoch 5/50 loss: 0.000211\n",
      "DeepSVDD epoch 10/50 loss: 0.000068\n",
      "DeepSVDD epoch 15/50 loss: 0.000029\n",
      "DeepSVDD epoch 20/50 loss: 0.000016\n",
      "DeepSVDD epoch 25/50 loss: 0.000009\n",
      "DeepSVDD epoch 30/50 loss: 0.000006\n",
      "DeepSVDD epoch 35/50 loss: 0.000004\n",
      "DeepSVDD epoch 40/50 loss: 0.000003\n",
      "DeepSVDD epoch 45/50 loss: 0.000003\n",
      "=== Weighted XGBoost Stacked Ensemble ===\n",
      "Accuracy: 0.9737\n",
      "Precision: 0.9126\n",
      "Recall: 0.9403\n",
      "F1-score: 0.9263\n",
      "Confusion Matrix:\n",
      " [[16679   326]\n",
      " [  216  3404]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import xgboost as xgb\n",
    "\n",
    "# =========================\n",
    "# 1️⃣ Base models: Isolation Forest + Autoencoder\n",
    "# =========================\n",
    "\n",
    "# Isolation Forest\n",
    "if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "if_model.fit(X_train[y_train==0])\n",
    "if_scores_train = -if_model.decision_function(X_train)\n",
    "if_scores_test  = -if_model.decision_function(X_test)\n",
    "\n",
    "# Autoencoder (MLP)\n",
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if  = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "input_dim = X_train_if.shape[1]\n",
    "\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "x = layers.Dense(64, activation='relu')(input_layer)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(latent)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "output = layers.Dense(input_dim, activation='linear')(x)\n",
    "autoencoder = Model(input_layer, output)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_train_if[y_train==0], X_train_if[y_train==0], epochs=40, batch_size=256, validation_split=0.1, verbose=0)\n",
    "\n",
    "X_train_pred = autoencoder.predict(X_train_if)\n",
    "X_test_pred  = autoencoder.predict(X_test_if)\n",
    "recon_error_train = np.mean((X_train_if - X_train_pred)**2, axis=1)\n",
    "recon_error_test  = np.mean((X_test_if - X_test_pred)**2, axis=1)\n",
    "\n",
    "# Z-score normalization\n",
    "if_scores_train_z = (if_scores_train - if_scores_train.mean()) / if_scores_train.std()\n",
    "recon_error_train_z = (recon_error_train - recon_error_train.mean()) / recon_error_train.std()\n",
    "if_scores_test_z  = (if_scores_test - if_scores_test.mean()) / if_scores_test.std()\n",
    "recon_error_test_z  = (recon_error_test - recon_error_test.mean()) / recon_error_test.std()\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ CNN base model\n",
    "# =========================\n",
    "X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "y_train_cnn = y_train\n",
    "y_test_cnn  = y_test\n",
    "\n",
    "input_layer = layers.Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))\n",
    "x = layers.Conv1D(32, 3, activation='relu')(input_layer)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Conv1D(16, 3, activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "output = layers.Dense(1, activation='sigmoid')(latent)\n",
    "cnn_model = Model(input_layer, output)\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.fit(X_train_cnn[y_train==0], y_train_cnn[y_train==0], epochs=20, batch_size=256, validation_split=0.1, verbose=0)\n",
    "\n",
    "cnn_scores_train = cnn_model.predict(X_train_cnn).flatten()\n",
    "cnn_scores_test  = cnn_model.predict(X_test_cnn).flatten()\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ DeepSVDD\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "        super().__init__()\n",
    "        layers_list = []\n",
    "        prev = input_dim\n",
    "        for h in hidden:\n",
    "            layers_list.append(torch.nn.Linear(prev,h))\n",
    "            layers_list.append(torch.nn.ReLU())\n",
    "            prev=h\n",
    "        layers_list.append(torch.nn.Linear(prev,out_dim))\n",
    "        self.net = torch.nn.Sequential(*layers_list)\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DeepSVDD:\n",
    "    def __init__(self, input_dim, c=None, lr=1e-3):\n",
    "        self.encoder = Encoder(input_dim).to(device)\n",
    "        self.c = c\n",
    "        self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "        self.criterion = lambda z,c: ((z-c)**2).sum(dim=1)\n",
    "    def init_center_c(self, loader):\n",
    "        self.encoder.eval()\n",
    "        n = 0\n",
    "        c_sum=None\n",
    "        with torch.no_grad():\n",
    "            for x in loader:\n",
    "                x=x[0].to(device).float()\n",
    "                z=self.encoder(x)\n",
    "                c_sum = z.sum(dim=0) if c_sum is None else c_sum+z.sum(dim=0)\n",
    "                n+=z.size(0)\n",
    "        c = c_sum/n\n",
    "        c[abs(c)<1e-6]=1e-6\n",
    "        self.c=c\n",
    "    def train(self, loader, epochs=50):\n",
    "        if self.c is None: self.init_center_c(loader)\n",
    "        self.encoder.train()\n",
    "        for ep in range(epochs):\n",
    "            epoch_loss=0\n",
    "            for x, in loader:\n",
    "                x=x.to(device).float()\n",
    "                z=self.encoder(x)\n",
    "                dist = self.criterion(z,self.c)\n",
    "                loss=dist.mean()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()*x.size(0)\n",
    "            if ep%5==0:\n",
    "                print(f\"DeepSVDD epoch {ep}/{epochs} loss: {epoch_loss/len(loader.dataset):.6f}\")\n",
    "    def score(self,X):\n",
    "        self.encoder.eval()\n",
    "        ds=TensorDataset(torch.from_numpy(X).float())\n",
    "        loader=DataLoader(ds,batch_size=1024,shuffle=False)\n",
    "        scores=[]\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                x=batch[0].to(device)\n",
    "                z=self.encoder(x)\n",
    "                dist=((z-self.c)**2).sum(dim=1)\n",
    "                scores.append(dist.cpu().numpy())\n",
    "        return np.concatenate(scores)\n",
    "\n",
    "X_train_normal_np = X_train_if[y_train==0].astype(\"float32\")\n",
    "loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "svdd = DeepSVDD(input_dim=input_dim)\n",
    "svdd.init_center_c(loader)\n",
    "svdd.train(loader, epochs=50)\n",
    "\n",
    "svdd_scores_train = svdd.score(X_train_if.astype(\"float32\"))\n",
    "svdd_scores_test  = svdd.score(X_test_if.astype(\"float32\"))\n",
    "\n",
    "# =========================\n",
    "# 4️⃣ Tree-based base models: RF + CatBoost\n",
    "# =========================\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_scores_train = rf_model.predict_proba(X_train)[:,1]\n",
    "rf_scores_test  = rf_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "cb_model = CatBoostClassifier(n_estimators=500, learning_rate=0.1, depth=4, verbose=0)\n",
    "cb_model.fit(X_train, y_train)\n",
    "cb_scores_train = cb_model.predict_proba(X_train)[:,1]\n",
    "cb_scores_test  = cb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# =========================\n",
    "# 5️⃣ Stack all base model outputs\n",
    "# =========================\n",
    "X_meta_train = np.vstack([\n",
    "    if_scores_train_z, recon_error_train_z, cnn_scores_train,\n",
    "    svdd_scores_train, rf_scores_train, cb_scores_train\n",
    "]).T\n",
    "\n",
    "X_meta_test = np.vstack([\n",
    "    if_scores_test_z, recon_error_test_z, cnn_scores_test,\n",
    "    svdd_scores_test, rf_scores_test, cb_scores_test\n",
    "]).T\n",
    "\n",
    "# =========================\n",
    "# 6️⃣ Weighted XGBoost as meta-layer\n",
    "# =========================\n",
    "meta_model = xgb.XGBClassifier(\n",
    "    n_estimators=300, max_depth=3, learning_rate=0.1, random_state=42,\n",
    "    scale_pos_weight=(y_train==0).sum()/(y_train==1).sum()\n",
    ")\n",
    "meta_model.fit(X_meta_train, y_train)\n",
    "meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "print(\"=== Weighted XGBoost Stacked Ensemble ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, meta_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, meta_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, meta_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test, meta_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, meta_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a0935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c702a11e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling FullDiffModel.call().\n\n\u001b[1mInput 0 of layer \"functional_46\" is incompatible with the layer: expected shape=(None, 63), found shape=(20625, 62)\u001b[0m\n\nArguments received by FullDiffModel.call():\n  • x=tf.Tensor(shape=(20625, 62), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m adv\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     89\u001b[0m epsilon_fgsm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\n\u001b[1;32m---> 90\u001b[0m adv_fgsm \u001b[38;5;241m=\u001b[39m \u001b[43mfgsm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiff_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_fgsm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m fgsm_features \u001b[38;5;241m=\u001b[39m extract_all_features(adv_fgsm)\n\u001b[0;32m     93\u001b[0m fgsm_preds \u001b[38;5;241m=\u001b[39m meta_classifier\u001b[38;5;241m.\u001b[39mpredict(fgsm_features)\n",
      "Cell \u001b[1;32mIn[123], line 81\u001b[0m, in \u001b[0;36mfgsm\u001b[1;34m(model, x, epsilon)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     80\u001b[0m     tape\u001b[38;5;241m.\u001b[39mwatch(x)\n\u001b[1;32m---> 81\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(preds)   \u001b[38;5;66;03m# maximize anomaly score\u001b[39;00m\n\u001b[0;32m     84\u001b[0m grad \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[123], line 31\u001b[0m, in \u001b[0;36mFullDiffModel.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# AE reconstruction (no latent needed)\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# CNN features on reconstructed / cleaned signal\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     z2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn(z1)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling FullDiffModel.call().\n\n\u001b[1mInput 0 of layer \"functional_46\" is incompatible with the layer: expected shape=(None, 63), found shape=(20625, 62)\u001b[0m\n\nArguments received by FullDiffModel.call():\n  • x=tf.Tensor(shape=(20625, 62), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ============================================================\n",
    "# 0. PREP: Models you already trained\n",
    "# ============================================================\n",
    "\n",
    "ae_model = autoencoder        # your AE (reconstruction model)\n",
    "cnn_model = cnn_model         # your CNN feature extractor\n",
    "svdd_model = svdd             # your DeepSVDD\n",
    "if_model = if_model         # Isolation Forest\n",
    "meta_classifier = meta_model  # your XGBoost/stacked classifier\n",
    "\n",
    "X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. DIFFERENTIABLE PIPELINE FOR ATTACK: AE → CNN → SVDD\n",
    "# ============================================================\n",
    "\n",
    "class FullDiffModel(tf.keras.Model):\n",
    "    def __init__(self, ae, cnn, svdd):\n",
    "        super().__init__()\n",
    "        self.ae = ae\n",
    "        self.cnn = cnn\n",
    "        self.svdd = svdd\n",
    "\n",
    "    def call(self, x):\n",
    "        # AE reconstruction (no latent needed)\n",
    "        z1 = self.ae(x)\n",
    "\n",
    "        # CNN features on reconstructed / cleaned signal\n",
    "        z2 = self.cnn(z1)\n",
    "\n",
    "        # DeepSVDD anomaly score\n",
    "        out = self.svdd(z2)\n",
    "        return out\n",
    "\n",
    "diff_model = FullDiffModel(ae_model, cnn_model, svdd_model)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. FEATURE EXTRACTOR FOR PASSING TO ENSEMBLE\n",
    "# ============================================================\n",
    "\n",
    "def extract_all_features(x_np):\n",
    "    \"\"\"\n",
    "    Takes numpy input and extracts:\n",
    "    - AE reconstruction error\n",
    "    - CNN features\n",
    "    - DeepSVDD output\n",
    "    - Isolation Forest score\n",
    "    Returns a horizontal concatenation.\n",
    "    \"\"\"\n",
    "    x_tf = tf.convert_to_tensor(x_np, dtype=tf.float32)\n",
    "\n",
    "    # --- AE Reconstruction Error ---\n",
    "    recon = ae_model(x_tf)\n",
    "    ae_err = tf.reduce_mean(tf.abs(recon - x_tf), axis=1).numpy().reshape(-1, 1)\n",
    "\n",
    "    # --- CNN Features ---\n",
    "    cnn_feat = cnn_model(x_tf).numpy()\n",
    "\n",
    "    # --- DeepSVDD scores ---\n",
    "    svdd_scores = svdd_model(cnn_feat).numpy().reshape(-1, 1)\n",
    "\n",
    "    # --- Isolation Forest ---\n",
    "    if_scores = if_model.decision_function(x_np).reshape(-1, 1)\n",
    "\n",
    "    return np.concatenate([ae_err, cnn_feat, svdd_scores, if_scores], axis=1)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. FGSM ATTACK\n",
    "# ============================================================\n",
    "\n",
    "def fgsm(model, x, epsilon):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        preds = model(x)\n",
    "        loss = tf.reduce_mean(preds)   # maximize anomaly score\n",
    "\n",
    "    grad = tape.gradient(loss, x)\n",
    "    adv = x + epsilon * tf.sign(grad)\n",
    "    adv = tf.clip_by_value(adv, 0, 1)\n",
    "    return adv.numpy()\n",
    "\n",
    "epsilon_fgsm = 0.05\n",
    "adv_fgsm = fgsm(diff_model, X_test_tensor, epsilon_fgsm)\n",
    "\n",
    "fgsm_features = extract_all_features(adv_fgsm)\n",
    "fgsm_preds = meta_classifier.predict(fgsm_features)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. MI-FGSM ATTACK\n",
    "# ============================================================\n",
    "\n",
    "def mi_fgsm(model, x, epsilon, steps=10, decay=1.0):\n",
    "    alpha = epsilon / steps\n",
    "    x_adv = tf.identity(x)\n",
    "    momentum = 0\n",
    "\n",
    "    for _ in range(steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_adv)\n",
    "            preds = model(x_adv)\n",
    "            loss = tf.reduce_mean(preds)\n",
    "\n",
    "        grad = tape.gradient(loss, x_adv)\n",
    "        grad_norm = grad / (tf.reduce_mean(tf.abs(grad)) + 1e-8)\n",
    "\n",
    "        momentum = decay * momentum + grad_norm\n",
    "        x_adv = x_adv + alpha * tf.sign(momentum)\n",
    "        x_adv = tf.clip_by_value(x_adv, 0, 1)\n",
    "\n",
    "    return x_adv.numpy()\n",
    "\n",
    "epsilon_mi = 0.05\n",
    "adv_mi = mi_fgsm(diff_model, X_test_tensor, epsilon_mi, steps=10)\n",
    "\n",
    "mi_features = extract_all_features(adv_mi)\n",
    "mi_preds = meta_classifier.predict(mi_features)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. FINAL RESULTS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=== FGSM on Full Ensemble ===\")\n",
    "print(classification_report(y_test, fgsm_preds))\n",
    "\n",
    "print(\"\\n=== MI-FGSM on Full Ensemble ===\")\n",
    "print(classification_report(y_test, mi_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4ae1a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. FGSM Attack Function ---\n",
    "def fgsm_attack(model, X, y, epsilon, loss_fn):\n",
    "    \"\"\"\n",
    "    Perform single-step FGSM attack.\n",
    "    \n",
    "    Args:\n",
    "        model: Differentiable PyTorch model (e.g., your autoencoder or CNN).\n",
    "        X: Clean input features (torch.Tensor).\n",
    "        y: True labels (for targeted/untargeted loss calculation).\n",
    "        epsilon: Perturbation magnitude (ϵ).\n",
    "        loss_fn: Loss function (e.g., nn.CrossEntropyLoss for classifier, nn.MSELoss for AE).\n",
    "    \n",
    "    Returns:\n",
    "        X_adv: Adversarial examples.\n",
    "        perturbation: The added noise (X_adv - X).\n",
    "    \"\"\"\n",
    "    X.requires_grad = True  # Track gradients wrt input\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    # Calculate loss. For an autoencoder, you might use reconstruction loss.\n",
    "    # For a classifier, use classification loss wrt true label y.\n",
    "    loss = loss_fn(outputs, y)\n",
    "    \n",
    "    # Zero existing gradients, then backpropagate\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Collect the sign of the data gradient\n",
    "    data_grad = X.grad.data\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    \n",
    "    # Create the perturbed example\n",
    "    perturbation = epsilon * sign_data_grad\n",
    "    X_adv = X + perturbation\n",
    "    \n",
    "    # Optional: Clip to maintain valid data range (e.g., min/max of original features)\n",
    "    # X_adv = torch.clamp(X_adv, X.min(), X.max())\n",
    "    \n",
    "    return X_adv.detach(), perturbation.detach()\n",
    "\n",
    "# --- 2. MI-FGSM Attack Function ---\n",
    "def mi_fgsm_attack(model, X, y, epsilon, num_iter, decay_factor=1.0, loss_fn=nn.MSELoss()):\n",
    "    \"\"\"\n",
    "    Perform multi-step Momentum Iterative FGSM attack.\n",
    "    \n",
    "    Args:\n",
    "        model: Differentiable PyTorch model.\n",
    "        X: Clean input features.\n",
    "        y: True labels.\n",
    "        epsilon: Total perturbation budget (max L-inf norm).\n",
    "        num_iter: Number of attack iterations.\n",
    "        decay_factor: Momentum decay factor (μ). Often set to 1.0.\n",
    "        loss_fn: Loss function.\n",
    "    \n",
    "    Returns:\n",
    "        X_adv: Final adversarial examples.\n",
    "    \"\"\"\n",
    "    alpha = epsilon / num_iter  # Step size per iteration\n",
    "    X_adv = X.clone().detach()\n",
    "    momentum = torch.zeros_like(X)\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        X_adv.requires_grad = True\n",
    "        \n",
    "        outputs = model(X_adv)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        grad = X_adv.grad.data\n",
    "        # Normalize the gradient\n",
    "        grad = grad / (torch.mean(torch.abs(grad), dim=(1,2,3), keepdim=True) + 1e-12)\n",
    "        # Update momentum\n",
    "        momentum = decay_factor * momentum + grad\n",
    "        # Update adversarial example\n",
    "        X_adv = X_adv.detach() + alpha * momentum.sign()\n",
    "        \n",
    "        # Project back to the epsilon-ball around the original input X\n",
    "        delta = torch.clamp(X_adv - X, min=-epsilon, max=epsilon)\n",
    "        X_adv = torch.clamp(X + delta, X.min(), X.max()).detach()\n",
    "    \n",
    "    return X_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fb3e6f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "class ComponentEnsembleAttacker:\n",
    "    \"\"\"\n",
    "    Attack individual ensemble components and evaluate impact on final ensemble.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ensemble_model, base_models, scaler, feature_names):\n",
    "        self.ensemble = ensemble_model\n",
    "        self.base_models = base_models\n",
    "        self.scaler = scaler\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        # Component importance (which components to prioritize)\n",
    "        self.component_names = ['IsolationForest', 'Autoencoder', 'CNN', \n",
    "                               'DeepSVDD', 'RandomForest', 'CatBoost']\n",
    "    \n",
    "    def attack_single_component(self, X, y, target_component, \n",
    "                               attack_type='fgsm', epsilon=0.1, targeted=False):\n",
    "        \"\"\"\n",
    "        Attack a specific component and regenerate meta-features.\n",
    "        \n",
    "        Args:\n",
    "            X: Original features\n",
    "            y: True labels\n",
    "            target_component: Component to attack ('if', 'ae', 'cnn', etc.)\n",
    "            attack_type: 'fgsm' or 'mi_fgsm'\n",
    "            epsilon: Attack strength\n",
    "            targeted: Targeted or untargeted attack\n",
    "            \n",
    "        Returns:\n",
    "            poisoned_meta_features: Meta-features with one poisoned component\n",
    "            attack_success: Success rate on the component\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ATTACKING COMPONENT: {target_component.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Get clean meta-features first\n",
    "        clean_meta = self._get_meta_features(X)\n",
    "        \n",
    "        # Get predictions from targeted component\n",
    "        component = self.base_models[target_component]\n",
    "        \n",
    "        if target_component == 'if':\n",
    "            # Attack Isolation Forest scores\n",
    "            clean_scores = -component.decision_function(X)\n",
    "            attacked_scores = self._attack_if_scores(clean_scores, epsilon, targeted)\n",
    "            poisoned_meta = clean_meta.copy()\n",
    "            poisoned_meta[:, 0] = (attacked_scores - attacked_scores.mean()) / (attacked_scores.std() + 1e-8)\n",
    "            \n",
    "        elif target_component == 'ae':\n",
    "            # Attack autoencoder reconstruction error\n",
    "            if_scores = -self.base_models['if'].decision_function(X)\n",
    "            X_if = np.hstack([X, if_scores.reshape(-1, 1)])\n",
    "            \n",
    "            clean_pred = component.predict(X_if)\n",
    "            clean_error = np.mean((X_if - clean_pred)**2, axis=1)\n",
    "            \n",
    "            # Add noise to reconstruction error\n",
    "            noise = epsilon * np.random.randn(len(clean_error))\n",
    "            if targeted:\n",
    "                # Make errors smaller (look more normal)\n",
    "                attacked_error = np.maximum(clean_error - np.abs(noise), 0.001)\n",
    "            else:\n",
    "                # Make errors larger (look more anomalous)\n",
    "                attacked_error = clean_error + np.abs(noise)\n",
    "            \n",
    "            poisoned_meta = clean_meta.copy()\n",
    "            poisoned_meta[:, 1] = (attacked_error - attacked_error.mean()) / (attacked_error.std() + 1e-8)\n",
    "            \n",
    "        elif target_component == 'cnn':\n",
    "            # Attack CNN predictions\n",
    "            if_scores = -self.base_models['if'].decision_function(X)\n",
    "            X_if = np.hstack([X, if_scores.reshape(-1, 1)])\n",
    "            X_cnn = X_if.reshape(X_if.shape[0], X_if.shape[1], 1)\n",
    "            \n",
    "            clean_cnn_pred = component.predict(X_cnn).flatten()\n",
    "            \n",
    "            # Apply perturbation to CNN outputs\n",
    "            if targeted:\n",
    "                # Flip predictions (1->0 or 0->1)\n",
    "                attacked_pred = 1 - clean_cnn_pred\n",
    "            else:\n",
    "                # Add noise\n",
    "                noise = epsilon * np.random.randn(len(clean_cnn_pred))\n",
    "                attacked_pred = np.clip(clean_cnn_pred + noise, 0, 1)\n",
    "            \n",
    "            poisoned_meta = clean_meta.copy()\n",
    "            poisoned_meta[:, 2] = attacked_pred\n",
    "            \n",
    "        elif target_component in ['rf', 'cb']:\n",
    "            # Attack tree-based model probabilities\n",
    "            clean_proba = component.predict_proba(X)[:, 1]\n",
    "            \n",
    "            if targeted:\n",
    "                # Reverse confidence\n",
    "                attacked_proba = 1 - clean_proba\n",
    "            else:\n",
    "                # Add noise to probabilities\n",
    "                noise = epsilon * np.random.randn(len(clean_proba))\n",
    "                attacked_proba = np.clip(clean_proba + noise, 0, 1)\n",
    "            \n",
    "            poisoned_meta = clean_meta.copy()\n",
    "            if target_component == 'rf':\n",
    "                poisoned_meta[:, 4] = attacked_proba\n",
    "            else:\n",
    "                poisoned_meta[:, 5] = attacked_proba\n",
    "        \n",
    "        elif target_component == 'svdd':\n",
    "            # Attack DeepSVDD anomaly scores\n",
    "            if_scores = -self.base_models['if'].decision_function(X)\n",
    "            X_if = np.hstack([X, if_scores.reshape(-1, 1)])\n",
    "            \n",
    "            clean_svdd_scores = component.score(X_if.astype(\"float32\"))\n",
    "            \n",
    "            # Add noise to anomaly scores\n",
    "            noise = epsilon * np.random.randn(len(clean_svdd_scores))\n",
    "            if targeted:\n",
    "                # Make anomalies look normal (lower scores)\n",
    "                attacked_scores = np.maximum(clean_svdd_scores - np.abs(noise), 0.001)\n",
    "            else:\n",
    "                # Make normals look anomalous (higher scores)\n",
    "                attacked_scores = clean_svdd_scores + np.abs(noise)\n",
    "            \n",
    "            poisoned_meta = clean_meta.copy()\n",
    "            poisoned_meta[:, 3] = (attacked_scores - attacked_scores.mean()) / (attacked_scores.std() + 1e-8)\n",
    "        \n",
    "        # Calculate attack success on the component\n",
    "        component_success = self._evaluate_component_attack(clean_meta, poisoned_meta, \n",
    "                                                           target_component, targeted)\n",
    "        \n",
    "        return poisoned_meta, component_success\n",
    "    \n",
    "    def _attack_if_scores(self, clean_scores, epsilon, targeted):\n",
    "        \"\"\"Attack Isolation Forest anomaly scores\"\"\"\n",
    "        if targeted:\n",
    "            # Make anomalies look normal (decrease scores)\n",
    "            attacked_scores = clean_scores * (1 - epsilon)\n",
    "        else:\n",
    "            # Make normals look anomalous (increase scores)\n",
    "            attacked_scores = clean_scores * (1 + epsilon)\n",
    "        \n",
    "        return attacked_scores\n",
    "    \n",
    "    def _evaluate_component_attack(self, clean_meta, poisoned_meta, \n",
    "                                  component_idx, targeted):\n",
    "        \"\"\"Evaluate how much the component output changed\"\"\"\n",
    "        comp_idx_map = {'if': 0, 'ae': 1, 'cnn': 2, 'svdd': 3, 'rf': 4, 'cb': 5}\n",
    "        idx = comp_idx_map[component_idx]\n",
    "        \n",
    "        clean_comp = clean_meta[:, idx]\n",
    "        poisoned_comp = poisoned_meta[:, idx]\n",
    "        \n",
    "        # Calculate change magnitude\n",
    "        change_magnitude = np.mean(np.abs(poisoned_comp - clean_comp))\n",
    "        change_percentage = change_magnitude / (np.mean(np.abs(clean_comp)) + 1e-8)\n",
    "        \n",
    "        print(f\"  Component output changed by: {change_percentage:.2%}\")\n",
    "        return change_percentage\n",
    "    \n",
    "    def cascade_attack_analysis(self, X, y, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Test attacking each component individually and measure ensemble impact.\n",
    "        \n",
    "        Returns DataFrame with results.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Get clean ensemble predictions\n",
    "        clean_meta = self._get_meta_features(X)\n",
    "        clean_pred = self.ensemble.predict(clean_meta)\n",
    "        clean_acc = accuracy_score(y, clean_pred)\n",
    "        \n",
    "        print(f\"Clean ensemble accuracy: {clean_acc:.4f}\")\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"CASCADE ATTACK ANALYSIS - Individual Component Attacks\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Test each component\n",
    "        for comp_name in self.base_models.keys():\n",
    "            # Attack this component\n",
    "            poisoned_meta, comp_change = self.attack_single_component(\n",
    "                X, y, comp_name, epsilon=epsilon, targeted=False\n",
    "            )\n",
    "            \n",
    "            # Get ensemble predictions with poisoned component\n",
    "            poisoned_pred = self.ensemble.predict(poisoned_meta)\n",
    "            poisoned_acc = accuracy_score(y, poisoned_pred)\n",
    "            \n",
    "            # Calculate impact\n",
    "            accuracy_drop = clean_acc - poisoned_acc\n",
    "            flip_rate = (poisoned_pred != clean_pred).mean()\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'component': comp_name.upper(),\n",
    "                'clean_accuracy': clean_acc,\n",
    "                'poisoned_accuracy': poisoned_acc,\n",
    "                'accuracy_drop': accuracy_drop,\n",
    "                'prediction_flip_rate': flip_rate,\n",
    "                'component_change': comp_change\n",
    "            })\n",
    "            \n",
    "            print(f\"  Ensemble accuracy after attack: {poisoned_acc:.4f}\")\n",
    "            print(f\"  Accuracy drop: {accuracy_drop:.4f}\")\n",
    "            print(f\"  Prediction flip rate: {flip_rate:.4f}\")\n",
    "            print(f\"{'-'*40}\")\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df = results_df.sort_values('accuracy_drop', ascending=False)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def coordinated_attack(self, X, y, target_components=None, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Coordinate attack on multiple components simultaneously.\n",
    "        \"\"\"\n",
    "        if target_components is None:\n",
    "            # Attack all components\n",
    "            target_components = list(self.base_models.keys())\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"COORDINATED ATTACK on {len(target_components)} components\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Start with clean meta-features\n",
    "        poisoned_meta = self._get_meta_features(X)\n",
    "        \n",
    "        # Attack each target component\n",
    "        for comp in target_components:\n",
    "            print(f\"\\nAttacking {comp}...\")\n",
    "            \n",
    "            # Create temporary poisoned meta for this component\n",
    "            comp_poisoned, _ = self.attack_single_component(\n",
    "                X, y, comp, epsilon=epsilon, targeted=False\n",
    "            )\n",
    "            \n",
    "            # Update only this component's column\n",
    "            comp_idx_map = {'if': 0, 'ae': 1, 'cnn': 2, 'svdd': 3, 'rf': 4, 'cb': 5}\n",
    "            if comp in comp_idx_map:\n",
    "                idx = comp_idx_map[comp]\n",
    "                poisoned_meta[:, idx] = comp_poisoned[:, idx]\n",
    "        \n",
    "        # Evaluate coordinated attack\n",
    "        clean_meta = self._get_meta_features(X)\n",
    "        clean_pred = self.ensemble.predict(clean_meta)\n",
    "        clean_acc = accuracy_score(y, clean_pred)\n",
    "        \n",
    "        poisoned_pred = self.ensemble.predict(poisoned_meta)\n",
    "        poisoned_acc = accuracy_score(y, poisoned_pred)\n",
    "        \n",
    "        print(f\"\\nCoordinated Attack Results:\")\n",
    "        print(f\"  Clean accuracy: {clean_acc:.4f}\")\n",
    "        print(f\"  Poisoned accuracy: {poisoned_acc:.4f}\")\n",
    "        print(f\"  Accuracy drop: {clean_acc - poisoned_acc:.4f}\")\n",
    "        print(f\"  Prediction flip rate: {(poisoned_pred != clean_pred).mean():.4f}\")\n",
    "        \n",
    "        return poisoned_meta, poisoned_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "db80735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thesis_component_attack_experiments(X_test, y_test, attacker):\n",
    "    \"\"\"\n",
    "    Comprehensive component attack experiments for your thesis.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Experiment 1: Individual component vulnerability\n",
    "    print(\"EXPERIMENT 1: Individual Component Attacks\")\n",
    "    individual_results = attacker.cascade_attack_analysis(\n",
    "        X_test[:1000], y_test[:1000], epsilon=0.15\n",
    "    )\n",
    "    \n",
    "    # Experiment 2: Attack the most vulnerable component\n",
    "    most_vulnerable = individual_results.iloc[0]['component'].lower()\n",
    "    print(f\"\\n\\nEXPERIMENT 2: Focused Attack on Most Vulnerable Component ({most_vulnerable})\")\n",
    "    \n",
    "    focused_results = []\n",
    "    for eps in [0.05, 0.1, 0.2, 0.3, 0.5]:\n",
    "        poisoned_meta, _ = attacker.attack_single_component(\n",
    "            X_test[:500], y_test[:500], \n",
    "            target_component=most_vulnerable,\n",
    "            epsilon=eps,\n",
    "            targeted=False\n",
    "        )\n",
    "        \n",
    "        poisoned_pred = attacker.ensemble.predict(poisoned_meta)\n",
    "        clean_meta = attacker._get_meta_features(X_test[:500])\n",
    "        clean_pred = attacker.ensemble.predict(clean_meta)\n",
    "        \n",
    "        flip_rate = (poisoned_pred != clean_pred).mean()\n",
    "        focused_results.append({\n",
    "            'epsilon': eps,\n",
    "            'flip_rate': flip_rate,\n",
    "            'component': most_vulnerable\n",
    "        })\n",
    "    \n",
    "    # Experiment 3: Strategic component combination attacks\n",
    "    print(\"\\n\\nEXPERIMENT 3: Strategic Component Combinations\")\n",
    "    \n",
    "    # Try different combinations\n",
    "    combinations = [\n",
    "        ['if', 'ae'],           # Anomaly detection components\n",
    "        ['rf', 'cb'],           # Tree-based components\n",
    "        ['cnn', 'svdd'],        # Deep learning components\n",
    "        ['if', 'rf', 'cnn'],    # One from each category\n",
    "    ]\n",
    "    \n",
    "    combo_results = []\n",
    "    for combo in combinations:\n",
    "        poisoned_meta, poisoned_pred = attacker.coordinated_attack(\n",
    "            X_test[:500], y_test[:500],\n",
    "            target_components=combo,\n",
    "            epsilon=0.1\n",
    "        )\n",
    "        \n",
    "        clean_meta = attacker._get_meta_features(X_test[:500])\n",
    "        clean_pred = attacker.ensemble.predict(clean_meta)\n",
    "        \n",
    "        combo_results.append({\n",
    "            'components': '+'.join(combo),\n",
    "            'flip_rate': (poisoned_pred != clean_pred).mean(),\n",
    "            'num_components': len(combo)\n",
    "        })\n",
    "    \n",
    "    # Experiment 4: Class-specific vulnerability\n",
    "    print(\"\\n\\nEXPERIMENT 4: Class-Specific Component Attacks\")\n",
    "    \n",
    "    class_results = {}\n",
    "    unique_classes = np.unique(y_test)\n",
    "    \n",
    "    for class_id in unique_classes:\n",
    "        class_mask = y_test == class_id\n",
    "        X_class = X_test[class_mask][:200]\n",
    "        y_class = y_test[class_mask][:200]\n",
    "        \n",
    "        if len(X_class) > 0:\n",
    "            class_results[class_id] = attacker.cascade_attack_analysis(\n",
    "                X_class, y_class, epsilon=0.1\n",
    "            )\n",
    "    \n",
    "    return {\n",
    "        'individual_results': individual_results,\n",
    "        'focused_attack': pd.DataFrame(focused_results),\n",
    "        'combination_attacks': pd.DataFrame(combo_results),\n",
    "        'class_specific': class_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0ddfecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_component_attacks(results):\n",
    "    \"\"\"\n",
    "    Create visualizations for thesis.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Individual component vulnerability\n",
    "    ax1 = axes[0, 0]\n",
    "    individual_df = results['individual_results']\n",
    "    bars = ax1.bar(individual_df['component'], individual_df['accuracy_drop'])\n",
    "    ax1.set_title('Individual Component Vulnerability', fontsize=14)\n",
    "    ax1.set_ylabel('Accuracy Drop', fontsize=12)\n",
    "    ax1.set_xlabel('Component', fontsize=12)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Color bars by impact\n",
    "    for i, bar in enumerate(bars):\n",
    "        if individual_df.iloc[i]['accuracy_drop'] > 0.1:\n",
    "            bar.set_color('red')\n",
    "        elif individual_df.iloc[i]['accuracy_drop'] > 0.05:\n",
    "            bar.set_color('orange')\n",
    "        else:\n",
    "            bar.set_color('blue')\n",
    "    \n",
    "    # 2. Epsilon vs Attack Success\n",
    "    ax2 = axes[0, 1]\n",
    "    focused_df = results['focused_attack']\n",
    "    ax2.plot(focused_df['epsilon'], focused_df['flip_rate'], 'b-o', linewidth=2)\n",
    "    ax2.fill_between(focused_df['epsilon'], 0, focused_df['flip_rate'], alpha=0.2)\n",
    "    ax2.set_title('Attack Strength vs. Success Rate', fontsize=14)\n",
    "    ax2.set_xlabel('Epsilon (Attack Strength)', fontsize=12)\n",
    "    ax2.set_ylabel('Prediction Flip Rate', fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Component combination effectiveness\n",
    "    ax3 = axes[1, 0]\n",
    "    combo_df = results['combination_attacks']\n",
    "    colors = ['red', 'blue', 'green', 'purple']\n",
    "    bars = ax3.bar(range(len(combo_df)), combo_df['flip_rate'], color=colors)\n",
    "    ax3.set_title('Component Combination Attacks', fontsize=14)\n",
    "    ax3.set_ylabel('Flip Rate', fontsize=12)\n",
    "    ax3.set_xticks(range(len(combo_df)))\n",
    "    ax3.set_xticklabels(combo_df['components'], rotation=45)\n",
    "    \n",
    "    # 4. Defense effectiveness visualization\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Simulate defense mechanisms\n",
    "    defense_methods = ['No Defense', 'Majority Voting', \n",
    "                      'Component Validation', 'Outlier Detection']\n",
    "    defense_effectiveness = [0.1, 0.3, 0.5, 0.7]  # Hypothetical\n",
    "    \n",
    "    bars = ax4.bar(defense_methods, defense_effectiveness)\n",
    "    for i, bar in enumerate(bars):\n",
    "        bar.set_color(['red', 'orange', 'yellow', 'green'][i])\n",
    "    \n",
    "    ax4.set_title('Hypothetical Defense Effectiveness', fontsize=14)\n",
    "    ax4.set_ylabel('Attack Success Reduction', fontsize=12)\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('component_attack_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9212a6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting all trained models from your ensemble...\n",
      "\n",
      "Available models:\n",
      "  ✓ if: IsolationForest\n",
      "  ✓ ae: Functional\n",
      "  ✓ cnn: Functional\n",
      "  ✓ svdd: DeepSVDD\n",
      "  ✓ rf: RandomForestClassifier\n",
      "  ✓ cb: CatBoostClassifier\n",
      "\n",
      "Feature count: 62\n",
      "Scaler available: True\n",
      "\n",
      "============================================================\n",
      "INITIALIZING COMPONENT ENSEMBLE ATTACKER\n",
      "============================================================\n",
      "ERROR: Missing parameter: ensemble_model\n",
      "Please define ensemble_model before continuing\n",
      "✓ Attacker initialized successfully!\n",
      "\n",
      "✓ Meta-feature generation implemented\n",
      "\n",
      "============================================================\n",
      "RUNNING COMPREHENSIVE EXPERIMENTS\n",
      "============================================================\n",
      "Using 500 samples for testing\n",
      "EXPERIMENT 1: Individual Component Attacks\n",
      "\n",
      "ERROR during experiments: 'NoneType' object has no attribute 'iloc'\n",
      "\n",
      "Debugging tips:\n",
      "1. Check if all base models are trained and loaded\n",
      "2. Verify X_test and y_test have correct shapes\n",
      "3. Ensure _get_meta_features method is correctly implemented\n",
      "   X_test shape: (20625, 62)\n",
      "   y_test shape: (20625,)\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC CHECK\n",
      "============================================================\n",
      "✓ meta_model: Found\n",
      "✓ if_model: Found\n",
      "✓ autoencoder: Found\n",
      "✓ cnn_model: Found\n",
      "✓ svdd: Found\n",
      "✓ rf_model: Found\n",
      "✓ cb_model: Found\n",
      "✓ scaler: Found\n",
      "✓ X_test: Found\n",
      "✓ y_test: Found\n",
      "\n",
      "Testing meta-feature generation...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
      "✓ Meta-features shape: (5, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================\n",
    "# STEP 1: COLLECT ALL YOUR TRAINED MODELS\n",
    "# ============================================\n",
    "\n",
    "print(\"Collecting all trained models from your ensemble...\")\n",
    "\n",
    "# Your models should already exist from your earlier training code.\n",
    "# If not, here's how to recreate them:\n",
    "\n",
    "# 1. Your XGBoost meta-model (from your ensemble training)\n",
    "# This should already exist as 'meta_model' in your code\n",
    "# If not, retrain it:\n",
    "# meta_model = xgb.XGBClassifier(...)\n",
    "# meta_model.fit(X_meta_train, y_train)\n",
    "\n",
    "# 2. Collect ALL base models into a dictionary\n",
    "base_models = {\n",
    "    'if': if_model,           # Isolation Forest from your code\n",
    "    'ae': autoencoder,        # Autoencoder from your code\n",
    "    'cnn': cnn_model,         # CNN from your code\n",
    "    'svdd': svdd,             # DeepSVDD from your code\n",
    "    'rf': rf_model,           # Random Forest from your code\n",
    "    'cb': cb_model            # CatBoost from your code\n",
    "}\n",
    "\n",
    "# Check which models you have\n",
    "print(\"\\nAvailable models:\")\n",
    "for name, model in base_models.items():\n",
    "    if model is not None:\n",
    "        print(f\"  ✓ {name}: {type(model).__name__}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {name}: NOT FOUND - You need to train this model first\")\n",
    "\n",
    "# 3. Get your scaler and feature names\n",
    "# These should already exist from your preprocessing:\n",
    "# scaler = StandardScaler()  # From your preprocessing\n",
    "# scaler.fit(X_train)        # Already done\n",
    "\n",
    "# Feature names - get them from your data\n",
    "if hasattr(X_train, 'columns'):\n",
    "    feature_names = X_train.columns.tolist()\n",
    "else:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "\n",
    "print(f\"\\nFeature count: {len(feature_names)}\")\n",
    "print(f\"Scaler available: {'scaler' in locals() or 'scaler' in globals()}\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 2: REUSE THE COMPONENT ATTACKER CLASS\n",
    "# ============================================\n",
    "\n",
    "# First, make sure you have the ComponentEnsembleAttacker class defined\n",
    "# If not, copy it from the previous message or define it here:\n",
    "\n",
    "class ComponentEnsembleAttacker:\n",
    "    \"\"\"\n",
    "    Attack individual ensemble components and evaluate impact on final ensemble.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ensemble_model, base_models, scaler, feature_names):\n",
    "        self.ensemble = ensemble_model\n",
    "        self.base_models = base_models\n",
    "        self.scaler = scaler\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        # Component importance (which components to prioritize)\n",
    "        self.component_names = ['IsolationForest', 'Autoencoder', 'CNN', \n",
    "                               'DeepSVDD', 'RandomForest', 'CatBoost']\n",
    "    \n",
    "    def _get_meta_features(self, X):\n",
    "        \"\"\"Recreate the meta-features from base models\"\"\"\n",
    "        # Your existing meta-feature generation code here\n",
    "        # (Copy from your stacking implementation)\n",
    "        \n",
    "        # For now, placeholder - you need to replace with your actual code\n",
    "        print(\"WARNING: Using placeholder for _get_meta_features\")\n",
    "        print(\"You need to implement this with your actual stacking logic\")\n",
    "        return np.random.randn(len(X), 6)  # 6 base models\n",
    "    \n",
    "    def attack_single_component(self, X, y, target_component, epsilon=0.1):\n",
    "        \"\"\"Attack a specific component\"\"\"\n",
    "        # Your attack implementation here\n",
    "        pass\n",
    "    \n",
    "    def cascade_attack_analysis(self, X, y, epsilon=0.1):\n",
    "        \"\"\"Test attacking each component individually\"\"\"\n",
    "        # Your analysis implementation here\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# STEP 3: INITIALIZE THE ATTACKER WITH REAL PARAMETERS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INITIALIZING COMPONENT ENSEMBLE ATTACKER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Make sure all required parameters exist\n",
    "    required_params = ['ensemble_model', 'base_models', 'scaler', 'feature_names']\n",
    "    \n",
    "    for param in required_params:\n",
    "        if param not in locals() and param not in globals():\n",
    "            print(f\"ERROR: Missing parameter: {param}\")\n",
    "            print(f\"Please define {param} before continuing\")\n",
    "    \n",
    "    # Initialize the attacker\n",
    "    attacker = ComponentEnsembleAttacker(\n",
    "        ensemble_model=meta_model,      # Your XGBoost meta-model\n",
    "        base_models=base_models,        # Dictionary of base models\n",
    "        scaler=scaler,                  # Your StandardScaler\n",
    "        feature_names=feature_names     # Feature names\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Attacker initialized successfully!\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"\\nERROR: Missing variable - {e}\")\n",
    "    print(\"\\nYou need to define these variables first:\")\n",
    "    print(\"1. meta_model: Your trained XGBoost ensemble\")\n",
    "    print(\"2. base_models: Dictionary with all 6 base models\")\n",
    "    print(\"3. scaler: StandardScaler from preprocessing\")\n",
    "    print(\"4. X_train: Your training data (for feature names)\")\n",
    "    \n",
    "    # Create minimal working example if variables are missing\n",
    "    print(\"\\nCreating minimal working example with dummy data...\")\n",
    "    \n",
    "    # Create dummy data if real data isn't available\n",
    "    X_test_dummy = np.random.randn(100, X_train.shape[1] if 'X_train' in locals() else 10)\n",
    "    y_test_dummy = np.random.randint(0, 2, 100)\n",
    "    \n",
    "    # Create dummy models (for testing only)\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    dummy_ensemble = RandomForestClassifier()\n",
    "    dummy_ensemble.fit(X_test_dummy[:50], y_test_dummy[:50])\n",
    "    \n",
    "    dummy_base_models = {\n",
    "        'if': RandomForestClassifier(),\n",
    "        'ae': RandomForestClassifier(),\n",
    "        'cnn': RandomForestClassifier(),\n",
    "        'svdd': RandomForestClassifier(),\n",
    "        'rf': RandomForestClassifier(),\n",
    "        'cb': RandomForestClassifier()\n",
    "    }\n",
    "    \n",
    "    for name, model in dummy_base_models.items():\n",
    "        model.fit(X_test_dummy[:50], y_test_dummy[:50])\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    dummy_scaler = StandardScaler()\n",
    "    dummy_scaler.fit(X_test_dummy)\n",
    "    \n",
    "    dummy_feature_names = [f'feature_{i}' for i in range(X_test_dummy.shape[1])]\n",
    "    \n",
    "    attacker = ComponentEnsembleAttacker(\n",
    "        ensemble_model=dummy_ensemble,\n",
    "        base_models=dummy_base_models,\n",
    "        scaler=dummy_scaler,\n",
    "        feature_names=dummy_feature_names\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Created dummy attacker for testing\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 4: IMPLEMENT MISSING METHODS\n",
    "# ============================================\n",
    "\n",
    "# You need to implement the actual meta-feature generation\n",
    "# Replace the placeholder with your actual stacking logic\n",
    "\n",
    "def implement_meta_features(self, X):\n",
    "    \"\"\"\n",
    "    IMPLEMENT THIS WITH YOUR ACTUAL CODE\n",
    "    This should match your ensemble stacking code\n",
    "    \"\"\"\n",
    "    # This is a CRITICAL method - you need to implement it\n",
    "    \n",
    "    # Your code should look something like this:\n",
    "    # 1. Isolation Forest scores\n",
    "    if_scores = -self.base_models['if'].decision_function(X)\n",
    "    if_scores_z = (if_scores - if_scores.mean()) / (if_scores.std() + 1e-8)\n",
    "    \n",
    "    # 2. Autoencoder reconstruction error\n",
    "    X_if = np.hstack([X, if_scores.reshape(-1, 1)])\n",
    "    X_pred = self.base_models['ae'].predict(X_if)\n",
    "    recon_error = np.mean((X_if - X_pred)**2, axis=1)\n",
    "    recon_error_z = (recon_error - recon_error.mean()) / (recon_error.std() + 1e-8)\n",
    "    \n",
    "    # 3. CNN scores\n",
    "    X_cnn = X_if.reshape(X_if.shape[0], X_if.shape[1], 1)\n",
    "    cnn_scores = self.base_models['cnn'].predict(X_cnn).flatten()\n",
    "    \n",
    "    # 4. DeepSVDD scores\n",
    "    svdd_scores = self.base_models['svdd'].score(X_if.astype(\"float32\"))\n",
    "    svdd_scores_z = (svdd_scores - svdd_scores.mean()) / (svdd_scores.std() + 1e-8)\n",
    "    \n",
    "    # 5. Random Forest probabilities\n",
    "    rf_probs = self.base_models['rf'].predict_proba(X)[:, 1]\n",
    "    \n",
    "    # 6. CatBoost probabilities\n",
    "    cb_probs = self.base_models['cb'].predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Stack all meta-features\n",
    "    meta_features = np.vstack([\n",
    "        if_scores_z, recon_error_z, cnn_scores,\n",
    "        svdd_scores_z, rf_probs, cb_probs\n",
    "    ]).T\n",
    "    \n",
    "    return meta_features\n",
    "\n",
    "# Replace the placeholder method\n",
    "ComponentEnsembleAttacker._get_meta_features = implement_meta_features\n",
    "\n",
    "print(\"\\n✓ Meta-feature generation implemented\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 5: RUN THE EXPERIMENTS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING COMPREHENSIVE EXPERIMENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use smaller subset for initial testing\n",
    "sample_size = min(500, len(X_test))\n",
    "print(f\"Using {sample_size} samples for testing\")\n",
    "\n",
    "try:\n",
    "    # Run experiments\n",
    "    thesis_results = thesis_component_attack_experiments(\n",
    "        X_test[:sample_size], \n",
    "        y_test[:sample_size], \n",
    "        attacker\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Experiments completed successfully!\")\n",
    "    \n",
    "    # Generate visualizations\n",
    "    visualize_component_attacks(thesis_results)\n",
    "    \n",
    "    # Identify critical findings\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CRITICAL THESIS FINDINGS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'individual_results' in thesis_results:\n",
    "        most_vuln = thesis_results['individual_results'].iloc[0]\n",
    "        print(f\"1. Most Vulnerable Component: {most_vuln['component']}\")\n",
    "        print(f\"   Accuracy drop when attacked: {most_vuln['accuracy_drop']:.2%}\")\n",
    "    \n",
    "    if 'combination_attacks' in thesis_results:\n",
    "        best_combo = thesis_results['combination_attacks'].iloc[0]\n",
    "        print(f\"\\n2. Most Effective Attack Combination: {best_combo['components']}\")\n",
    "        print(f\"   Prediction flip rate: {best_combo['flip_rate']:.2%}\")\n",
    "    \n",
    "    print(\"\\n3. Defense Recommendations:\")\n",
    "    print(\"   - Monitor component output consistency\")\n",
    "    print(\"   - Implement majority voting for outlier detection\")\n",
    "    print(\"   - Regularly update most vulnerable components\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR during experiments: {e}\")\n",
    "    print(\"\\nDebugging tips:\")\n",
    "    print(\"1. Check if all base models are trained and loaded\")\n",
    "    print(\"2. Verify X_test and y_test have correct shapes\")\n",
    "    print(\"3. Ensure _get_meta_features method is correctly implemented\")\n",
    "    print(f\"   X_test shape: {X_test.shape if 'X_test' in locals() else 'Not found'}\")\n",
    "    print(f\"   y_test shape: {y_test.shape if 'y_test' in locals() else 'Not found'}\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 6: QUICK DIAGNOSTIC CHECK\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if all necessary variables exist\n",
    "variables_to_check = [\n",
    "    'meta_model', 'if_model', 'autoencoder', 'cnn_model',\n",
    "    'svdd', 'rf_model', 'cb_model', 'scaler', 'X_test', 'y_test'\n",
    "]\n",
    "\n",
    "for var in variables_to_check:\n",
    "    exists = var in locals() or var in globals()\n",
    "    status = \"✓\" if exists else \"✗\"\n",
    "    print(f\"{status} {var}: {'Found' if exists else 'MISSING'}\")\n",
    "\n",
    "# Test meta-feature generation\n",
    "print(\"\\nTesting meta-feature generation...\")\n",
    "try:\n",
    "    test_meta = attacker._get_meta_features(X_test[:5])\n",
    "    print(f\"✓ Meta-features shape: {test_meta.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n",
    "    print(\"You need to properly implement _get_meta_features()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "381ed3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Make sure your CNN is compiled and trained: cnn_model\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "def fgsm_attack(model, x, y, epsilon=0.01):\n",
    "    \"\"\"\n",
    "    x: input tensor (batch, features, 1)\n",
    "    y: true labels (0/1)\n",
    "    epsilon: perturbation magnitude\n",
    "    \"\"\"\n",
    "    x_tensor = tf.convert_to_tensor(x)\n",
    "    y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x_tensor)\n",
    "        pred = model(x_tensor)\n",
    "        loss = loss_object(y_tensor, pred)\n",
    "\n",
    "    gradient = tape.gradient(loss, x_tensor)\n",
    "    perturbation = epsilon * tf.sign(gradient)\n",
    "    x_adv = x_tensor + perturbation\n",
    "    x_adv = tf.clip_by_value(x_adv, 0, 1)  # keep input in valid range\n",
    "    return x_adv.numpy()\n",
    "\n",
    "# Example usage:\n",
    "X_test_cnn_tensor = X_test_cnn.astype(np.float32)\n",
    "y_test_tensor = y_test_cnn.astype(np.float32)\n",
    "X_test_adv_fgsm = fgsm_attack(cnn_model, X_test_cnn_tensor, y_test_tensor, epsilon=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_fgsm_attack(model, x, y, epsilon=0.01, alpha=0.005, iters=10, decay=1.0):\n",
    "    \"\"\"\n",
    "    x: input tensor\n",
    "    y: true labels\n",
    "    epsilon: max perturbation\n",
    "    alpha: step size per iteration\n",
    "    iters: number of iterations\n",
    "    decay: momentum decay factor\n",
    "    \"\"\"\n",
    "    x_adv = tf.identity(x)\n",
    "    g = tf.zeros_like(x_adv)\n",
    "\n",
    "    for i in range(iters):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_adv)\n",
    "            pred = model(x_adv)\n",
    "            loss = loss_object(y, pred)\n",
    "\n",
    "        gradient = tape.gradient(loss, x_adv)\n",
    "        g = decay * g + gradient / tf.reduce_mean(tf.abs(gradient), axis=[1,2], keepdims=True)\n",
    "        x_adv = x_adv + alpha * tf.sign(g)\n",
    "        x_adv = tf.clip_by_value(x_adv, 0, 1)\n",
    "\n",
    "    # Ensure perturbation stays within epsilon-ball\n",
    "    x_adv = tf.clip_by_value(x + tf.clip_by_value(x_adv - x, -epsilon, epsilon), 0, 1)\n",
    "    return x_adv.numpy()\n",
    "\n",
    "# Example usage:\n",
    "X_test_adv_mifgsm = mi_fgsm_attack(cnn_model, X_test_cnn_tensor, y_test_tensor,\n",
    "                                   epsilon=0.01, alpha=0.005, iters=10, decay=1.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
