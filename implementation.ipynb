{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131a7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                           f1_score, roc_auc_score, average_precision_score,\n",
    "                           classification_report, confusion_matrix)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b155f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0f10007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2150cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103121 entries, 0 to 103120\n",
      "Data columns (total 79 columns):\n",
      " #   Column                      Non-Null Count   Dtype   \n",
      "---  ------                      --------------   -----   \n",
      " 0   Protocol                    103121 non-null  int8    \n",
      " 1   Flow Duration               103121 non-null  int32   \n",
      " 2   Total Fwd Packet            103121 non-null  int32   \n",
      " 3   Total Bwd packets           103121 non-null  int32   \n",
      " 4   Total Length of Fwd Packet  103121 non-null  int32   \n",
      " 5   Total Length of Bwd Packet  103121 non-null  int32   \n",
      " 6   Fwd Packet Length Max       103121 non-null  int32   \n",
      " 7   Fwd Packet Length Min       103121 non-null  int16   \n",
      " 8   Fwd Packet Length Mean      103121 non-null  float32 \n",
      " 9   Fwd Packet Length Std       103121 non-null  float32 \n",
      " 10  Bwd Packet Length Max       103121 non-null  int32   \n",
      " 11  Bwd Packet Length Min       103121 non-null  int16   \n",
      " 12  Bwd Packet Length Mean      103121 non-null  float32 \n",
      " 13  Bwd Packet Length Std       103121 non-null  float32 \n",
      " 14  Flow Bytes/s                103121 non-null  float64 \n",
      " 15  Flow Packets/s              103121 non-null  float64 \n",
      " 16  Flow IAT Mean               103121 non-null  float32 \n",
      " 17  Flow IAT Std                103121 non-null  float32 \n",
      " 18  Flow IAT Max                103121 non-null  int32   \n",
      " 19  Flow IAT Min                103121 non-null  int32   \n",
      " 20  Fwd IAT Total               103121 non-null  int32   \n",
      " 21  Fwd IAT Mean                103121 non-null  float32 \n",
      " 22  Fwd IAT Std                 103121 non-null  float32 \n",
      " 23  Fwd IAT Max                 103121 non-null  int32   \n",
      " 24  Fwd IAT Min                 103121 non-null  int32   \n",
      " 25  Bwd IAT Total               103121 non-null  int32   \n",
      " 26  Bwd IAT Mean                103121 non-null  float32 \n",
      " 27  Bwd IAT Std                 103121 non-null  float32 \n",
      " 28  Bwd IAT Max                 103121 non-null  int32   \n",
      " 29  Bwd IAT Min                 103121 non-null  int32   \n",
      " 30  Fwd PSH Flags               103121 non-null  int8    \n",
      " 31  Bwd PSH Flags               103121 non-null  int8    \n",
      " 32  Fwd URG Flags               103121 non-null  int8    \n",
      " 33  Bwd URG Flags               103121 non-null  int8    \n",
      " 34  Fwd Header Length           103121 non-null  int32   \n",
      " 35  Bwd Header Length           103121 non-null  int32   \n",
      " 36  Fwd Packets/s               103121 non-null  float32 \n",
      " 37  Bwd Packets/s               103121 non-null  float32 \n",
      " 38  Packet Length Min           103121 non-null  int16   \n",
      " 39  Packet Length Max           103121 non-null  int32   \n",
      " 40  Packet Length Mean          103121 non-null  float32 \n",
      " 41  Packet Length Std           103121 non-null  float32 \n",
      " 42  Packet Length Variance      103121 non-null  float32 \n",
      " 43  FIN Flag Count              103121 non-null  int8    \n",
      " 44  SYN Flag Count              103121 non-null  int8    \n",
      " 45  RST Flag Count              103121 non-null  int8    \n",
      " 46  PSH Flag Count              103121 non-null  int32   \n",
      " 47  ACK Flag Count              103121 non-null  int32   \n",
      " 48  URG Flag Count              103121 non-null  int8    \n",
      " 49  CWE Flag Count              103121 non-null  int8    \n",
      " 50  ECE Flag Count              103121 non-null  int8    \n",
      " 51  Down/Up Ratio               103121 non-null  int16   \n",
      " 52  Avg Packet Size             103121 non-null  float32 \n",
      " 53  Fwd Segment Size Avg        103121 non-null  float32 \n",
      " 54  Bwd Segment Size Avg        103121 non-null  float32 \n",
      " 55  Fwd Bytes/Bulk Avg          103121 non-null  int8    \n",
      " 56  Fwd Packet/Bulk Avg         103121 non-null  int8    \n",
      " 57  Fwd Bulk Rate Avg           103121 non-null  int8    \n",
      " 58  Bwd Bytes/Bulk Avg          103121 non-null  int8    \n",
      " 59  Bwd Packet/Bulk Avg         103121 non-null  int32   \n",
      " 60  Bwd Bulk Rate Avg           103121 non-null  int32   \n",
      " 61  Subflow Fwd Packets         103121 non-null  int8    \n",
      " 62  Subflow Fwd Bytes           103121 non-null  int16   \n",
      " 63  Subflow Bwd Packets         103121 non-null  int8    \n",
      " 64  Subflow Bwd Bytes           103121 non-null  int16   \n",
      " 65  FWD Init Win Bytes          103121 non-null  int32   \n",
      " 66  Bwd Init Win Bytes          103121 non-null  int32   \n",
      " 67  Fwd Act Data Packets        103121 non-null  int32   \n",
      " 68  Fwd Seg Size Min            103121 non-null  int8    \n",
      " 69  Active Mean                 103121 non-null  int8    \n",
      " 70  Active Std                  103121 non-null  int8    \n",
      " 71  Active Max                  103121 non-null  int8    \n",
      " 72  Active Min                  103121 non-null  int8    \n",
      " 73  Idle Mean                   103121 non-null  float32 \n",
      " 74  Idle Std                    103121 non-null  float32 \n",
      " 75  Idle Max                    103121 non-null  float32 \n",
      " 76  Idle Min                    103121 non-null  float32 \n",
      " 77  Label                       103121 non-null  category\n",
      " 78  Label.1                     103121 non-null  category\n",
      "dtypes: category(2), float32(22), float64(2), int16(6), int32(25), int8(22)\n",
      "memory usage: 23.6 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Label\n",
       "Non-Tor    64804\n",
       "NonVPN     20216\n",
       "VPN        16922\n",
       "Tor         1179\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"cicdarknet2020.parquet\", engine=\"fastparquet\")\n",
    "df.info()\n",
    "df['Label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c1c1ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INITIAL DATA INSPECTION ===\n",
      "DataFrame shape: (103121, 79)\n",
      "Memory usage: 23.60 MB\n",
      "\n",
      "=== DATA TYPES DETAILED ===\n",
      "Label column info:\n",
      "  dtype: category\n",
      "  type of dtype: <class 'pandas.core.dtypes.dtypes.CategoricalDtype'>\n",
      "  is categorical? True\n",
      "  is string? True\n",
      "\n",
      "Label.1 column info:\n",
      "  dtype: category\n",
      "  type of dtype: <class 'pandas.core.dtypes.dtypes.CategoricalDtype'>\n",
      "  is categorical? True\n",
      "  is string? True\n",
      "\n",
      "=== DECIDING WHICH LABEL TO USE ===\n",
      "Based on your output:\n",
      "1. Label column: Has values 0, 1, 2, 3 (4 classes)\n",
      "2. Label.1 column: Has actual names like 'Browsing', 'P2P', etc.\n",
      "\n",
      "Checking if Label is already encoded...\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: NonVPN -> Label.1: Chat\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Tor -> Label.1: VOIP\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: NonVPN -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Email\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Email\n",
      "Label: VPN -> Label.1: VOIP\n",
      "Label: Non-Tor -> Label.1: Video-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: VOIP\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: VPN -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Video-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Video-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: NonVPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Email\n",
      "Label: NonVPN -> Label.1: Chat\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: File-Transfer\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: VPN -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Email\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: NonVPN -> Label.1: Email\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: NonVPN -> Label.1: Email\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: AUDIO-STREAMING\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: VPN -> Label.1: Chat\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: NonVPN -> Label.1: Video-Streaming\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: NonVPN -> Label.1: File-transfer\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: NonVPN -> Label.1: Video-Streaming\n",
      "Label: NonVPN -> Label.1: File-Transfer\n",
      "Label: VPN -> Label.1: File-Transfer\n",
      "Label: NonVPN -> Label.1: Audio-Streaming\n",
      "Label: VPN -> Label.1: Audio-Streaming\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: Non-Tor -> Label.1: P2P\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: Non-Tor -> Label.1: AUDIO-STREAMING\n",
      "Label: Non-Tor -> Label.1: File-Transfer\n",
      "Label: Non-Tor -> Label.1: Browsing\n",
      "Label: VPN -> Label.1: Video-Streaming\n",
      "\n",
      "=== CREATING PROPER LABEL MAPPING ===\n",
      "             most_common  num_unique  \\\n",
      "Label                                  \n",
      "Non-Tor  AUDIO-STREAMING           8   \n",
      "NonVPN              Chat           8   \n",
      "Tor      Audio-Streaming           8   \n",
      "VPN        File-Transfer           6   \n",
      "\n",
      "                                             sample_values  \n",
      "Label                                                       \n",
      "Non-Tor  [AUDIO-STREAMING, Browsing, Chat, Email, File-...  \n",
      "NonVPN   [Chat, Audio-Streaming, Email, File-Transfer, ...  \n",
      "Tor      [Audio-Streaming, Browsing, Chat, File-Transfe...  \n",
      "VPN      [File-Transfer, Chat, Audio-Streaming, Email, ...  \n",
      "\n",
      "Based on analysis, I recommend using Label.1 as it has the actual class names\n",
      "\n",
      "=== DATA CLEANING ===\n",
      "Checking for duplicate columns...\n",
      "Duplicate columns: []\n",
      "Constant columns: ['Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Bwd Bytes/Bulk Avg', 'Subflow Bwd Packets', 'Active Mean', 'Active Std', 'Active Max', 'Active Min']\n",
      "Dropped constant columns: ['Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Bwd Bytes/Bulk Avg', 'Subflow Bwd Packets', 'Active Mean', 'Active Std', 'Active Max', 'Active Min']\n",
      "\n",
      "=== HANDLING MISSING/INFINITE VALUES ===\n",
      "\n",
      "=== LABEL PROCESSING ===\n",
      "\n",
      "Cleaned label distribution:\n",
      "  'browsing': 29862 samples (28.96%)\n",
      "  'p2p': 23404 samples (22.70%)\n",
      "  'audio-streaming': 11328 samples (10.99%)\n",
      "  'file-transfer': 10647 samples (10.32%)\n",
      "  'chat': 10365 samples (10.05%)\n",
      "  'video-streaming': 9012 samples (8.74%)\n",
      "  'email': 5442 samples (5.28%)\n",
      "  'voip': 3061 samples (2.97%)\n",
      "\n",
      "=== FINAL LABEL ENCODING ===\n",
      "Class mapping:\n",
      "  0: 'audio-streaming' - 11328 samples (10.99%)\n",
      "  1: 'browsing' - 29862 samples (28.96%)\n",
      "  2: 'chat' - 10365 samples (10.05%)\n",
      "  3: 'email' - 5442 samples (5.28%)\n",
      "  4: 'file-transfer' - 10647 samples (10.32%)\n",
      "  5: 'p2p' - 23404 samples (22.70%)\n",
      "  6: 'video-streaming' - 9012 samples (8.74%)\n",
      "  7: 'voip' - 3061 samples (2.97%)\n",
      "\n",
      "=== FEATURE PREPARATION ===\n",
      "Features shape: (103121, 62)\n",
      "Labels shape: (103121,)\n",
      "\n",
      "=== FEATURE SCALING ===\n",
      "Scaled 62 numeric features\n",
      "\n",
      "=== DATA SPLITTING ===\n",
      "Class distribution:\n",
      "  1: 'browsing' - 29862 samples (28.96%)\n",
      "  5: 'p2p' - 23404 samples (22.70%)\n",
      "  0: 'audio-streaming' - 11328 samples (10.99%)\n",
      "  4: 'file-transfer' - 10647 samples (10.32%)\n",
      "  2: 'chat' - 10365 samples (10.05%)\n",
      "  6: 'video-streaming' - 9012 samples (8.74%)\n",
      "  3: 'email' - 5442 samples (5.28%)\n",
      "  7: 'voip' - 3061 samples (2.97%)\n",
      "\n",
      "Using stratified split\n",
      "\n",
      "Training set: 82496 samples\n",
      "Testing set: 20625 samples\n",
      "\n",
      "=== SAVING DATA ===\n",
      "\n",
      "Data preprocessing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(\"=== INITIAL DATA INSPECTION ===\")\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage().sum() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Let's check the actual dtypes more carefully\n",
    "print(\"\\n=== DATA TYPES DETAILED ===\")\n",
    "print(\"Label column info:\")\n",
    "print(f\"  dtype: {df['Label'].dtype}\")\n",
    "print(f\"  type of dtype: {type(df['Label'].dtype)}\")\n",
    "print(f\"  is categorical? {pd.api.types.is_categorical_dtype(df['Label'])}\")\n",
    "print(f\"  is string? {pd.api.types.is_string_dtype(df['Label'])}\")\n",
    "\n",
    "print(\"\\nLabel.1 column info:\")\n",
    "print(f\"  dtype: {df['Label.1'].dtype}\")\n",
    "print(f\"  type of dtype: {type(df['Label.1'].dtype)}\")\n",
    "print(f\"  is categorical? {pd.api.types.is_categorical_dtype(df['Label.1'])}\")\n",
    "print(f\"  is string? {pd.api.types.is_string_dtype(df['Label.1'])}\")\n",
    "\n",
    "# DECISION TIME: Which label to use?\n",
    "print(\"\\n=== DECIDING WHICH LABEL TO USE ===\")\n",
    "print(\"Based on your output:\")\n",
    "print(\"1. Label column: Has values 0, 1, 2, 3 (4 classes)\")\n",
    "print(\"2. Label.1 column: Has actual names like 'Browsing', 'P2P', etc.\")\n",
    "\n",
    "# Let me check if Label might be encoded already\n",
    "print(\"\\nChecking if Label is already encoded...\")\n",
    "# Get a mapping by sampling\n",
    "sample_size = min(100, len(df))\n",
    "sample = df[['Label', 'Label.1']].sample(sample_size)\n",
    "for _, row in sample.iterrows():\n",
    "    print(f\"Label: {row['Label']} -> Label.1: {row['Label.1']}\")\n",
    "\n",
    "\n",
    "\n",
    "# Let's create a proper mapping\n",
    "print(\"\\n=== CREATING PROPER LABEL MAPPING ===\")\n",
    "# Group by Label and see what Label.1 values correspond\n",
    "label_mapping_df = df.groupby('Label')['Label.1'].agg(['first', 'nunique', lambda x: list(x.unique())[:5]])\n",
    "label_mapping_df.columns = ['most_common', 'num_unique', 'sample_values']\n",
    "print(label_mapping_df)\n",
    "\n",
    "# If Label is already encoded and Label.1 has the names, use Label.1\n",
    "print(\"\\nBased on analysis, I recommend using Label.1 as it has the actual class names\")\n",
    "\n",
    "# Clean up the data\n",
    "print(\"\\n=== DATA CLEANING ===\")\n",
    "print(\"Checking for duplicate columns...\")\n",
    "duplicate_columns = df.columns[df.columns.duplicated()]\n",
    "print(f\"Duplicate columns: {list(duplicate_columns)}\")\n",
    "\n",
    "# Check for constant columns\n",
    "constant_columns = [col for col in df.columns if df[col].nunique() == 1]\n",
    "print(f\"Constant columns: {constant_columns}\")\n",
    "\n",
    "if constant_columns:\n",
    "    df = df.drop(columns=constant_columns)\n",
    "    print(f\"Dropped constant columns: {constant_columns}\")\n",
    "\n",
    "# Handle missing/infinite values\n",
    "print(\"\\n=== HANDLING MISSING/INFINITE VALUES ===\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "nan_counts = df.isnull().sum()\n",
    "if nan_counts.any():\n",
    "    nan_cols = nan_counts[nan_counts > 0].index.tolist()\n",
    "    print(f\"Columns with NaN: {nan_cols}\")\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Now process the labels\n",
    "print(\"\\n=== LABEL PROCESSING ===\")\n",
    "# Use Label.1 since it has the actual names\n",
    "df['Label_original'] = df['Label.1'].astype(str)\n",
    "\n",
    "# Clean the labels\n",
    "df['Label_cleaned'] = df['Label_original'].str.lower().str.strip()\n",
    "\n",
    "# Check cleaned labels\n",
    "print(\"\\nCleaned label distribution:\")\n",
    "cleaned_counts = df['Label_cleaned'].value_counts()\n",
    "for label, count in cleaned_counts.items():\n",
    "    proportion = count / len(df) * 100\n",
    "    print(f\"  '{label}': {count} samples ({proportion:.2f}%)\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['Label_encoded'] = label_encoder.fit_transform(df['Label_cleaned'])\n",
    "\n",
    "print(\"\\n=== FINAL LABEL ENCODING ===\")\n",
    "print(\"Class mapping:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    count = (df['Label_cleaned'] == label).sum()\n",
    "    proportion = count / len(df) * 100\n",
    "    print(f\"  {i}: '{label}' - {count} samples ({proportion:.2f}%)\")\n",
    "\n",
    "label_mapping = dict(zip(range(len(label_encoder.classes_)), label_encoder.classes_))\n",
    "\n",
    "# Prepare features\n",
    "print(\"\\n=== FEATURE PREPARATION ===\")\n",
    "# Exclude all label-related columns\n",
    "label_cols = ['Label', 'Label.1', 'Label_original', 'Label_cleaned', 'Label_encoded']\n",
    "exclude_cols = [col for col in label_cols if col in df.columns]\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['Label_encoded']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "# Scale features\n",
    "print(\"\\n=== FEATURE SCALING ===\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "if len(numeric_features) > 0:\n",
    "    X_scaled[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "    print(f\"Scaled {len(numeric_features)} numeric features\")\n",
    "else:\n",
    "    print(\"No numeric features to scale\")\n",
    "\n",
    "# Split data\n",
    "print(\"\\n=== DATA SPLITTING ===\")\n",
    "class_counts = y.value_counts()\n",
    "print(\"Class distribution:\")\n",
    "for class_id, count in class_counts.items():\n",
    "    class_name = label_encoder.inverse_transform([class_id])[0]\n",
    "    proportion = count / len(y) * 100\n",
    "    print(f\"  {class_id}: '{class_name}' - {count} samples ({proportion:.2f}%)\")\n",
    "\n",
    "# Use stratification if possible\n",
    "if class_counts.min() >= 2:\n",
    "    print(\"\\nUsing stratified split\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nUsing random split (some classes have < 2 samples)\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Save processed data\n",
    "print(\"\\n=== SAVING DATA ===\")\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "preprocessed_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'scaler': scaler,\n",
    "    'label_encoder': label_encoder,\n",
    "    'feature_names': X_train.columns.tolist(),\n",
    "    'label_mapping': label_mapping,\n",
    "    'num_classes': len(label_encoder.classes_)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Quick Summary \n",
    "print(\"\\nData preprocessing completed successfully.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea64c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_encrypted'] = df['Label'].apply(lambda x: 1 if x in ['Tor','VPN'] else 0)\n",
    "df['is_encrypted'] = df['is_encrypted'].astype(int)\n",
    "df['is_encrypted'].value_counts()\n",
    "X = df.drop(columns=['Label', 'is_encrypted'])\n",
    "y_multiclass = df['Label']         # for multiclass classification\n",
    "y_binary = df['is_encrypted']      # for encrypted vs non-encrypted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "091c982f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTE:\n",
      "  Class 0: 8987 samples\n",
      "  Class 1: 23732 samples\n",
      "  Class 2: 8321 samples\n",
      "  Class 3: 4405 samples\n",
      "  Class 4: 8603 samples\n",
      "  Class 5: 18782 samples\n",
      "  Class 6: 7206 samples\n",
      "  Class 7: 2460 samples\n",
      "\n",
      "Class distribution after SMOTE:\n",
      "  Class 0: 23732 samples\n",
      "  Class 1: 23732 samples\n",
      "  Class 2: 23732 samples\n",
      "  Class 3: 23732 samples\n",
      "  Class 4: 23732 samples\n",
      "  Class 5: 23732 samples\n",
      "  Class 6: 23732 samples\n",
      "  Class 7: 23732 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df['Label_encoded'] exists\n",
    "X = X_scaled  # scaled numeric features\n",
    "y = df['Label_encoded'].values  # integer-encoded labels\n",
    "\n",
    "# Split train/test (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "print(\"Class distribution before SMOTE:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  Class {u}: {c} samples\")\n",
    "\n",
    "# Apply SMOTE to training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "unique_res, counts_res = np.unique(y_train_res, return_counts=True)\n",
    "for u, c in zip(unique_res, counts_res):\n",
    "    print(f\"  Class {u}: {c} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5c9669b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTE (binary):\n",
      "  Non-encrypted: 85020 samples\n",
      "  Encrypted: 18101 samples\n",
      "\n",
      "Class distribution after SMOTE (binary):\n",
      "  Non-encrypted: 68015 samples\n",
      "  Encrypted: 68015 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Binary target\n",
    "y_binary = df['is_encrypted'].values  # 0 = non-encrypted, 1 = Tor/VPN\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_binary, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_binary\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Class distribution before SMOTE (binary):\")\n",
    "unique, counts = np.unique(y_binary, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    label_name = \"Encrypted\" if u == 1 else \"Non-encrypted\"\n",
    "    print(f\"  {label_name}: {c} samples\")\n",
    "\n",
    "# Apply SMOTE to training set\n",
    "smote_bin = SMOTE(random_state=42)\n",
    "X_train_bin_res, y_train_bin_res = smote_bin.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE (binary):\")\n",
    "unique_res, counts_res = np.unique(y_train_bin_res, return_counts=True)\n",
    "for u, c in zip(unique_res, counts_res):\n",
    "    label_name = \"Encrypted\" if u == 1 else \"Non-encrypted\"\n",
    "    print(f\"  {label_name}: {c} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f27b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train IF on all training data (or only normal traffic if you have labels)\n",
    "if_model = IsolationForest(n_estimators=200, contamination=0.03, random_state=42)\n",
    "if_model.fit(X_train)\n",
    "\n",
    "# Get anomaly scores (more negative = more anomalous)\n",
    "if_scores_train = if_model.decision_function(X_train)\n",
    "if_scores_test = if_model.decision_function(X_test)\n",
    "\n",
    "# Add IF score as an additional feature\n",
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0407832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train encrypted count: 14481\n",
      "Test encrypted count: 3620\n",
      "Unique values in y_binary: [0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_binary, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_binary\n",
    ")\n",
    "\n",
    "print(\"Train encrypted count:\", sum(y_train))\n",
    "print(\"Test encrypted count:\", sum(y_test))\n",
    "\n",
    "print(\"Unique values in y_binary:\", np.unique(y_binary))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10c83f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ISOLATION FOREST ===\n",
      "Accuracy: 0.8000484848484849\n",
      "ROC-AUC: 0.6621479220443776\n",
      "Precision: 0.28716216216216217\n",
      "Recall: 0.09392265193370165\n",
      "F1-score: 0.14154870940882597\n",
      "Confusion Matrix:\n",
      " [[16161   844]\n",
      " [ 3280   340]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Train IF only on normal samples (y_train == 0)\n",
    "if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "if_model.fit(X_train[y_train == 0])\n",
    "\n",
    "# anomaly scores (more negative = more anomalous)\n",
    "if_scores_train = -if_model.decision_function(X_train)\n",
    "if_scores_test = -if_model.decision_function(X_test)\n",
    "\n",
    "# pick threshold = 95th percentile of normal scores\n",
    "threshold_if = np.percentile(if_scores_train[y_train == 0], 95)\n",
    "if_pred = (if_scores_test > threshold_if).astype(int)\n",
    "\n",
    "print(\"\\n=== ISOLATION FOREST ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, if_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, if_scores_test))\n",
    "print(\"Precision:\", precision_score(y_test, if_pred))\n",
    "print(\"Recall:\", recall_score(y_test, if_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, if_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, if_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7647701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.5208 - val_loss: 0.3395\n",
      "Epoch 2/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2251 - val_loss: 0.2094\n",
      "Epoch 3/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1634 - val_loss: 0.1612\n",
      "Epoch 4/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1383 - val_loss: 0.1413\n",
      "Epoch 5/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1149 - val_loss: 0.1251\n",
      "Epoch 6/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1021 - val_loss: 0.1302\n",
      "Epoch 7/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0946 - val_loss: 0.1034\n",
      "Epoch 8/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0856 - val_loss: 0.1022\n",
      "Epoch 9/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0768 - val_loss: 0.1252\n",
      "Epoch 10/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0743 - val_loss: 0.0899\n",
      "Epoch 11/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0672 - val_loss: 0.1009\n",
      "Epoch 12/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0751 - val_loss: 0.0855\n",
      "Epoch 13/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0737 - val_loss: 0.0903\n",
      "Epoch 14/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0582 - val_loss: 0.0899\n",
      "Epoch 15/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0529 - val_loss: 0.0900\n",
      "Epoch 16/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0566 - val_loss: 0.0733\n",
      "Epoch 17/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0885\n",
      "Epoch 18/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0552 - val_loss: 0.0743\n",
      "Epoch 19/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0466 - val_loss: 0.0751\n",
      "Epoch 20/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0555 - val_loss: 0.0629\n",
      "Epoch 21/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0815\n",
      "Epoch 22/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0526 - val_loss: 0.0675\n",
      "Epoch 23/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0504 - val_loss: 0.0646\n",
      "Epoch 24/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0434 - val_loss: 0.0673\n",
      "Epoch 25/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0391 - val_loss: 0.0731\n",
      "Epoch 26/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0394 - val_loss: 0.0652\n",
      "Epoch 27/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0409 - val_loss: 0.0682\n",
      "Epoch 28/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0454 - val_loss: 0.0839\n",
      "Epoch 29/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0407 - val_loss: 0.0616\n",
      "Epoch 30/30\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0374 - val_loss: 0.0631\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "from tensorflow.keras import layers, models, Model\n",
    "# 1️⃣ Prepare data\n",
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if  = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "input_dim = X_train_if.shape[1]\n",
    "\n",
    "# 2️⃣ Define autoencoder\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "x = layers.Dense(64, activation='relu')(input_layer)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(latent)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "output = layers.Dense(input_dim, activation='linear')(x)\n",
    "autoencoder = Model(input_layer, output)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 3️⃣ Train autoencoder\n",
    "autoencoder.fit(\n",
    "    X_train_if[y_train == 0], X_train_if[y_train == 0],\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4️⃣ Predict reconstruction for test set\n",
    "X_test_pred = autoencoder.predict(X_test_if)\n",
    "\n",
    "# 5️⃣ Compute reconstruction error\n",
    "reconstruction_error = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "# 5️⃣ Compute reconstruction error\n",
    "reconstruction_error = np.mean(np.square(X_test_if - X_test_pred), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52760e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 624us/step\n",
      "\u001b[1m2126/2126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 636us/step\n",
      "\n",
      "=== AUTOENCODER ===\n",
      "Accuracy: 0.8111515151515152\n",
      "ROC-AUC: 0.6312620272555521\n",
      "Precision: 0.40575736806031526\n",
      "Recall: 0.16353591160220995\n",
      "F1-score: 0.2331167552667848\n",
      "Confusion Matrix:\n",
      " [[16138   867]\n",
      " [ 3028   592]]\n"
     ]
    }
   ],
   "source": [
    "X_test_pred = autoencoder.predict(X_test_if)\n",
    "test_rec_err = np.mean((X_test_if - X_test_pred)**2, axis=1)\n",
    "\n",
    "X_train_pred = autoencoder.predict(X_train_if[y_train == 0])\n",
    "train_rec_err = np.mean((X_train_if[y_train == 0] - X_train_pred)**2, axis=1)\n",
    "\n",
    "threshold_ae = np.percentile(train_rec_err, 95)\n",
    "ae_pred = (test_rec_err > threshold_ae).astype(int)\n",
    "print(\"\\n=== AUTOENCODER ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, ae_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, test_rec_err))\n",
    "print(\"Precision:\", precision_score(y_test, ae_pred))\n",
    "print(\"Recall:\", recall_score(y_test, ae_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, ae_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, ae_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb04ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Deep SVDD implementation in PyTorch\n",
    "# --- Encoder network (small MLP) ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Deep SVDD trainer ---\n",
    "class DeepSVDD:\n",
    "    def __init__(self, input_dim, c=None, nu=0.1, lr=1e-3):\n",
    "        self.encoder = Encoder(input_dim).to(device)\n",
    "        self.c = c  # center (torch tensor) or None -> init from data\n",
    "        self.nu = nu\n",
    "        self.optimizer = optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "        self.criterion = lambda z, c: ((z - c)**2).sum(dim=1)  # per-sample squared dist\n",
    "\n",
    "    def init_center_c(self, loader):\n",
    "        # initialize center as mean of encoder outputs on normal data\n",
    "        self.encoder.eval()\n",
    "        n = 0\n",
    "        c_sum = None\n",
    "        with torch.no_grad():\n",
    "            for x in loader:\n",
    "                x = x[0].to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                if c_sum is None:\n",
    "                    c_sum = z.sum(dim=0)\n",
    "                else:\n",
    "                    c_sum += z.sum(dim=0)\n",
    "                n += z.size(0)\n",
    "        c = c_sum / n\n",
    "        # avoid components too close to zero\n",
    "        c[(abs(c) < 1e-6)] = 1e-6\n",
    "        self.c = c\n",
    "\n",
    "    def train(self, loader, epochs=50):\n",
    "        if self.c is None:\n",
    "            self.init_center_c(loader)\n",
    "        self.encoder.train()\n",
    "        for ep in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for x, in loader:\n",
    "                x = x.to(device).float()\n",
    "                z = self.encoder(x)\n",
    "                dist = self.criterion(z, self.c)\n",
    "                loss = dist.mean()  # minimize avg distance of normal samples\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item() * x.size(0)\n",
    "            # print progress\n",
    "            if ep % 5 == 0:\n",
    "                print(f\"DeepSVDD epoch {ep}/{epochs} loss: {epoch_loss/len(loader.dataset):.6f}\")\n",
    "\n",
    "    def score(self, X):  # X: numpy array\n",
    "        self.encoder.eval()\n",
    "        ds = TensorDataset(torch.from_numpy(X).float())\n",
    "        loader = DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                x = batch[0].to(device)\n",
    "                z = self.encoder(x)\n",
    "                dist = ((z - self.c) ** 2).sum(dim=1)  # per-sample\n",
    "                scores.append(dist.cpu().numpy())\n",
    "        return np.concatenate(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e819807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSVDD epoch 0/50 loss: 0.016185\n",
      "DeepSVDD epoch 5/50 loss: 0.000109\n",
      "DeepSVDD epoch 10/50 loss: 0.000032\n",
      "DeepSVDD epoch 15/50 loss: 0.000010\n",
      "DeepSVDD epoch 20/50 loss: 0.000005\n",
      "DeepSVDD epoch 25/50 loss: 0.000003\n",
      "DeepSVDD epoch 30/50 loss: 0.000002\n",
      "DeepSVDD epoch 35/50 loss: 0.000002\n",
      "DeepSVDD epoch 40/50 loss: 0.000001\n",
      "DeepSVDD epoch 45/50 loss: 0.000001\n"
     ]
    }
   ],
   "source": [
    "# Train DeepSVDD on *normal* training samples only (y_train==0)\n",
    "# Convert DataFrame → NumPy → Tensor\n",
    "X_train_normal_np = X_train[y_train == 0].to_numpy().astype(\"float32\")\n",
    "X_test_np = X_test.to_numpy().astype(\"float32\")\n",
    "\n",
    "\n",
    "ds = TensorDataset(torch.from_numpy(X_train_normal_np).float())\n",
    "loader = DataLoader(ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "svdd = DeepSVDD(input_dim=X_train.shape[1])\n",
    "svdd.init_center_c(DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024))\n",
    "svdd.train(loader, epochs=50)\n",
    "\n",
    "# get scores on test set (higher = more anomalous)\n",
    "# Convert X_test and X_train to NumPy arrays\n",
    "X_test_np = X_test.to_numpy().astype(\"float32\")\n",
    "X_train_np = X_train.to_numpy().astype(\"float32\")\n",
    "\n",
    "# Then score\n",
    "svdd_scores_test = svdd.score(X_test_np)\n",
    "svdd_scores_train = svdd.score(X_train_np)  # optional for thresholding\n",
    " # optional for thresholding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26629e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DeepSVDD ===\n",
      "Accuracy: 0.8067\n",
      "ROC-AUC: 0.5375\n",
      "Precision: 0.3761\n",
      "Recall: 0.1539\n",
      "F1-score: 0.2184\n",
      "Confusion Matrix:\n",
      "[[16081   924]\n",
      " [ 3063   557]]\n"
     ]
    }
   ],
   "source": [
    "# # Threshold for binary prediction\n",
    "# # Usually you can set it based on 95th percentile of training normal scores\n",
    "# threshold = np.percentile(svdd_scores_train[y_train == 0], 95)\n",
    "# svdd_pred = (svdd_scores_test > threshold).astype(int)\n",
    "\n",
    "# # Evaluation metrics\n",
    "# y_test_binary = (y_test != 0).astype(int)  # treat all non-normal as anomaly\n",
    "# print(\"=== DeepSVDD ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_test_binary, svdd_pred):.4f}\")\n",
    "# print(f\"ROC-AUC: {roc_auc_score(y_test_binary, svdd_scores_test):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_test_binary, svdd_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_test_binary, svdd_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_test_binary, svdd_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test_binary, svdd_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b29f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5301 - val_loss: 0.3875\n",
      "Epoch 2/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2390 - val_loss: 0.2016\n",
      "Epoch 3/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1792 - val_loss: 0.1800\n",
      "Epoch 4/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1532 - val_loss: 0.1555\n",
      "Epoch 5/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1262 - val_loss: 0.1799\n",
      "Epoch 6/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1233 - val_loss: 0.1379\n",
      "Epoch 7/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1077 - val_loss: 0.1748\n",
      "Epoch 8/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1106 - val_loss: 0.1154\n",
      "Epoch 9/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0997 - val_loss: 0.1124\n",
      "Epoch 10/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0849 - val_loss: 0.1509\n",
      "Epoch 11/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0830 - val_loss: 0.1030\n",
      "Epoch 12/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0712 - val_loss: 0.1012\n",
      "Epoch 13/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0661 - val_loss: 0.1049\n",
      "Epoch 14/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0775 - val_loss: 0.0960\n",
      "Epoch 15/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0645 - val_loss: 0.0983\n",
      "Epoch 16/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0624 - val_loss: 0.0819\n",
      "Epoch 17/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0566 - val_loss: 0.1307\n",
      "Epoch 18/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0577 - val_loss: 0.0853\n",
      "Epoch 19/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0578 - val_loss: 0.0798\n",
      "Epoch 20/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499 - val_loss: 0.0819\n",
      "Epoch 21/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0553 - val_loss: 0.0749\n",
      "Epoch 22/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0567 - val_loss: 0.0735\n",
      "Epoch 23/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0544 - val_loss: 0.0811\n",
      "Epoch 24/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0478 - val_loss: 0.0712\n",
      "Epoch 25/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0500 - val_loss: 0.0689\n",
      "Epoch 26/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0503 - val_loss: 0.0798\n",
      "Epoch 27/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0489 - val_loss: 0.0901\n",
      "Epoch 28/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0475 - val_loss: 0.0613\n",
      "Epoch 29/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0542 - val_loss: 0.0735\n",
      "Epoch 30/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0460 - val_loss: 0.0634\n",
      "Epoch 31/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0367 - val_loss: 0.0717\n",
      "Epoch 32/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0405 - val_loss: 0.0732\n",
      "Epoch 33/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0391 - val_loss: 0.0603\n",
      "Epoch 34/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0444 - val_loss: 0.0595\n",
      "Epoch 35/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0389 - val_loss: 0.0593\n",
      "Epoch 36/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0467 - val_loss: 0.0662\n",
      "Epoch 37/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0320 - val_loss: 0.0582\n",
      "Epoch 38/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0337 - val_loss: 0.0524\n",
      "Epoch 39/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0325 - val_loss: 0.0500\n",
      "Epoch 40/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0342 - val_loss: 0.0580\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 500us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step\n",
      "Epoch 1/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.9974 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 0.0029\n",
      "Epoch 2/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.9551e-06 - val_accuracy: 1.0000 - val_loss: 2.7116e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.6419e-06 - val_accuracy: 1.0000 - val_loss: 1.7147e-06\n",
      "Epoch 4/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.2951e-06 - val_accuracy: 1.0000 - val_loss: 9.3555e-07\n",
      "Epoch 5/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.0352e-06 - val_accuracy: 1.0000 - val_loss: 7.6194e-07\n",
      "Epoch 6/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.9614e-07 - val_accuracy: 1.0000 - val_loss: 5.3316e-07\n",
      "Epoch 7/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 6.9478e-07 - val_accuracy: 1.0000 - val_loss: 3.9595e-07\n",
      "Epoch 8/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.5199e-07 - val_accuracy: 1.0000 - val_loss: 3.0881e-07\n",
      "Epoch 9/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.0213e-07 - val_accuracy: 1.0000 - val_loss: 3.0058e-07\n",
      "Epoch 10/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 3.9243e-07 - val_accuracy: 1.0000 - val_loss: 2.4511e-07\n",
      "Epoch 11/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.5197e-07 - val_accuracy: 1.0000 - val_loss: 2.3657e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.8428e-07 - val_accuracy: 1.0000 - val_loss: 1.5650e-07\n",
      "Epoch 13/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.4266e-07 - val_accuracy: 1.0000 - val_loss: 1.3884e-07\n",
      "Epoch 14/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.9337e-07 - val_accuracy: 1.0000 - val_loss: 1.1523e-07\n",
      "Epoch 15/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.7470e-07 - val_accuracy: 1.0000 - val_loss: 1.1671e-07\n",
      "Epoch 16/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.4674e-07 - val_accuracy: 1.0000 - val_loss: 7.3257e-08\n",
      "Epoch 17/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.3264e-07 - val_accuracy: 1.0000 - val_loss: 9.6660e-08\n",
      "Epoch 18/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.0656e-07 - val_accuracy: 1.0000 - val_loss: 5.4172e-08\n",
      "Epoch 19/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 8.6650e-08 - val_accuracy: 1.0000 - val_loss: 4.9754e-08\n",
      "Epoch 20/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.8366e-08 - val_accuracy: 1.0000 - val_loss: 3.9571e-08\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 875us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 885us/step\n",
      "DeepSVDD epoch 0/50 loss: 0.026542\n",
      "DeepSVDD epoch 5/50 loss: 0.000173\n",
      "DeepSVDD epoch 10/50 loss: 0.000050\n",
      "DeepSVDD epoch 15/50 loss: 0.000022\n",
      "DeepSVDD epoch 20/50 loss: 0.000010\n",
      "DeepSVDD epoch 25/50 loss: 0.000000\n",
      "DeepSVDD epoch 30/50 loss: 0.000000\n",
      "DeepSVDD epoch 35/50 loss: 0.000000\n",
      "DeepSVDD epoch 40/50 loss: 0.000000\n",
      "DeepSVDD epoch 45/50 loss: 0.000000\n",
      "=== Stacking Ensemble (Random Forest) ===\n",
      "Accuracy: 0.9525\n",
      "Precision: 0.8800\n",
      "Recall: 0.8448\n",
      "F1-score: 0.8620\n",
      "Confusion Matrix:\n",
      " [[16588   417]\n",
      " [  562  3058]]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# from tensorflow.keras import layers, Model\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# # =========================\n",
    "# # 1️⃣ Base model: Isolation Forest\n",
    "# # =========================\n",
    "# if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "# if_model.fit(X_train[y_train == 0])  # train only on normal samples\n",
    "\n",
    "# # anomaly scores (more negative = more anomalous)\n",
    "# if_scores_train = -if_model.decision_function(X_train)\n",
    "# if_scores_test  = -if_model.decision_function(X_test)\n",
    "\n",
    "# # =========================\n",
    "# # 2️⃣ Base model: Autoencoder\n",
    "# # =========================\n",
    "# # Add IF score as extra feature\n",
    "# X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "# X_test_if  = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "# input_dim = X_train_if.shape[1]\n",
    "\n",
    "# # Define AE\n",
    "# input_layer = layers.Input(shape=(input_dim,))\n",
    "# x = layers.Dense(64, activation='relu')(input_layer)\n",
    "# x = layers.Dense(32, activation='relu')(x)\n",
    "# latent = layers.Dense(16, activation='relu')(x)\n",
    "# x = layers.Dense(32, activation='relu')(latent)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "# output = layers.Dense(input_dim, activation='linear')(x)\n",
    "# autoencoder = Model(input_layer, output)\n",
    "# autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Train AE only on normal samples\n",
    "# autoencoder.fit(\n",
    "#     X_train_if[y_train == 0], X_train_if[y_train == 0],\n",
    "#     epochs=40,\n",
    "#     batch_size=256,\n",
    "#     validation_split=0.1,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # AE reconstruction error\n",
    "# X_train_pred = autoencoder.predict(X_train_if)\n",
    "# X_test_pred  = autoencoder.predict(X_test_if)\n",
    "# reconstruction_error_train = np.mean(np.square(X_train_if - X_train_pred), axis=1)\n",
    "# reconstruction_error_test  = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "# # CNN based Autoencoder can also be used for better performance\n",
    "# import numpy as np\n",
    "\n",
    "# # Convert to 3D tensor for 1D CNN: (samples, timesteps, channels)\n",
    "# X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "# X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "\n",
    "# y_train_cnn = y_train\n",
    "# y_test_cnn  = y_test\n",
    "\n",
    "# input_layer = layers.Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))\n",
    "# x = layers.Conv1D(32, kernel_size=3, activation='relu')(input_layer)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "# x = layers.Conv1D(16, kernel_size=3, activation='relu')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# latent = layers.Dense(16, activation='relu')(x)\n",
    "# output = layers.Dense(1, activation='sigmoid')(latent)\n",
    "\n",
    "# cnn_model = models.Model(input_layer, output)\n",
    "# cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# cnn_model = Model(input_layer, output)\n",
    "# cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# cnn_model.fit(\n",
    "#     X_train_cnn[y_train==0], y_train_cnn[y_train==0],  # semi-supervised if you want\n",
    "#     epochs=20,\n",
    "#     batch_size=256,\n",
    "#     validation_split=0.1,\n",
    "#     verbose=1\n",
    "# )\n",
    "# cnn_scores_train = cnn_model.predict(X_train_cnn).flatten()\n",
    "# cnn_scores_test  = cnn_model.predict(X_test_cnn).flatten()\n",
    "\n",
    "# # =========================\n",
    "# # 3️⃣ Base model: Deep SVDD\n",
    "# # =========================\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# class Encoder(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "#         super().__init__()\n",
    "#         layers_list = []\n",
    "#         prev = input_dim\n",
    "#         for h in hidden:\n",
    "#             layers_list.append(torch.nn.Linear(prev, h))\n",
    "#             layers_list.append(torch.nn.ReLU())\n",
    "#             prev = h\n",
    "#         layers_list.append(torch.nn.Linear(prev, out_dim))\n",
    "#         self.net = torch.nn.Sequential(*layers_list)\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# class DeepSVDD:\n",
    "#     def __init__(self, input_dim, c=None, nu=0.1, lr=1e-3):\n",
    "#         self.encoder = Encoder(input_dim).to(device)\n",
    "#         self.c = c\n",
    "#         self.nu = nu\n",
    "#         self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "#         self.criterion = lambda z, c: ((z - c)**2).sum(dim=1)\n",
    "\n",
    "#     def init_center_c(self, loader):\n",
    "#         self.encoder.eval()\n",
    "#         n = 0\n",
    "#         c_sum = None\n",
    "#         with torch.no_grad():\n",
    "#             for x in loader:\n",
    "#                 x = x[0].to(device).float()\n",
    "#                 z = self.encoder(x)\n",
    "#                 c_sum = z.sum(dim=0) if c_sum is None else c_sum + z.sum(dim=0)\n",
    "#                 n += z.size(0)\n",
    "#         c = c_sum / n\n",
    "#         c[(abs(c) < 1e-6)] = 1e-6\n",
    "#         self.c = c\n",
    "\n",
    "#     def train(self, loader, epochs=50):\n",
    "#         if self.c is None:\n",
    "#             self.init_center_c(loader)\n",
    "#         self.encoder.train()\n",
    "#         for ep in range(epochs):\n",
    "#             epoch_loss = 0.0\n",
    "#             for x, in loader:\n",
    "#                 x = x.to(device).float()\n",
    "#                 z = self.encoder(x)\n",
    "#                 dist = self.criterion(z, self.c)\n",
    "#                 loss = dist.mean()\n",
    "#                 self.optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 self.optimizer.step()\n",
    "#                 epoch_loss += loss.item() * x.size(0)\n",
    "#             if ep % 5 == 0:\n",
    "#                 print(f\"DeepSVDD epoch {ep}/{epochs} loss: {epoch_loss/len(loader.dataset):.6f}\")\n",
    "\n",
    "#     def score(self, X):  # X must be np.ndarray\n",
    "#         self.encoder.eval()\n",
    "#         ds = TensorDataset(torch.from_numpy(X).float())\n",
    "#         loader = DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "#         scores = []\n",
    "#         with torch.no_grad():\n",
    "#             for batch in loader:\n",
    "#                 x = batch[0].to(device)\n",
    "#                 z = self.encoder(x)\n",
    "#                 dist = ((z - self.c)**2).sum(dim=1)\n",
    "#                 scores.append(dist.cpu().numpy())\n",
    "#         return np.concatenate(scores)\n",
    "\n",
    "# # Train DeepSVDD\n",
    "# X_train_normal_np = X_train_if[y_train == 0].astype(\"float32\")\n",
    "# loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "# svdd = DeepSVDD(input_dim=input_dim)\n",
    "# svdd.init_center_c(loader)\n",
    "# svdd.train(loader, epochs=50)\n",
    "\n",
    "# svdd_scores_train = svdd.score(X_train_if.astype(\"float32\"))\n",
    "# svdd_scores_test  = svdd.score(X_test_if.astype(\"float32\"))\n",
    "# from scipy.stats import zscore\n",
    "\n",
    "# # Z-score normalization\n",
    "# if_scores_train_z = zscore(if_scores_train)\n",
    "# reconstruction_error_train_z = zscore(reconstruction_error_train)\n",
    "# if_scores_test_z = zscore(if_scores_test)\n",
    "# reconstruction_error_test_z = zscore(reconstruction_error_test)\n",
    "\n",
    "# # Stack meta-features\n",
    "# X_meta_train = np.vstack([\n",
    "#     if_scores_train,\n",
    "#     if_scores_train_z,\n",
    "#     reconstruction_error_train,\n",
    "#     reconstruction_error_train_z,\n",
    "#     svdd_scores_train\n",
    "# ]).T\n",
    "\n",
    "# X_meta_test = np.vstack([\n",
    "#     if_scores_test,\n",
    "#     if_scores_test_z,\n",
    "#     reconstruction_error_test,\n",
    "#     reconstruction_error_test_z,\n",
    "#     svdd_scores_test\n",
    "# ]).T\n",
    "\n",
    "# y_meta_train = y_train  # binary labels\n",
    "# y_meta_test  = y_test\n",
    "\n",
    "# # =========================\n",
    "# # 4️⃣ Stacking ensemble\n",
    "# # =========================\n",
    "# X_meta_train = np.vstack([if_scores_train, reconstruction_error_train, svdd_scores_train, cnn_scores_train]).T\n",
    "# X_meta_test  = np.vstack([if_scores_test, reconstruction_error_test, svdd_scores_test, cnn_scores_test]).T\n",
    "# y_meta_train = y_train\n",
    "# y_meta_test  = y_test\n",
    "\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "\n",
    "# meta_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "# meta_model.fit(X_meta_train, y_meta_train)\n",
    "# meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "# print(\"=== Stacking Ensemble (Random Forest) ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, meta_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, meta_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, meta_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, meta_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, meta_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f700a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stacking Ensemble (XGBoost on meta-features) ===\n",
      "Accuracy: 0.9114\n",
      "Precision: 0.8238\n",
      "Recall: 0.6301\n",
      "F1-score: 0.7140\n",
      "Confusion Matrix:\n",
      " [[16517   488]\n",
      " [ 1339  2281]]\n"
     ]
    }
   ],
   "source": [
    "# import xgboost as xgb\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# # Gradient Boosting meta-classifier\n",
    "# xgb_model = xgb.XGBClassifier(\n",
    "#     n_estimators=200,\n",
    "#     max_depth=3,\n",
    "#     learning_rate=0.1,\n",
    "#     random_state=42,\n",
    "#     use_label_encoder=False,\n",
    "#     eval_metric='logloss'\n",
    "# )\n",
    "\n",
    "# # Option 1: Train XGBoost on the same meta-features\n",
    "# xgb_model.fit(X_meta_train, y_meta_train)\n",
    "# xgb_pred = xgb_model.predict(X_meta_test)\n",
    "\n",
    "# print(\"=== Stacking Ensemble (XGBoost on meta-features) ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, xgb_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, xgb_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, xgb_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, xgb_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, xgb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5074 - val_loss: 0.3378\n",
      "Epoch 2/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2330 - val_loss: 0.2100\n",
      "Epoch 3/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1740 - val_loss: 0.1726\n",
      "Epoch 4/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1539 - val_loss: 0.1623\n",
      "Epoch 5/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1265 - val_loss: 0.1399\n",
      "Epoch 6/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1119 - val_loss: 0.1356\n",
      "Epoch 7/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1016 - val_loss: 0.1140\n",
      "Epoch 8/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0929 - val_loss: 0.1061\n",
      "Epoch 9/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0827 - val_loss: 0.0932\n",
      "Epoch 10/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0794 - val_loss: 0.1086\n",
      "Epoch 11/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0864 - val_loss: 0.0996\n",
      "Epoch 12/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0747 - val_loss: 0.0892\n",
      "Epoch 13/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0705 - val_loss: 0.0835\n",
      "Epoch 14/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0696 - val_loss: 0.0944\n",
      "Epoch 15/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0681 - val_loss: 0.0970\n",
      "Epoch 16/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0665 - val_loss: 0.0823\n",
      "Epoch 17/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0568 - val_loss: 0.0782\n",
      "Epoch 18/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0617 - val_loss: 0.0769\n",
      "Epoch 19/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0559 - val_loss: 0.0738\n",
      "Epoch 20/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0519 - val_loss: 0.0883\n",
      "Epoch 21/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0499 - val_loss: 0.0698\n",
      "Epoch 22/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0514 - val_loss: 0.1129\n",
      "Epoch 23/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0507 - val_loss: 0.0822\n",
      "Epoch 24/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0539 - val_loss: 0.0605\n",
      "Epoch 25/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0534 - val_loss: 0.0586\n",
      "Epoch 26/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0421 - val_loss: 0.0664\n",
      "Epoch 27/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0397 - val_loss: 0.0776\n",
      "Epoch 28/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0431 - val_loss: 0.0620\n",
      "Epoch 29/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0410 - val_loss: 0.0600\n",
      "Epoch 30/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0422 - val_loss: 0.0649\n",
      "Epoch 31/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0415 - val_loss: 0.0639\n",
      "Epoch 32/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0418 - val_loss: 0.0563\n",
      "Epoch 33/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0382 - val_loss: 0.0613\n",
      "Epoch 34/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0381 - val_loss: 0.0660\n",
      "Epoch 35/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0411 - val_loss: 0.0592\n",
      "Epoch 36/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0572 - val_loss: 0.1094\n",
      "Epoch 37/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0442 - val_loss: 0.0638\n",
      "Epoch 38/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0332 - val_loss: 0.0619\n",
      "Epoch 39/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0327 - val_loss: 0.0515\n",
      "Epoch 40/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0337 - val_loss: 0.0603\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 450us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step\n",
      "Epoch 1/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.9977 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 0.0092\n",
      "Epoch 2/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 6.0354e-06 - val_accuracy: 1.0000 - val_loss: 1.0053e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 4.1414e-06 - val_accuracy: 1.0000 - val_loss: 8.1064e-06\n",
      "Epoch 4/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 3.1538e-06 - val_accuracy: 1.0000 - val_loss: 3.3039e-06\n",
      "Epoch 5/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 2.4165e-06 - val_accuracy: 1.0000 - val_loss: 2.5121e-06\n",
      "Epoch 6/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.7389e-06 - val_accuracy: 1.0000 - val_loss: 1.8270e-06\n",
      "Epoch 7/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.3943e-06 - val_accuracy: 1.0000 - val_loss: 1.3413e-06\n",
      "Epoch 8/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.1307e-06 - val_accuracy: 1.0000 - val_loss: 1.1753e-06\n",
      "Epoch 9/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 9.0774e-07 - val_accuracy: 1.0000 - val_loss: 9.2735e-07\n",
      "Epoch 10/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 7.3588e-07 - val_accuracy: 1.0000 - val_loss: 8.4034e-07\n",
      "Epoch 11/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 6.3200e-07 - val_accuracy: 1.0000 - val_loss: 6.3632e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 4.9561e-07 - val_accuracy: 1.0000 - val_loss: 5.0954e-07\n",
      "Epoch 13/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 4.1591e-07 - val_accuracy: 1.0000 - val_loss: 3.3597e-07\n",
      "Epoch 14/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 3.5385e-07 - val_accuracy: 1.0000 - val_loss: 3.5306e-07\n",
      "Epoch 15/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.1389e-07 - val_accuracy: 1.0000 - val_loss: 2.4493e-07\n",
      "Epoch 16/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 2.6698e-07 - val_accuracy: 1.0000 - val_loss: 2.5394e-07\n",
      "Epoch 17/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.3087e-07 - val_accuracy: 1.0000 - val_loss: 2.7447e-07\n",
      "Epoch 18/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 1.8822e-07 - val_accuracy: 1.0000 - val_loss: 1.7249e-07\n",
      "Epoch 19/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 1.6955e-07 - val_accuracy: 1.0000 - val_loss: 1.6972e-07\n",
      "Epoch 20/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 1.4967e-07 - val_accuracy: 1.0000 - val_loss: 1.5103e-07\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "DeepSVDD epoch 0/50 loss: 0.027364\n",
      "DeepSVDD epoch 5/50 loss: 0.000161\n",
      "DeepSVDD epoch 10/50 loss: 0.000057\n",
      "DeepSVDD epoch 15/50 loss: 0.000035\n",
      "DeepSVDD epoch 20/50 loss: 0.000021\n",
      "DeepSVDD epoch 25/50 loss: 0.000015\n",
      "DeepSVDD epoch 30/50 loss: 0.000010\n",
      "DeepSVDD epoch 35/50 loss: 0.000006\n",
      "DeepSVDD epoch 40/50 loss: 0.000005\n",
      "DeepSVDD epoch 45/50 loss: 0.000004\n",
      "Optimal threshold: 0.530\n",
      "=== Stacking Ensemble + Threshold Tuning ===\n",
      "Accuracy: 0.9516\n",
      "Precision: 0.8842\n",
      "Recall: 0.8331\n",
      "F1-score: 0.8579\n",
      "Confusion Matrix:\n",
      " [[16610   395]\n",
      " [  604  3016]]\n",
      "=== Two-Level Stacking Ensemble (RF + XGBoost) ===\n",
      "Accuracy: 0.9506\n",
      "Precision: 0.8669\n",
      "Recall: 0.8489\n",
      "F1-score: 0.8578\n",
      "Confusion Matrix:\n",
      " [[16533   472]\n",
      " [  547  3073]]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# from tensorflow.keras import layers, Model\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import xgboost as xgb\n",
    "# from scipy.stats import zscore\n",
    "\n",
    "# # =========================\n",
    "# # 1️⃣ Base model: Isolation Forest\n",
    "# # =========================\n",
    "# if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "# if_model.fit(X_train[y_train == 0])  # train only on normal samples\n",
    "\n",
    "# # anomaly scores\n",
    "# if_scores_train = -if_model.decision_function(X_train)\n",
    "# if_scores_test  = -if_model.decision_function(X_test)\n",
    "\n",
    "# # =========================\n",
    "# # 2️⃣ Base model: Autoencoder\n",
    "# # =========================\n",
    "# X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "# X_test_if  = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "# input_dim = X_train_if.shape[1]\n",
    "\n",
    "# input_layer = layers.Input(shape=(input_dim,))\n",
    "# x = layers.Dense(64, activation='relu')(input_layer)\n",
    "# x = layers.Dense(32, activation='relu')(x)\n",
    "# latent = layers.Dense(16, activation='relu')(x)\n",
    "# x = layers.Dense(32, activation='relu')(latent)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "# output = layers.Dense(input_dim, activation='linear')(x)\n",
    "# autoencoder = Model(input_layer, output)\n",
    "# autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# autoencoder.fit(\n",
    "#     X_train_if[y_train == 0], X_train_if[y_train == 0],\n",
    "#     epochs=40, batch_size=256, validation_split=0.1, verbose=1\n",
    "# )\n",
    "\n",
    "# X_train_pred = autoencoder.predict(X_train_if)\n",
    "# X_test_pred  = autoencoder.predict(X_test_if)\n",
    "# reconstruction_error_train = np.mean(np.square(X_train_if - X_train_pred), axis=1)\n",
    "# reconstruction_error_test  = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "# # =========================\n",
    "# # 3️⃣ Base model: CNN\n",
    "# # =========================\n",
    "# X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "# X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "\n",
    "# input_layer = layers.Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))\n",
    "# x = layers.Conv1D(32, kernel_size=3, activation='relu')(input_layer)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "# x = layers.Conv1D(16, kernel_size=3, activation='relu')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# latent = layers.Dense(16, activation='relu')(x)\n",
    "# output = layers.Dense(1, activation='sigmoid')(latent)\n",
    "\n",
    "# cnn_model = Model(input_layer, output)\n",
    "# cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# cnn_model.fit(\n",
    "#     X_train_cnn[y_train==0], y_train[y_train==0],\n",
    "#     epochs=20, batch_size=256, validation_split=0.1, verbose=1\n",
    "# )\n",
    "\n",
    "# cnn_scores_train = cnn_model.predict(X_train_cnn).flatten()\n",
    "# cnn_scores_test  = cnn_model.predict(X_test_cnn).flatten()\n",
    "\n",
    "# # =========================\n",
    "# # 4️⃣ Base model: Deep SVDD\n",
    "# # =========================\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# class Encoder(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "#         super().__init__()\n",
    "#         layers_list = []\n",
    "#         prev = input_dim\n",
    "#         for h in hidden:\n",
    "#             layers_list.append(torch.nn.Linear(prev, h))\n",
    "#             layers_list.append(torch.nn.ReLU())\n",
    "#             prev = h\n",
    "#         layers_list.append(torch.nn.Linear(prev, out_dim))\n",
    "#         self.net = torch.nn.Sequential(*layers_list)\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# class DeepSVDD:\n",
    "#     def __init__(self, input_dim, c=None, nu=0.1, lr=1e-3):\n",
    "#         self.encoder = Encoder(input_dim).to(device)\n",
    "#         self.c = c\n",
    "#         self.nu = nu\n",
    "#         self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "#         self.criterion = lambda z, c: ((z - c)**2).sum(dim=1)\n",
    "#     def init_center_c(self, loader):\n",
    "#         self.encoder.eval()\n",
    "#         n = 0\n",
    "#         c_sum = None\n",
    "#         with torch.no_grad():\n",
    "#             for x in loader:\n",
    "#                 x = x[0].to(device).float()\n",
    "#                 z = self.encoder(x)\n",
    "#                 c_sum = z.sum(dim=0) if c_sum is None else c_sum + z.sum(dim=0)\n",
    "#                 n += z.size(0)\n",
    "#         c = c_sum / n\n",
    "#         c[(abs(c) < 1e-6)] = 1e-6\n",
    "#         self.c = c\n",
    "#     def train(self, loader, epochs=50):\n",
    "#         if self.c is None:\n",
    "#             self.init_center_c(loader)\n",
    "#         self.encoder.train()\n",
    "#         for ep in range(epochs):\n",
    "#             epoch_loss = 0.0\n",
    "#             for x, in loader:\n",
    "#                 x = x.to(device).float()\n",
    "#                 z = self.encoder(x)\n",
    "#                 dist = self.criterion(z, self.c)\n",
    "#                 loss = dist.mean()\n",
    "#                 self.optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 self.optimizer.step()\n",
    "#                 epoch_loss += loss.item() * x.size(0)\n",
    "#             if ep % 5 == 0:\n",
    "#                 print(f\"DeepSVDD epoch {ep}/{epochs} loss: {epoch_loss/len(loader.dataset):.6f}\")\n",
    "#     def score(self, X):\n",
    "#         self.encoder.eval()\n",
    "#         ds = TensorDataset(torch.from_numpy(X).float())\n",
    "#         loader = DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "#         scores = []\n",
    "#         with torch.no_grad():\n",
    "#             for batch in loader:\n",
    "#                 x = batch[0].to(device)\n",
    "#                 z = self.encoder(x)\n",
    "#                 dist = ((z - self.c)**2).sum(dim=1)\n",
    "#                 scores.append(dist.cpu().numpy())\n",
    "#         return np.concatenate(scores)\n",
    "\n",
    "# X_train_normal_np = X_train_if[y_train==0].astype(\"float32\")\n",
    "# loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "# svdd = DeepSVDD(input_dim=input_dim)\n",
    "# svdd.init_center_c(loader)\n",
    "# svdd.train(loader, epochs=50)\n",
    "\n",
    "# svdd_scores_train = svdd.score(X_train_if.astype(\"float32\"))\n",
    "# svdd_scores_test  = svdd.score(X_test_if.astype(\"float32\"))\n",
    "\n",
    "# # =========================\n",
    "# # 5️⃣ Stack meta-features\n",
    "# # =========================\n",
    "# X_meta_train = np.vstack([\n",
    "#     if_scores_train,\n",
    "#     reconstruction_error_train,\n",
    "#     svdd_scores_train,\n",
    "#     cnn_scores_train\n",
    "# ]).T\n",
    "# X_meta_test = np.vstack([\n",
    "#     if_scores_test,\n",
    "#     reconstruction_error_test,\n",
    "#     svdd_scores_test,\n",
    "#     cnn_scores_test\n",
    "# ]).T\n",
    "\n",
    "\n",
    "\n",
    "# # Train meta-model\n",
    "# meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# # --- Step 1: Get probabilities ---\n",
    "# probs = meta_model.predict_proba(X_meta_test)[:, 1]\n",
    "\n",
    "# # --- Step 2: Find best threshold ---\n",
    "# from sklearn.metrics import precision_recall_curve, f1_score\n",
    "# precision, recall, thresholds = precision_recall_curve(y_meta_test, probs)\n",
    "# f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "# best_idx = f1_scores.argmax()\n",
    "# best_thresh = thresholds[best_idx]\n",
    "# print(f\"Optimal threshold: {best_thresh:.3f}\")\n",
    "\n",
    "# # --- Step 3: Apply threshold ---\n",
    "# meta_pred_adjusted = (probs >= best_thresh).astype(int)\n",
    "\n",
    "# # --- Step 4: Evaluate ---\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "# print(\"=== Stacking Ensemble + Threshold Tuning ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, meta_pred_adjusted))\n",
    "\n",
    "# # =========================\n",
    "# # 6️⃣ First-level meta-classifier: Random Forest\n",
    "# # =========================\n",
    "# rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "# rf_model.fit(X_meta_train, y_train)\n",
    "# rf_probs_train = rf_model.predict_proba(X_meta_train)[:,1]\n",
    "# rf_probs_test  = rf_model.predict_proba(X_meta_test)[:,1]\n",
    "\n",
    "# # =========================\n",
    "# # 7️⃣ Second-level meta-classifier: XGBoost\n",
    "# # =========================\n",
    "# # Use RF probabilities as extra feature\n",
    "# X_meta_train_xgb = np.vstack([X_meta_train.T, rf_probs_train]).T\n",
    "# X_meta_test_xgb  = np.vstack([X_meta_test.T, rf_probs_test]).T\n",
    "\n",
    "# xgb_model = xgb.XGBClassifier(\n",
    "#     n_estimators=200,\n",
    "#     max_depth=3,\n",
    "#     learning_rate=0.3,\n",
    "#     random_state=42,\n",
    "#     use_label_encoder=False,\n",
    "#     eval_metric='logloss'\n",
    "# )\n",
    "# xgb_model.fit(X_meta_train_xgb, y_train)\n",
    "# xgb_pred = xgb_model.predict(X_meta_test_xgb)\n",
    "\n",
    "# # =========================\n",
    "# # 8️⃣ Evaluate final stacked ensemble\n",
    "# # =========================\n",
    "# print(\"=== Two-Level Stacking Ensemble (RF + XGBoost) ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_test, xgb_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_test, xgb_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_test, xgb_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_test, xgb_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, xgb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a665996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.4948 - val_loss: 0.2806\n",
      "Epoch 2/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2180 - val_loss: 0.2298\n",
      "Epoch 3/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1859 - val_loss: 0.1835\n",
      "Epoch 4/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1590 - val_loss: 0.1761\n",
      "Epoch 5/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1432 - val_loss: 0.1493\n",
      "Epoch 6/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1297 - val_loss: 0.1340\n",
      "Epoch 7/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1184 - val_loss: 0.1260\n",
      "Epoch 8/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1132 - val_loss: 0.1152\n",
      "Epoch 9/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0982 - val_loss: 0.1332\n",
      "Epoch 10/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1100 - val_loss: 0.1186\n",
      "Epoch 11/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0929 - val_loss: 0.1027\n",
      "Epoch 12/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0745 - val_loss: 0.1264\n",
      "Epoch 13/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0715 - val_loss: 0.0869\n",
      "Epoch 14/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0645 - val_loss: 0.0922\n",
      "Epoch 15/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0620 - val_loss: 0.0934\n",
      "Epoch 16/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0647 - val_loss: 0.0784\n",
      "Epoch 17/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0647 - val_loss: 0.1038\n",
      "Epoch 18/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0572 - val_loss: 0.0759\n",
      "Epoch 19/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0555 - val_loss: 0.0821\n",
      "Epoch 20/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0544 - val_loss: 0.0875\n",
      "Epoch 21/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0507 - val_loss: 0.0906\n",
      "Epoch 22/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0521 - val_loss: 0.0740\n",
      "Epoch 23/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0486 - val_loss: 0.0811\n",
      "Epoch 24/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0621 - val_loss: 0.0685\n",
      "Epoch 25/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0445 - val_loss: 0.0644\n",
      "Epoch 26/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0492 - val_loss: 0.0637\n",
      "Epoch 27/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0438 - val_loss: 0.0849\n",
      "Epoch 28/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0474 - val_loss: 0.0661\n",
      "Epoch 29/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0403 - val_loss: 0.0653\n",
      "Epoch 30/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0425 - val_loss: 0.0715\n",
      "Epoch 31/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0410 - val_loss: 0.0554\n",
      "Epoch 32/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0405 - val_loss: 0.0944\n",
      "Epoch 33/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0453 - val_loss: 0.0571\n",
      "Epoch 34/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0360 - val_loss: 0.0501\n",
      "Epoch 35/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0314 - val_loss: 0.0594\n",
      "Epoch 36/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0366 - val_loss: 0.0530\n",
      "Epoch 37/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0388 - val_loss: 0.0522\n",
      "Epoch 38/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0345 - val_loss: 0.0510\n",
      "Epoch 39/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0340 - val_loss: 0.0506\n",
      "Epoch 40/40\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0351 - val_loss: 0.0502\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 584us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step\n",
      "Epoch 1/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9972 - loss: 0.0068 - val_accuracy: 1.0000 - val_loss: 0.0134\n",
      "Epoch 2/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.2121e-05 - val_accuracy: 1.0000 - val_loss: 1.9856e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.5825e-06 - val_accuracy: 1.0000 - val_loss: 1.2441e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.2523e-06 - val_accuracy: 1.0000 - val_loss: 5.7919e-06\n",
      "Epoch 5/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.6878e-06 - val_accuracy: 1.0000 - val_loss: 3.6685e-06\n",
      "Epoch 6/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 2.6404e-06 - val_accuracy: 1.0000 - val_loss: 2.5699e-06\n",
      "Epoch 7/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 1.9859e-06 - val_accuracy: 1.0000 - val_loss: 1.7727e-06\n",
      "Epoch 8/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.5595e-06 - val_accuracy: 1.0000 - val_loss: 1.8104e-06\n",
      "Epoch 9/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.2276e-06 - val_accuracy: 1.0000 - val_loss: 9.3954e-07\n",
      "Epoch 10/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 9.9163e-07 - val_accuracy: 1.0000 - val_loss: 7.2372e-07\n",
      "Epoch 11/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 8.0515e-07 - val_accuracy: 1.0000 - val_loss: 6.3146e-07\n",
      "Epoch 12/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 6.6725e-07 - val_accuracy: 1.0000 - val_loss: 4.9743e-07\n",
      "Epoch 13/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 5.4044e-07 - val_accuracy: 1.0000 - val_loss: 4.6978e-07\n",
      "Epoch 14/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 4.5804e-07 - val_accuracy: 1.0000 - val_loss: 4.4845e-07\n",
      "Epoch 15/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 3.8357e-07 - val_accuracy: 1.0000 - val_loss: 2.7617e-07\n",
      "Epoch 16/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 3.1983e-07 - val_accuracy: 1.0000 - val_loss: 2.6742e-07\n",
      "Epoch 17/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 2.7266e-07 - val_accuracy: 1.0000 - val_loss: 2.0211e-07\n",
      "Epoch 18/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 2.2543e-07 - val_accuracy: 1.0000 - val_loss: 1.8309e-07\n",
      "Epoch 19/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.9361e-07 - val_accuracy: 1.0000 - val_loss: 1.4209e-07\n",
      "Epoch 20/20\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.6730e-07 - val_accuracy: 1.0000 - val_loss: 1.2086e-07\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "DeepSVDD epoch 0/50 loss: 0.021284\n",
      "DeepSVDD epoch 5/50 loss: 0.000167\n",
      "DeepSVDD epoch 10/50 loss: 0.000056\n",
      "DeepSVDD epoch 15/50 loss: 0.000026\n",
      "DeepSVDD epoch 20/50 loss: 0.000015\n",
      "DeepSVDD epoch 25/50 loss: 0.000010\n",
      "DeepSVDD epoch 30/50 loss: 0.000007\n",
      "DeepSVDD epoch 35/50 loss: 0.000005\n",
      "DeepSVDD epoch 40/50 loss: 0.000003\n",
      "DeepSVDD epoch 45/50 loss: 0.000002\n",
      "Optimal threshold: 0.525\n",
      "=== Stacking Ensemble + Threshold Tuning ===\n",
      "Accuracy: 0.9530\n",
      "Precision: 0.8892\n",
      "Recall: 0.8362\n",
      "F1-score: 0.8619\n",
      "Confusion Matrix:\n",
      " [[16628   377]\n",
      " [  593  3027]]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.ensemble import IsolationForest, RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve\n",
    "# from tensorflow.keras import layers, Model\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from scipy.stats import zscore\n",
    "\n",
    "# # =========================\n",
    "# # 1️⃣ Base model: Isolation Forest\n",
    "# # =========================\n",
    "# if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "# if_model.fit(X_train[y_train == 0])  # train only on normal samples\n",
    "\n",
    "# if_scores_train = -if_model.decision_function(X_train)\n",
    "# if_scores_test  = -if_model.decision_function(X_test)\n",
    "\n",
    "# # =========================\n",
    "# # 2️⃣ Base model: Autoencoder\n",
    "# # =========================\n",
    "# X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "# X_test_if  = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "# input_dim = X_train_if.shape[1]\n",
    "\n",
    "# input_layer = layers.Input(shape=(input_dim,))\n",
    "# x = layers.Dense(64, activation='relu')(input_layer)\n",
    "# x = layers.Dense(32, activation='relu')(x)\n",
    "# latent = layers.Dense(16, activation='relu')(x)\n",
    "# x = layers.Dense(32, activation='relu')(latent)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "# output = layers.Dense(input_dim, activation='linear')(x)\n",
    "# autoencoder = Model(input_layer, output)\n",
    "# autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# autoencoder.fit(\n",
    "#     X_train_if[y_train == 0], X_train_if[y_train == 0],\n",
    "#     epochs=40,\n",
    "#     batch_size=256,\n",
    "#     validation_split=0.1,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# X_train_pred = autoencoder.predict(X_train_if)\n",
    "# X_test_pred  = autoencoder.predict(X_test_if)\n",
    "# reconstruction_error_train = np.mean(np.square(X_train_if - X_train_pred), axis=1)\n",
    "# reconstruction_error_test  = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "# # =========================\n",
    "# # 3️⃣ Base model: CNN (optional)\n",
    "# # =========================\n",
    "# X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "# X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "\n",
    "# input_layer = layers.Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))\n",
    "# x = layers.Conv1D(32, kernel_size=3, activation='relu')(input_layer)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "# x = layers.Conv1D(16, kernel_size=3, activation='relu')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# latent = layers.Dense(16, activation='relu')(x)\n",
    "# output = layers.Dense(1, activation='sigmoid')(latent)\n",
    "\n",
    "# cnn_model = Model(input_layer, output)\n",
    "# cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# cnn_model.fit(\n",
    "#     X_train_cnn[y_train==0], y_train[y_train==0],\n",
    "#     epochs=20,\n",
    "#     batch_size=256,\n",
    "#     validation_split=0.1,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# cnn_scores_train = cnn_model.predict(X_train_cnn).flatten()\n",
    "# cnn_scores_test  = cnn_model.predict(X_test_cnn).flatten()\n",
    "\n",
    "# # =========================\n",
    "# # 4️⃣ Base model: Deep SVDD\n",
    "# # =========================\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# class Encoder(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "#         super().__init__()\n",
    "#         layers_list = []\n",
    "#         prev = input_dim\n",
    "#         for h in hidden:\n",
    "#             layers_list.append(torch.nn.Linear(prev, h))\n",
    "#             layers_list.append(torch.nn.ReLU())\n",
    "#             prev = h\n",
    "#         layers_list.append(torch.nn.Linear(prev, out_dim))\n",
    "#         self.net = torch.nn.Sequential(*layers_list)\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# class DeepSVDD:\n",
    "#     def __init__(self, input_dim, c=None, nu=0.1, lr=1e-3):\n",
    "#         self.encoder = Encoder(input_dim).to(device)\n",
    "#         self.c = c\n",
    "#         self.nu = nu\n",
    "#         self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "#         self.criterion = lambda z, c: ((z - c)**2).sum(dim=1)\n",
    "\n",
    "#     def init_center_c(self, loader):\n",
    "#         self.encoder.eval()\n",
    "#         n = 0\n",
    "#         c_sum = None\n",
    "#         with torch.no_grad():\n",
    "#             for x in loader:\n",
    "#                 x = x[0].to(device).float()\n",
    "#                 z = self.encoder(x)\n",
    "#                 c_sum = z.sum(dim=0) if c_sum is None else c_sum + z.sum(dim=0)\n",
    "#                 n += z.size(0)\n",
    "#         c = c_sum / n\n",
    "#         c[(abs(c) < 1e-6)] = 1e-6\n",
    "#         self.c = c\n",
    "\n",
    "#     def train(self, loader, epochs=50):\n",
    "#         if self.c is None:\n",
    "#             self.init_center_c(loader)\n",
    "#         self.encoder.train()\n",
    "#         for ep in range(epochs):\n",
    "#             epoch_loss = 0.0\n",
    "#             for x, in loader:\n",
    "#                 x = x.to(device).float()\n",
    "#                 z = self.encoder(x)\n",
    "#                 dist = self.criterion(z, self.c)\n",
    "#                 loss = dist.mean()\n",
    "#                 self.optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 self.optimizer.step()\n",
    "#                 epoch_loss += loss.item() * x.size(0)\n",
    "#             if ep % 5 == 0:\n",
    "#                 print(f\"DeepSVDD epoch {ep}/{epochs} loss: {epoch_loss/len(loader.dataset):.6f}\")\n",
    "\n",
    "#     def score(self, X):\n",
    "#         self.encoder.eval()\n",
    "#         ds = TensorDataset(torch.from_numpy(X).float())\n",
    "#         loader = DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "#         scores = []\n",
    "#         with torch.no_grad():\n",
    "#             for batch in loader:\n",
    "#                 x = batch[0].to(device)\n",
    "#                 z = self.encoder(x)\n",
    "#                 dist = ((z - self.c)**2).sum(dim=1)\n",
    "#                 scores.append(dist.cpu().numpy())\n",
    "#         return np.concatenate(scores)\n",
    "\n",
    "# X_train_normal_np = X_train_if[y_train==0].astype(\"float32\")\n",
    "# loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "\n",
    "# svdd = DeepSVDD(input_dim=input_dim)\n",
    "# svdd.init_center_c(loader)\n",
    "# svdd.train(loader, epochs=50)\n",
    "\n",
    "# svdd_scores_train = svdd.score(X_train_if.astype(\"float32\"))\n",
    "# svdd_scores_test  = svdd.score(X_test_if.astype(\"float32\"))\n",
    "\n",
    "# # =========================\n",
    "# # 5️⃣ Stack meta-features\n",
    "# # =========================\n",
    "# X_meta_train = np.vstack([if_scores_train, reconstruction_error_train, svdd_scores_train, cnn_scores_train]).T\n",
    "# X_meta_test  = np.vstack([if_scores_test, reconstruction_error_test, svdd_scores_test, cnn_scores_test]).T\n",
    "# y_meta_train = y_train\n",
    "# y_meta_test  = y_test\n",
    "\n",
    "# # =========================\n",
    "# # 6️⃣ Train meta-classifier with threshold tuning\n",
    "# # =========================\n",
    "# meta_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "# meta_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# # --- Get probabilities ---\n",
    "# probs = meta_model.predict_proba(X_meta_test)[:,1]\n",
    "\n",
    "# # --- Precision-recall threshold tuning ---\n",
    "# precision, recall, thresholds = precision_recall_curve(y_meta_test, probs)\n",
    "# f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "# best_idx = f1_scores.argmax()\n",
    "# best_thresh = thresholds[best_idx]\n",
    "# print(f\"Optimal threshold: {best_thresh:.3f}\")\n",
    "\n",
    "# # --- Apply threshold ---\n",
    "# meta_pred_adjusted = (probs >= best_thresh).astype(int)\n",
    "\n",
    "# # --- Evaluate ---\n",
    "# print(\"=== Stacking Ensemble + Threshold Tuning ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, meta_pred_adjusted):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, meta_pred_adjusted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a52522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4973357\ttotal: 174ms\tremaining: 1m 26s\n",
      "100:\tlearn: 0.6078141\ttotal: 2.02s\tremaining: 7.96s\n",
      "200:\tlearn: 0.6646870\ttotal: 4.04s\tremaining: 6.01s\n",
      "300:\tlearn: 0.6849326\ttotal: 5.73s\tremaining: 3.79s\n",
      "400:\tlearn: 0.7126731\ttotal: 7.47s\tremaining: 1.84s\n",
      "499:\tlearn: 0.7346923\ttotal: 9.04s\tremaining: 0us\n",
      "=== Stacking Ensemble (CatBoost) ===\n",
      "Accuracy: 0.9193\n",
      "Precision: 0.8493\n",
      "Recall: 0.6569\n",
      "F1-score: 0.7408\n",
      "Confusion Matrix:\n",
      " [[16583   422]\n",
      " [ 1242  2378]]\n"
     ]
    }
   ],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# # Initialize CatBoost\n",
    "# cat_model = CatBoostClassifier(\n",
    "#     iterations=500,\n",
    "#     depth=4,\n",
    "#     learning_rate=0.05,\n",
    "#     loss_function='Logloss',\n",
    "#     eval_metric='F1',\n",
    "#     random_seed=42,\n",
    "#     verbose=100\n",
    "# )\n",
    "\n",
    "# # Train on meta-features\n",
    "# cat_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# # Predict\n",
    "# cat_pred = cat_model.predict(X_meta_test)\n",
    "\n",
    "# # Evaluation\n",
    "# print(\"=== Stacking Ensemble (CatBoost) ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, cat_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, cat_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, cat_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, cat_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, cat_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6477f525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Stacking Ensemble (Random Forest) ===\n",
      "Accuracy: 0.9525\n",
      "Precision: 0.8826\n",
      "Recall: 0.8412\n",
      "F1-score: 0.8614\n",
      "Confusion Matrix:\n",
      " [[16600   405]\n",
      " [  575  3045]]\n",
      "0:\tlearn: 0.4973357\ttotal: 14.9ms\tremaining: 7.41s\n",
      "100:\tlearn: 0.6078141\ttotal: 1.35s\tremaining: 5.35s\n",
      "200:\tlearn: 0.6646870\ttotal: 2.69s\tremaining: 4s\n",
      "300:\tlearn: 0.6849326\ttotal: 4.08s\tremaining: 2.69s\n",
      "400:\tlearn: 0.7126731\ttotal: 5.46s\tremaining: 1.35s\n",
      "499:\tlearn: 0.7346923\ttotal: 7.01s\tremaining: 0us\n",
      "=== Stacking Ensemble (CatBoost) ===\n",
      "Accuracy: 0.9193\n",
      "Precision: 0.8493\n",
      "Recall: 0.6569\n",
      "F1-score: 0.7408\n",
      "Confusion Matrix:\n",
      " [[16583   422]\n",
      " [ 1242  2378]]\n",
      "=== Ensemble RF + CatBoost ===\n",
      "Accuracy: 0.9280\n",
      "Precision: 0.9406\n",
      "Recall: 0.6296\n",
      "F1-score: 0.7543\n",
      "Confusion Matrix:\n",
      " [[16861   144]\n",
      " [ 1341  2279]]\n"
     ]
    }
   ],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# # --- Random Forest ---\n",
    "# rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "# rf_model.fit(X_meta_train, y_meta_train)\n",
    "# rf_pred = rf_model.predict(X_meta_test)\n",
    "\n",
    "# print(\"=== Stacking Ensemble (Random Forest) ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, rf_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, rf_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, rf_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, rf_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, rf_pred))\n",
    "\n",
    "\n",
    "# # --- CatBoost ---\n",
    "# cat_model = CatBoostClassifier(\n",
    "#     iterations=500,\n",
    "#     depth=4,\n",
    "#     learning_rate=0.05,\n",
    "#     loss_function='Logloss',\n",
    "#     eval_metric='F1',\n",
    "#     random_seed=42,\n",
    "#     verbose=100\n",
    "# )\n",
    "# cat_model.fit(X_meta_train, y_meta_train)\n",
    "# cat_pred = cat_model.predict(X_meta_test)\n",
    "\n",
    "# print(\"=== Stacking Ensemble (CatBoost) ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, cat_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, cat_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, cat_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, cat_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, cat_pred))\n",
    "\n",
    "\n",
    "# # Optional: combine RF + CatBoost predictions (majority vote)\n",
    "# import numpy as np\n",
    "# ensemble_pred = np.round((rf_pred + cat_pred)/2).astype(int)\n",
    "# print(\"=== Ensemble RF + CatBoost ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, ensemble_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef7548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 454us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 974us/step\n",
      "=== Weighted Ensemble (RF + CatBoost) ===\n",
      "Accuracy: 0.9531\n",
      "Precision: 0.8778\n",
      "Recall: 0.8514\n",
      "F1-score: 0.8644\n",
      "Confusion Matrix:\n",
      " [[16576   429]\n",
      " [  538  3082]]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# from tensorflow.keras import layers, Model\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from scipy.stats import zscore\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# # =========================\n",
    "# # 1️⃣ Base model: Isolation Forest\n",
    "# # =========================\n",
    "# if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "# if_model.fit(X_train[y_train == 0])  # train only on normal samples\n",
    "\n",
    "# if_scores_train = -if_model.decision_function(X_train)\n",
    "# if_scores_test  = -if_model.decision_function(X_test)\n",
    "\n",
    "# # =========================\n",
    "# # 2️⃣ Base model: Autoencoder\n",
    "# # =========================\n",
    "# X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "# X_test_if  = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "# input_dim = X_train_if.shape[1]\n",
    "\n",
    "# # Define AE\n",
    "# input_layer = layers.Input(shape=(input_dim,))\n",
    "# x = layers.Dense(64, activation='relu')(input_layer)\n",
    "# x = layers.Dense(32, activation='relu')(x)\n",
    "# latent = layers.Dense(16, activation='relu')(x)\n",
    "# x = layers.Dense(32, activation='relu')(latent)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "# output = layers.Dense(input_dim, activation='linear')(x)\n",
    "# autoencoder = Model(input_layer, output)\n",
    "# autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Train AE only on normal samples\n",
    "# autoencoder.fit(\n",
    "#     X_train_if[y_train == 0], X_train_if[y_train == 0],\n",
    "#     epochs=40, batch_size=256, validation_split=0.1, verbose=0\n",
    "# )\n",
    "\n",
    "# X_train_pred = autoencoder.predict(X_train_if)\n",
    "# X_test_pred  = autoencoder.predict(X_test_if)\n",
    "# reconstruction_error_train = np.mean(np.square(X_train_if - X_train_pred), axis=1)\n",
    "# reconstruction_error_test  = np.mean(np.square(X_test_if - X_test_pred), axis=1)\n",
    "\n",
    "# # =========================\n",
    "# # 3️⃣ Base model: CNN\n",
    "# # =========================\n",
    "# X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "# X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "\n",
    "# input_layer = layers.Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))\n",
    "# x = layers.Conv1D(32, kernel_size=3, activation='relu')(input_layer)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "# x = layers.Conv1D(16, kernel_size=3, activation='relu')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# latent = layers.Dense(16, activation='relu')(x)\n",
    "# output = layers.Dense(1, activation='sigmoid')(latent)\n",
    "\n",
    "# cnn_model = Model(input_layer, output)\n",
    "# cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# cnn_model.fit(\n",
    "#     X_train_cnn[y_train==0], y_train[y_train==0],\n",
    "#     epochs=20, batch_size=256, validation_split=0.1, verbose=0\n",
    "# )\n",
    "\n",
    "# cnn_scores_train = cnn_model.predict(X_train_cnn).flatten()\n",
    "# cnn_scores_test  = cnn_model.predict(X_test_cnn).flatten()\n",
    "\n",
    "# # =========================\n",
    "# # 4️⃣ Base model: Deep SVDD\n",
    "# # =========================\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# class Encoder(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "#         super().__init__()\n",
    "#         layers_list = []\n",
    "#         prev = input_dim\n",
    "#         for h in hidden:\n",
    "#             layers_list.append(torch.nn.Linear(prev, h))\n",
    "#             layers_list.append(torch.nn.ReLU())\n",
    "#             prev = h\n",
    "#         layers_list.append(torch.nn.Linear(prev, out_dim))\n",
    "#         self.net = torch.nn.Sequential(*layers_list)\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# class DeepSVDD:\n",
    "#     def __init__(self, input_dim, c=None, nu=0.1, lr=1e-3):\n",
    "#         self.encoder = Encoder(input_dim).to(device)\n",
    "#         self.c = c\n",
    "#         self.nu = nu\n",
    "#         self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "#         self.criterion = lambda z, c: ((z - c)**2).sum(dim=1)\n",
    "\n",
    "#     def init_center_c(self, loader):\n",
    "#         self.encoder.eval()\n",
    "#         n = 0\n",
    "#         c_sum = None\n",
    "#         with torch.no_grad():\n",
    "#             for x in loader:\n",
    "#                 x = x[0].to(device).float()\n",
    "#                 z = self.encoder(x)\n",
    "#                 c_sum = z.sum(dim=0) if c_sum is None else c_sum + z.sum(dim=0)\n",
    "#                 n += z.size(0)\n",
    "#         c = c_sum / n\n",
    "#         c[(abs(c) < 1e-6)] = 1e-6\n",
    "#         self.c = c\n",
    "\n",
    "#     def train(self, loader, epochs=50):\n",
    "#         if self.c is None:\n",
    "#             self.init_center_c(loader)\n",
    "#         self.encoder.train()\n",
    "#         for ep in range(epochs):\n",
    "#             epoch_loss = 0.0\n",
    "#             for x, in loader:\n",
    "#                 x = x.to(device).float()\n",
    "#                 z = self.encoder(x)\n",
    "#                 dist = self.criterion(z, self.c)\n",
    "#                 loss = dist.mean()\n",
    "#                 self.optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 self.optimizer.step()\n",
    "#                 epoch_loss += loss.item() * x.size(0)\n",
    "\n",
    "#     def score(self, X):\n",
    "#         self.encoder.eval()\n",
    "#         ds = TensorDataset(torch.from_numpy(X).float())\n",
    "#         loader = DataLoader(ds, batch_size=1024, shuffle=False)\n",
    "#         scores = []\n",
    "#         with torch.no_grad():\n",
    "#             for batch in loader:\n",
    "#                 x = batch[0].to(device)\n",
    "#                 z = self.encoder(x)\n",
    "#                 dist = ((z - self.c)**2).sum(dim=1)\n",
    "#                 scores.append(dist.cpu().numpy())\n",
    "#         return np.concatenate(scores)\n",
    "\n",
    "# X_train_normal_np = X_train_if[y_train==0].astype(\"float32\")\n",
    "# loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "# svdd = DeepSVDD(input_dim=input_dim)\n",
    "# svdd.init_center_c(loader)\n",
    "# svdd.train(loader, epochs=50)\n",
    "\n",
    "# svdd_scores_train = svdd.score(X_train_if.astype(\"float32\"))\n",
    "# svdd_scores_test  = svdd.score(X_test_if.astype(\"float32\"))\n",
    "\n",
    "# # =========================\n",
    "# # 5️⃣ Stack meta-features\n",
    "# # =========================\n",
    "# X_meta_train = np.vstack([if_scores_train, reconstruction_error_train, svdd_scores_train, cnn_scores_train]).T\n",
    "# X_meta_test  = np.vstack([if_scores_test, reconstruction_error_test, svdd_scores_test, cnn_scores_test]).T\n",
    "\n",
    "# y_meta_train = y_train\n",
    "# y_meta_test  = y_test\n",
    "\n",
    "# # =========================\n",
    "# # 6️⃣ Train meta-models: RF + CatBoost\n",
    "# # =========================\n",
    "# rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "# cat_model = CatBoostClassifier(n_estimators=500, learning_rate=0.1, verbose=0, random_state=42)\n",
    "\n",
    "# rf_model.fit(X_meta_train, y_meta_train)\n",
    "# cat_model.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# # =========================\n",
    "# # 7️⃣ Weighted ensemble + threshold tuning\n",
    "# # =========================\n",
    "# rf_probs  = rf_model.predict_proba(X_meta_test)[:,1]\n",
    "# cat_probs = cat_model.predict_proba(X_meta_test)[:,1]\n",
    "\n",
    "# # Weighting\n",
    "# w_rf, w_cat = 0.6, 0.4\n",
    "# ensemble_probs = w_rf * rf_probs + w_cat * cat_probs\n",
    "\n",
    "# # Threshold tuning\n",
    "# precision, recall, thresholds = precision_recall_curve(y_meta_test, ensemble_probs)\n",
    "# f1_scores = 2 * (precision*recall)/(precision+recall+1e-8)\n",
    "# best_thresh = thresholds[f1_scores.argmax()]\n",
    "\n",
    "# ensemble_pred = (ensemble_probs >= best_thresh).astype(int)\n",
    "\n",
    "# # =========================\n",
    "# # 8️⃣ Evaluate\n",
    "# # =========================\n",
    "# print(\"=== Weighted Ensemble (RF + CatBoost) ===\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(f\"F1-score: {f1_score(y_meta_test, ensemble_pred):.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(y_meta_test, ensemble_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28249f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 776us/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 798us/step\n",
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "DeepSVDD epoch 0/50 loss: 0.018761\n",
      "DeepSVDD epoch 5/50 loss: 0.000113\n",
      "DeepSVDD epoch 10/50 loss: 0.000043\n",
      "DeepSVDD epoch 15/50 loss: 0.000021\n",
      "DeepSVDD epoch 20/50 loss: 0.000011\n",
      "DeepSVDD epoch 25/50 loss: 0.000004\n",
      "DeepSVDD epoch 30/50 loss: 0.000002\n",
      "DeepSVDD epoch 35/50 loss: 0.000002\n",
      "DeepSVDD epoch 40/50 loss: 0.000001\n",
      "DeepSVDD epoch 45/50 loss: 0.000001\n",
      "=== Weighted XGBoost Stacked Ensemble ===\n",
      "Accuracy: 0.9728\n",
      "Precision: 0.9086\n",
      "Recall: 0.9395\n",
      "F1-score: 0.9238\n",
      "Confusion Matrix:\n",
      " [[16663   342]\n",
      " [  219  3401]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import xgboost as xgb\n",
    "\n",
    "# =========================\n",
    "# 1️⃣ Base models: Isolation Forest + Autoencoder\n",
    "# =========================\n",
    "\n",
    "# Isolation Forest\n",
    "if_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "if_model.fit(X_train[y_train==0])\n",
    "if_scores_train = -if_model.decision_function(X_train)\n",
    "if_scores_test  = -if_model.decision_function(X_test)\n",
    "\n",
    "# Autoencoder (MLP)\n",
    "X_train_if = np.hstack([X_train, if_scores_train.reshape(-1,1)])\n",
    "X_test_if  = np.hstack([X_test, if_scores_test.reshape(-1,1)])\n",
    "input_dim = X_train_if.shape[1]\n",
    "\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "x = layers.Dense(64, activation='relu')(input_layer)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(latent)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "output = layers.Dense(input_dim, activation='linear')(x)\n",
    "autoencoder = Model(input_layer, output)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_train_if[y_train==0], X_train_if[y_train==0], epochs=40, batch_size=256, validation_split=0.1, verbose=0)\n",
    "\n",
    "X_train_pred = autoencoder.predict(X_train_if)\n",
    "X_test_pred  = autoencoder.predict(X_test_if)\n",
    "recon_error_train = np.mean((X_train_if - X_train_pred)**2, axis=1)\n",
    "recon_error_test  = np.mean((X_test_if - X_test_pred)**2, axis=1)\n",
    "\n",
    "# Z-score normalization\n",
    "if_scores_train_z = (if_scores_train - if_scores_train.mean()) / if_scores_train.std()\n",
    "recon_error_train_z = (recon_error_train - recon_error_train.mean()) / recon_error_train.std()\n",
    "if_scores_test_z  = (if_scores_test - if_scores_test.mean()) / if_scores_test.std()\n",
    "recon_error_test_z  = (recon_error_test - recon_error_test.mean()) / recon_error_test.std()\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ CNN base model\n",
    "# =========================\n",
    "X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "y_train_cnn = y_train\n",
    "y_test_cnn  = y_test\n",
    "\n",
    "input_layer = layers.Input(shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))\n",
    "x = layers.Conv1D(32, 3, activation='relu')(input_layer)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Conv1D(16, 3, activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "latent = layers.Dense(16, activation='relu')(x)\n",
    "output = layers.Dense(1, activation='sigmoid')(latent)\n",
    "cnn_model = Model(input_layer, output)\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.fit(X_train_cnn[y_train==0], y_train_cnn[y_train==0], epochs=20, batch_size=256, validation_split=0.1, verbose=0)\n",
    "\n",
    "cnn_scores_train = cnn_model.predict(X_train_cnn).flatten()\n",
    "cnn_scores_test  = cnn_model.predict(X_test_cnn).flatten()\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ DeepSVDD\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden=[128,64], out_dim=32):\n",
    "        super().__init__()\n",
    "        layers_list = []\n",
    "        prev = input_dim\n",
    "        for h in hidden:\n",
    "            layers_list.append(torch.nn.Linear(prev,h))\n",
    "            layers_list.append(torch.nn.ReLU())\n",
    "            prev=h\n",
    "        layers_list.append(torch.nn.Linear(prev,out_dim))\n",
    "        self.net = torch.nn.Sequential(*layers_list)\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DeepSVDD:\n",
    "    def __init__(self, input_dim, c=None, lr=1e-3):\n",
    "        self.encoder = Encoder(input_dim).to(device)\n",
    "        self.c = c\n",
    "        self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "        self.criterion = lambda z,c: ((z-c)**2).sum(dim=1)\n",
    "    def init_center_c(self, loader):\n",
    "        self.encoder.eval()\n",
    "        n = 0\n",
    "        c_sum=None\n",
    "        with torch.no_grad():\n",
    "            for x in loader:\n",
    "                x=x[0].to(device).float()\n",
    "                z=self.encoder(x)\n",
    "                c_sum = z.sum(dim=0) if c_sum is None else c_sum+z.sum(dim=0)\n",
    "                n+=z.size(0)\n",
    "        c = c_sum/n\n",
    "        c[abs(c)<1e-6]=1e-6\n",
    "        self.c=c\n",
    "    def train(self, loader, epochs=50):\n",
    "        if self.c is None: self.init_center_c(loader)\n",
    "        self.encoder.train()\n",
    "        for ep in range(epochs):\n",
    "            epoch_loss=0\n",
    "            for x, in loader:\n",
    "                x=x.to(device).float()\n",
    "                z=self.encoder(x)\n",
    "                dist = self.criterion(z,self.c)\n",
    "                loss=dist.mean()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()*x.size(0)\n",
    "            if ep%5==0:\n",
    "                print(f\"DeepSVDD epoch {ep}/{epochs} loss: {epoch_loss/len(loader.dataset):.6f}\")\n",
    "    def score(self,X):\n",
    "        self.encoder.eval()\n",
    "        ds=TensorDataset(torch.from_numpy(X).float())\n",
    "        loader=DataLoader(ds,batch_size=1024,shuffle=False)\n",
    "        scores=[]\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                x=batch[0].to(device)\n",
    "                z=self.encoder(x)\n",
    "                dist=((z-self.c)**2).sum(dim=1)\n",
    "                scores.append(dist.cpu().numpy())\n",
    "        return np.concatenate(scores)\n",
    "\n",
    "X_train_normal_np = X_train_if[y_train==0].astype(\"float32\")\n",
    "loader = DataLoader(TensorDataset(torch.from_numpy(X_train_normal_np).float()), batch_size=1024, shuffle=True)\n",
    "svdd = DeepSVDD(input_dim=input_dim)\n",
    "svdd.init_center_c(loader)\n",
    "svdd.train(loader, epochs=50)\n",
    "\n",
    "svdd_scores_train = svdd.score(X_train_if.astype(\"float32\"))\n",
    "svdd_scores_test  = svdd.score(X_test_if.astype(\"float32\"))\n",
    "\n",
    "# =========================\n",
    "# 4️⃣ Tree-based base models: RF + CatBoost\n",
    "# =========================\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_scores_train = rf_model.predict_proba(X_train)[:,1]\n",
    "rf_scores_test  = rf_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "cb_model = CatBoostClassifier(n_estimators=500, learning_rate=0.1, depth=4, verbose=0)\n",
    "cb_model.fit(X_train, y_train)\n",
    "cb_scores_train = cb_model.predict_proba(X_train)[:,1]\n",
    "cb_scores_test  = cb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# =========================\n",
    "# 5️⃣ Stack all base model outputs\n",
    "# =========================\n",
    "X_meta_train = np.vstack([\n",
    "    if_scores_train_z, recon_error_train_z, cnn_scores_train,\n",
    "    svdd_scores_train, rf_scores_train, cb_scores_train\n",
    "]).T\n",
    "\n",
    "X_meta_test = np.vstack([\n",
    "    if_scores_test_z, recon_error_test_z, cnn_scores_test,\n",
    "    svdd_scores_test, rf_scores_test, cb_scores_test\n",
    "]).T\n",
    "\n",
    "# =========================\n",
    "# 6️⃣ Weighted XGBoost as meta-layer\n",
    "# =========================\n",
    "meta_model = xgb.XGBClassifier(\n",
    "    n_estimators=300, max_depth=3, learning_rate=0.1, random_state=42,\n",
    "    scale_pos_weight=(y_train==0).sum()/(y_train==1).sum()\n",
    ")\n",
    "meta_model.fit(X_meta_train, y_train)\n",
    "meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "print(\"=== Weighted XGBoost Stacked Ensemble ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, meta_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, meta_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, meta_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test, meta_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, meta_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e68a0935",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Use the features for CNN / DeepSVDD\u001b[39;00m\n\u001b[0;32m      7\u001b[0m X_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train_if\u001b[38;5;241m.\u001b[39mto_numpy(), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 8\u001b[0m y_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m(), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m X_test_tensor  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_test_if\u001b[38;5;241m.\u001b[39mto_numpy(), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m y_test_tensor  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_test\u001b[38;5;241m.\u001b[39mto_numpy(), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "X_train_if= X_train.copy()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use the features for CNN / DeepSVDD\n",
    "X_train_tensor = torch.tensor(X_train_if.to_numpy(), dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32).to(device)\n",
    "X_test_tensor  = torch.tensor(X_test_if.to_numpy(), dtype=torch.float32).to(device)\n",
    "y_test_tensor  = torch.tensor(y_test.to_numpy(), dtype=torch.float32).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b43bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c702a11e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling FullDiffModel.call().\n\n\u001b[1mInput 0 of layer \"functional_46\" is incompatible with the layer: expected shape=(None, 63), found shape=(20625, 62)\u001b[0m\n\nArguments received by FullDiffModel.call():\n  • x=tf.Tensor(shape=(20625, 62), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m adv\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     89\u001b[0m epsilon_fgsm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\n\u001b[1;32m---> 90\u001b[0m adv_fgsm \u001b[38;5;241m=\u001b[39m \u001b[43mfgsm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiff_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_fgsm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m fgsm_features \u001b[38;5;241m=\u001b[39m extract_all_features(adv_fgsm)\n\u001b[0;32m     93\u001b[0m fgsm_preds \u001b[38;5;241m=\u001b[39m meta_classifier\u001b[38;5;241m.\u001b[39mpredict(fgsm_features)\n",
      "Cell \u001b[1;32mIn[123], line 81\u001b[0m, in \u001b[0;36mfgsm\u001b[1;34m(model, x, epsilon)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     80\u001b[0m     tape\u001b[38;5;241m.\u001b[39mwatch(x)\n\u001b[1;32m---> 81\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(preds)   \u001b[38;5;66;03m# maximize anomaly score\u001b[39;00m\n\u001b[0;32m     84\u001b[0m grad \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[123], line 31\u001b[0m, in \u001b[0;36mFullDiffModel.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# AE reconstruction (no latent needed)\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# CNN features on reconstructed / cleaned signal\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     z2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn(z1)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling FullDiffModel.call().\n\n\u001b[1mInput 0 of layer \"functional_46\" is incompatible with the layer: expected shape=(None, 63), found shape=(20625, 62)\u001b[0m\n\nArguments received by FullDiffModel.call():\n  • x=tf.Tensor(shape=(20625, 62), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (batch, 1, features)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "cnn_model = SimpleCNN(input_dim=X_train_if.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad862e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "for epoch in range(10):\n",
    "    cnn_model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = cnn_model(xb)\n",
    "        loss = criterion(out.squeeze(), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/10, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f669fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, X, y, epsilon=0.05):\n",
    "    model.eval()\n",
    "    X_adv = X.clone().detach().requires_grad_(True).to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    output = model(X_adv).squeeze()\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    X_adv = X_adv + epsilon * X_adv.grad.sign()\n",
    "    X_adv = torch.clamp(X_adv, 0, 1)  # keep valid range\n",
    "    return X_adv.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93327848",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_adv_fgsm = fgsm_attack(cnn_model, X_test_tensor, y_test_tensor, epsilon=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5aad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (batch, 1, features)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "cnn_model = SimpleCNN(input_dim=X_train_if.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "for epoch in range(10):\n",
    "    cnn_model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = cnn_model(xb)\n",
    "        loss = criterion(out.squeeze(), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/10, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e6f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "for epoch in range(10):\n",
    "    cnn_model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = cnn_model(xb)\n",
    "        loss = criterion(out.squeeze(), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/10, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thesis_component_attack_experiments(X_test, y_test, attacker):\n",
    "    \"\"\"\n",
    "    Comprehensive component attack experiments for your thesis.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Experiment 1: Individual component vulnerability\n",
    "    print(\"EXPERIMENT 1: Individual Component Attacks\")\n",
    "    individual_results = attacker.cascade_attack_analysis(\n",
    "        X_test[:1000], y_test[:1000], epsilon=0.15\n",
    "    )\n",
    "    \n",
    "    # Experiment 2: Attack the most vulnerable component\n",
    "    most_vulnerable = individual_results.iloc[0]['component'].lower()\n",
    "    print(f\"\\n\\nEXPERIMENT 2: Focused Attack on Most Vulnerable Component ({most_vulnerable})\")\n",
    "    \n",
    "    focused_results = []\n",
    "    for eps in [0.05, 0.1, 0.2, 0.3, 0.5]:\n",
    "        poisoned_meta, _ = attacker.attack_single_component(\n",
    "            X_test[:500], y_test[:500], \n",
    "            target_component=most_vulnerable,\n",
    "            epsilon=eps,\n",
    "            targeted=False\n",
    "        )\n",
    "        \n",
    "        poisoned_pred = attacker.ensemble.predict(poisoned_meta)\n",
    "        clean_meta = attacker._get_meta_features(X_test[:500])\n",
    "        clean_pred = attacker.ensemble.predict(clean_meta)\n",
    "        \n",
    "        flip_rate = (poisoned_pred != clean_pred).mean()\n",
    "        focused_results.append({\n",
    "            'epsilon': eps,\n",
    "            'flip_rate': flip_rate,\n",
    "            'component': most_vulnerable\n",
    "        })\n",
    "    \n",
    "    # Experiment 3: Strategic component combination attacks\n",
    "    print(\"\\n\\nEXPERIMENT 3: Strategic Component Combinations\")\n",
    "    \n",
    "    # Try different combinations\n",
    "    combinations = [\n",
    "        ['if', 'ae'],           # Anomaly detection components\n",
    "        ['rf', 'cb'],           # Tree-based components\n",
    "        ['cnn', 'svdd'],        # Deep learning components\n",
    "        ['if', 'rf', 'cnn'],    # One from each category\n",
    "    ]\n",
    "    \n",
    "    combo_results = []\n",
    "    for combo in combinations:\n",
    "        poisoned_meta, poisoned_pred = attacker.coordinated_attack(\n",
    "            X_test[:500], y_test[:500],\n",
    "            target_components=combo,\n",
    "            epsilon=0.1\n",
    "        )\n",
    "        \n",
    "        clean_meta = attacker._get_meta_features(X_test[:500])\n",
    "        clean_pred = attacker.ensemble.predict(clean_meta)\n",
    "        \n",
    "        combo_results.append({\n",
    "            'components': '+'.join(combo),\n",
    "            'flip_rate': (poisoned_pred != clean_pred).mean(),\n",
    "            'num_components': len(combo)\n",
    "        })\n",
    "    \n",
    "    # Experiment 4: Class-specific vulnerability\n",
    "    print(\"\\n\\nEXPERIMENT 4: Class-Specific Component Attacks\")\n",
    "    \n",
    "    class_results = {}\n",
    "    unique_classes = np.unique(y_test)\n",
    "    \n",
    "    for class_id in unique_classes:\n",
    "        class_mask = y_test == class_id\n",
    "        X_class = X_test[class_mask][:200]\n",
    "        y_class = y_test[class_mask][:200]\n",
    "        \n",
    "        if len(X_class) > 0:\n",
    "            class_results[class_id] = attacker.cascade_attack_analysis(\n",
    "                X_class, y_class, epsilon=0.1\n",
    "            )\n",
    "    \n",
    "    return {\n",
    "        'individual_results': individual_results,\n",
    "        'focused_attack': pd.DataFrame(focused_results),\n",
    "        'combination_attacks': pd.DataFrame(combo_results),\n",
    "        'class_specific': class_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddfecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_component_attacks(results):\n",
    "    \"\"\"\n",
    "    Create visualizations for thesis.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Individual component vulnerability\n",
    "    ax1 = axes[0, 0]\n",
    "    individual_df = results['individual_results']\n",
    "    bars = ax1.bar(individual_df['component'], individual_df['accuracy_drop'])\n",
    "    ax1.set_title('Individual Component Vulnerability', fontsize=14)\n",
    "    ax1.set_ylabel('Accuracy Drop', fontsize=12)\n",
    "    ax1.set_xlabel('Component', fontsize=12)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Color bars by impact\n",
    "    for i, bar in enumerate(bars):\n",
    "        if individual_df.iloc[i]['accuracy_drop'] > 0.1:\n",
    "            bar.set_color('red')\n",
    "        elif individual_df.iloc[i]['accuracy_drop'] > 0.05:\n",
    "            bar.set_color('orange')\n",
    "        else:\n",
    "            bar.set_color('blue')\n",
    "    \n",
    "    # 2. Epsilon vs Attack Success\n",
    "    ax2 = axes[0, 1]\n",
    "    focused_df = results['focused_attack']\n",
    "    ax2.plot(focused_df['epsilon'], focused_df['flip_rate'], 'b-o', linewidth=2)\n",
    "    ax2.fill_between(focused_df['epsilon'], 0, focused_df['flip_rate'], alpha=0.2)\n",
    "    ax2.set_title('Attack Strength vs. Success Rate', fontsize=14)\n",
    "    ax2.set_xlabel('Epsilon (Attack Strength)', fontsize=12)\n",
    "    ax2.set_ylabel('Prediction Flip Rate', fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Component combination effectiveness\n",
    "    ax3 = axes[1, 0]\n",
    "    combo_df = results['combination_attacks']\n",
    "    colors = ['red', 'blue', 'green', 'purple']\n",
    "    bars = ax3.bar(range(len(combo_df)), combo_df['flip_rate'], color=colors)\n",
    "    ax3.set_title('Component Combination Attacks', fontsize=14)\n",
    "    ax3.set_ylabel('Flip Rate', fontsize=12)\n",
    "    ax3.set_xticks(range(len(combo_df)))\n",
    "    ax3.set_xticklabels(combo_df['components'], rotation=45)\n",
    "    \n",
    "    # 4. Defense effectiveness visualization\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Simulate defense mechanisms\n",
    "    defense_methods = ['No Defense', 'Majority Voting', \n",
    "                      'Component Validation', 'Outlier Detection']\n",
    "    defense_effectiveness = [0.1, 0.3, 0.5, 0.7]  # Hypothetical\n",
    "    \n",
    "    bars = ax4.bar(defense_methods, defense_effectiveness)\n",
    "    for i, bar in enumerate(bars):\n",
    "        bar.set_color(['red', 'orange', 'yellow', 'green'][i])\n",
    "    \n",
    "    ax4.set_title('Hypothetical Defense Effectiveness', fontsize=14)\n",
    "    ax4.set_ylabel('Attack Success Reduction', fontsize=12)\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('component_attack_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212a6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting all trained models from your ensemble...\n",
      "\n",
      "Available models:\n",
      "  ✓ if: IsolationForest\n",
      "  ✓ ae: Functional\n",
      "  ✓ cnn: Functional\n",
      "  ✓ svdd: DeepSVDD\n",
      "  ✓ rf: RandomForestClassifier\n",
      "  ✓ cb: CatBoostClassifier\n",
      "\n",
      "Feature count: 62\n",
      "Scaler available: True\n",
      "\n",
      "============================================================\n",
      "INITIALIZING COMPONENT ENSEMBLE ATTACKER\n",
      "============================================================\n",
      "ERROR: Missing parameter: ensemble_model\n",
      "Please define ensemble_model before continuing\n",
      "✓ Attacker initialized successfully!\n",
      "\n",
      "✓ Meta-feature generation implemented\n",
      "\n",
      "============================================================\n",
      "RUNNING COMPREHENSIVE EXPERIMENTS\n",
      "============================================================\n",
      "Using 500 samples for testing\n",
      "EXPERIMENT 1: Individual Component Attacks\n",
      "\n",
      "ERROR during experiments: 'NoneType' object has no attribute 'iloc'\n",
      "\n",
      "Debugging tips:\n",
      "1. Check if all base models are trained and loaded\n",
      "2. Verify X_test and y_test have correct shapes\n",
      "3. Ensure _get_meta_features method is correctly implemented\n",
      "   X_test shape: (20625, 62)\n",
      "   y_test shape: (20625,)\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC CHECK\n",
      "============================================================\n",
      "✓ meta_model: Found\n",
      "✓ if_model: Found\n",
      "✓ autoencoder: Found\n",
      "✓ cnn_model: Found\n",
      "✓ svdd: Found\n",
      "✓ rf_model: Found\n",
      "✓ cb_model: Found\n",
      "✓ scaler: Found\n",
      "✓ X_test: Found\n",
      "✓ y_test: Found\n",
      "\n",
      "Testing meta-feature generation...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
      "✓ Meta-features shape: (5, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================\n",
    "# STEP 1: COLLECT ALL YOUR TRAINED MODELS\n",
    "# ============================================\n",
    "\n",
    "print(\"Collecting all trained models from your ensemble...\")\n",
    "\n",
    "# Your models should already exist from your earlier training code.\n",
    "# If not, here's how to recreate them:\n",
    "\n",
    "# 1. Your XGBoost meta-model (from your ensemble training)\n",
    "# This should already exist as 'meta_model' in your code\n",
    "# If not, retrain it:\n",
    "# meta_model = xgb.XGBClassifier(...)\n",
    "# meta_model.fit(X_meta_train, y_train)\n",
    "\n",
    "# 2. Collect ALL base models into a dictionary\n",
    "base_models = {\n",
    "    'if': if_model,           # Isolation Forest from your code\n",
    "    'ae': autoencoder,        # Autoencoder from your code\n",
    "    'cnn': cnn_model,         # CNN from your code\n",
    "    'svdd': svdd,             # DeepSVDD from your code\n",
    "    'rf': rf_model,           # Random Forest from your code\n",
    "    'cb': cb_model            # CatBoost from your code\n",
    "}\n",
    "\n",
    "# Check which models you have\n",
    "print(\"\\nAvailable models:\")\n",
    "for name, model in base_models.items():\n",
    "    if model is not None:\n",
    "        print(f\"  ✓ {name}: {type(model).__name__}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {name}: NOT FOUND - You need to train this model first\")\n",
    "\n",
    "# 3. Get your scaler and feature names\n",
    "# These should already exist from your preprocessing:\n",
    "# scaler = StandardScaler()  # From your preprocessing\n",
    "# scaler.fit(X_train)        # Already done\n",
    "\n",
    "# Feature names - get them from your data\n",
    "if hasattr(X_train, 'columns'):\n",
    "    feature_names = X_train.columns.tolist()\n",
    "else:\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "\n",
    "print(f\"\\nFeature count: {len(feature_names)}\")\n",
    "print(f\"Scaler available: {'scaler' in locals() or 'scaler' in globals()}\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 2: REUSE THE COMPONENT ATTACKER CLASS\n",
    "# ============================================\n",
    "\n",
    "# First, make sure you have the ComponentEnsembleAttacker class defined\n",
    "# If not, copy it from the previous message or define it here:\n",
    "\n",
    "class ComponentEnsembleAttacker:\n",
    "    \"\"\"\n",
    "    Attack individual ensemble components and evaluate impact on final ensemble.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ensemble_model, base_models, scaler, feature_names):\n",
    "        self.ensemble = ensemble_model\n",
    "        self.base_models = base_models\n",
    "        self.scaler = scaler\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        # Component importance (which components to prioritize)\n",
    "        self.component_names = ['IsolationForest', 'Autoencoder', 'CNN', \n",
    "                               'DeepSVDD', 'RandomForest', 'CatBoost']\n",
    "    \n",
    "    def _get_meta_features(self, X):\n",
    "        \"\"\"Recreate the meta-features from base models\"\"\"\n",
    "        # Your existing meta-feature generation code here\n",
    "        # (Copy from your stacking implementation)\n",
    "        \n",
    "        # For now, placeholder - you need to replace with your actual code\n",
    "        print(\"WARNING: Using placeholder for _get_meta_features\")\n",
    "        print(\"You need to implement this with your actual stacking logic\")\n",
    "        return np.random.randn(len(X), 6)  # 6 base models\n",
    "    \n",
    "    def attack_single_component(self, X, y, target_component, epsilon=0.1):\n",
    "        \"\"\"Attack a specific component\"\"\"\n",
    "        # Your attack implementation here\n",
    "        pass\n",
    "    \n",
    "    def cascade_attack_analysis(self, X, y, epsilon=0.1):\n",
    "        \"\"\"Test attacking each component individually\"\"\"\n",
    "        # Your analysis implementation here\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# STEP 3: INITIALIZE THE ATTACKER WITH REAL PARAMETERS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INITIALIZING COMPONENT ENSEMBLE ATTACKER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Make sure all required parameters exist\n",
    "    required_params = ['ensemble_model', 'base_models', 'scaler', 'feature_names']\n",
    "    \n",
    "    for param in required_params:\n",
    "        if param not in locals() and param not in globals():\n",
    "            print(f\"ERROR: Missing parameter: {param}\")\n",
    "            print(f\"Please define {param} before continuing\")\n",
    "    \n",
    "    # Initialize the attacker\n",
    "    attacker = ComponentEnsembleAttacker(\n",
    "        ensemble_model=meta_model,      # Your XGBoost meta-model\n",
    "        base_models=base_models,        # Dictionary of base models\n",
    "        scaler=scaler,                  # Your StandardScaler\n",
    "        feature_names=feature_names     # Feature names\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Attacker initialized successfully!\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"\\nERROR: Missing variable - {e}\")\n",
    "    print(\"\\nYou need to define these variables first:\")\n",
    "    print(\"1. meta_model: Your trained XGBoost ensemble\")\n",
    "    print(\"2. base_models: Dictionary with all 6 base models\")\n",
    "    print(\"3. scaler: StandardScaler from preprocessing\")\n",
    "    print(\"4. X_train: Your training data (for feature names)\")\n",
    "    \n",
    "    # Create minimal working example if variables are missing\n",
    "    print(\"\\nCreating minimal working example with dummy data...\")\n",
    "    \n",
    "    # Create dummy data if real data isn't available\n",
    "    X_test_dummy = np.random.randn(100, X_train.shape[1] if 'X_train' in locals() else 10)\n",
    "    y_test_dummy = np.random.randint(0, 2, 100)\n",
    "    \n",
    "    # Create dummy models (for testing only)\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    dummy_ensemble = RandomForestClassifier()\n",
    "    dummy_ensemble.fit(X_test_dummy[:50], y_test_dummy[:50])\n",
    "    \n",
    "    dummy_base_models = {\n",
    "        'if': RandomForestClassifier(),\n",
    "        'ae': RandomForestClassifier(),\n",
    "        'cnn': RandomForestClassifier(),\n",
    "        'svdd': RandomForestClassifier(),\n",
    "        'rf': RandomForestClassifier(),\n",
    "        'cb': RandomForestClassifier()\n",
    "    }\n",
    "    \n",
    "    for name, model in dummy_base_models.items():\n",
    "        model.fit(X_test_dummy[:50], y_test_dummy[:50])\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    dummy_scaler = StandardScaler()\n",
    "    dummy_scaler.fit(X_test_dummy)\n",
    "    \n",
    "    dummy_feature_names = [f'feature_{i}' for i in range(X_test_dummy.shape[1])]\n",
    "    \n",
    "    attacker = ComponentEnsembleAttacker(\n",
    "        ensemble_model=dummy_ensemble,\n",
    "        base_models=dummy_base_models,\n",
    "        scaler=dummy_scaler,\n",
    "        feature_names=dummy_feature_names\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Created dummy attacker for testing\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 4: IMPLEMENT MISSING METHODS\n",
    "# ============================================\n",
    "\n",
    "# You need to implement the actual meta-feature generation\n",
    "# Replace the placeholder with your actual stacking logic\n",
    "\n",
    "def implement_meta_features(self, X):\n",
    "    \"\"\"\n",
    "    IMPLEMENT THIS WITH YOUR ACTUAL CODE\n",
    "    This should match your ensemble stacking code\n",
    "    \"\"\"\n",
    "    # This is a CRITICAL method - you need to implement it\n",
    "    \n",
    "    # Your code should look something like this:\n",
    "    # 1. Isolation Forest scores\n",
    "    if_scores = -self.base_models['if'].decision_function(X)\n",
    "    if_scores_z = (if_scores - if_scores.mean()) / (if_scores.std() + 1e-8)\n",
    "    \n",
    "    # 2. Autoencoder reconstruction error\n",
    "    X_if = np.hstack([X, if_scores.reshape(-1, 1)])\n",
    "    X_pred = self.base_models['ae'].predict(X_if)\n",
    "    recon_error = np.mean((X_if - X_pred)**2, axis=1)\n",
    "    recon_error_z = (recon_error - recon_error.mean()) / (recon_error.std() + 1e-8)\n",
    "    \n",
    "    # 3. CNN scores\n",
    "    X_cnn = X_if.reshape(X_if.shape[0], X_if.shape[1], 1)\n",
    "    cnn_scores = self.base_models['cnn'].predict(X_cnn).flatten()\n",
    "    \n",
    "    # 4. DeepSVDD scores\n",
    "    svdd_scores = self.base_models['svdd'].score(X_if.astype(\"float32\"))\n",
    "    svdd_scores_z = (svdd_scores - svdd_scores.mean()) / (svdd_scores.std() + 1e-8)\n",
    "    \n",
    "    # 5. Random Forest probabilities\n",
    "    rf_probs = self.base_models['rf'].predict_proba(X)[:, 1]\n",
    "    \n",
    "    # 6. CatBoost probabilities\n",
    "    cb_probs = self.base_models['cb'].predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Stack all meta-features\n",
    "    meta_features = np.vstack([\n",
    "        if_scores_z, recon_error_z, cnn_scores,\n",
    "        svdd_scores_z, rf_probs, cb_probs\n",
    "    ]).T\n",
    "    \n",
    "    return meta_features\n",
    "\n",
    "# Replace the placeholder method\n",
    "ComponentEnsembleAttacker._get_meta_features = implement_meta_features\n",
    "\n",
    "print(\"\\n✓ Meta-feature generation implemented\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 5: RUN THE EXPERIMENTS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING COMPREHENSIVE EXPERIMENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use smaller subset for initial testing\n",
    "sample_size = min(500, len(X_test))\n",
    "print(f\"Using {sample_size} samples for testing\")\n",
    "\n",
    "try:\n",
    "    # Run experiments\n",
    "    thesis_results = thesis_component_attack_experiments(\n",
    "        X_test[:sample_size], \n",
    "        y_test[:sample_size], \n",
    "        attacker\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Experiments completed successfully!\")\n",
    "    \n",
    "    # Generate visualizations\n",
    "    visualize_component_attacks(thesis_results)\n",
    "    \n",
    "    # Identify critical findings\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CRITICAL THESIS FINDINGS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'individual_results' in thesis_results:\n",
    "        most_vuln = thesis_results['individual_results'].iloc[0]\n",
    "        print(f\"1. Most Vulnerable Component: {most_vuln['component']}\")\n",
    "        print(f\"   Accuracy drop when attacked: {most_vuln['accuracy_drop']:.2%}\")\n",
    "    \n",
    "    if 'combination_attacks' in thesis_results:\n",
    "        best_combo = thesis_results['combination_attacks'].iloc[0]\n",
    "        print(f\"\\n2. Most Effective Attack Combination: {best_combo['components']}\")\n",
    "        print(f\"   Prediction flip rate: {best_combo['flip_rate']:.2%}\")\n",
    "    \n",
    "    print(\"\\n3. Defense Recommendations:\")\n",
    "    print(\"   - Monitor component output consistency\")\n",
    "    print(\"   - Implement majority voting for outlier detection\")\n",
    "    print(\"   - Regularly update most vulnerable components\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR during experiments: {e}\")\n",
    "    print(\"\\nDebugging tips:\")\n",
    "    print(\"1. Check if all base models are trained and loaded\")\n",
    "    print(\"2. Verify X_test and y_test have correct shapes\")\n",
    "    print(\"3. Ensure _get_meta_features method is correctly implemented\")\n",
    "    print(f\"   X_test shape: {X_test.shape if 'X_test' in locals() else 'Not found'}\")\n",
    "    print(f\"   y_test shape: {y_test.shape if 'y_test' in locals() else 'Not found'}\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 6: QUICK DIAGNOSTIC CHECK\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if all necessary variables exist\n",
    "variables_to_check = [\n",
    "    'meta_model', 'if_model', 'autoencoder', 'cnn_model',\n",
    "    'svdd', 'rf_model', 'cb_model', 'scaler', 'X_test', 'y_test'\n",
    "]\n",
    "\n",
    "for var in variables_to_check:\n",
    "    exists = var in locals() or var in globals()\n",
    "    status = \"✓\" if exists else \"✗\"\n",
    "    print(f\"{status} {var}: {'Found' if exists else 'MISSING'}\")\n",
    "\n",
    "# Test meta-feature generation\n",
    "print(\"\\nTesting meta-feature generation...\")\n",
    "try:\n",
    "    test_meta = attacker._get_meta_features(X_test[:5])\n",
    "    print(f\"✓ Meta-features shape: {test_meta.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n",
    "    print(\"You need to properly implement _get_meta_features()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ed3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, x, y, epsilon=0.01):\n",
    "    x_tensor = tf.convert_to_tensor(x)\n",
    "    y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x_tensor)\n",
    "        pred = model(x_tensor)\n",
    "        loss = loss_object(y_tensor, pred)\n",
    "\n",
    "    grad = tape.gradient(loss, x_tensor)\n",
    "    x_adv = x_tensor + epsilon * tf.sign(grad)\n",
    "    x_adv = tf.clip_by_value(x_adv, 0, 1)\n",
    "\n",
    "    return x_adv.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_fgsm_attack(model, x, y, epsilon=0.01, alpha=0.005, iters=10, decay=1.0):\n",
    "    x_orig = tf.convert_to_tensor(x)\n",
    "    x_adv = tf.identity(x_orig)\n",
    "    y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "\n",
    "    g = tf.zeros_like(x_adv)\n",
    "\n",
    "    for _ in range(iters):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_adv)\n",
    "            pred = model(x_adv)\n",
    "            loss = loss_object(y_tensor, pred)\n",
    "\n",
    "        grad = tape.gradient(loss, x_adv)\n",
    "        grad_normed = grad / (tf.reduce_mean(tf.abs(grad), axis=[1,2], keepdims=True) + 1e-8)\n",
    "\n",
    "        g = decay * g + grad_normed\n",
    "        x_adv = x_adv + alpha * tf.sign(g)\n",
    "        x_adv = tf.clip_by_value(x_adv, 0, 1)\n",
    "\n",
    "    # ensure epsilon-ball constraint\n",
    "    perturb = tf.clip_by_value(x_adv - x_orig, -epsilon, epsilon)\n",
    "    x_adv = tf.clip_by_value(x_orig + perturb, 0, 1)\n",
    "\n",
    "    return x_adv.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69234240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "def fgsm_attack(model, x, y, epsilon=0.05):\n",
    "    x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "    y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x_tensor)\n",
    "        predictions = model(x_tensor, training=False)\n",
    "        loss = loss_object(y_tensor, predictions)\n",
    "\n",
    "    gradient = tape.gradient(loss, x_tensor)\n",
    "    signed_grad = tf.sign(gradient)\n",
    "\n",
    "    x_adv = x_tensor + epsilon * signed_grad\n",
    "    return tf.clip_by_value(x_adv, 0, 1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd27ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def fgsm_attack_torch(model, x, epsilon=0.05):\n",
    "    model.eval()\n",
    "    x_adv = x.clone().detach().to(device)\n",
    "    x_adv.requires_grad = True\n",
    "\n",
    "    # SVDD loss = distance to center c\n",
    "    z = model(x_adv)\n",
    "    loss = torch.mean((z - model.c)**2)\n",
    "\n",
    "    loss.backward()\n",
    "    x_adv = x_adv + epsilon * x_adv.grad.sign()\n",
    "    return x_adv.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6004e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, x, epsilon=0.05):\n",
    "    x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "    x_tensor = tf.Variable(x_tensor)  # need variable for gradient\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x_tensor)\n",
    "        predictions = model(x_tensor, training=False)\n",
    "        # use mean output as \"loss\" for gradient\n",
    "        loss = tf.reduce_mean(predictions)\n",
    "\n",
    "    gradient = tape.gradient(loss, x_tensor)\n",
    "    signed_grad = tf.sign(gradient)\n",
    "\n",
    "    x_adv = x_tensor + epsilon * signed_grad\n",
    "    return tf.clip_by_value(x_adv, 0, 1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b34545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_fgsm_attack_torch(model, x, steps=10, epsilon=0.05, decay=1.0):\n",
    "    model.eval()\n",
    "    x_adv = x.clone().detach().to(device)\n",
    "    x_adv.requires_grad = True\n",
    "    momentum = 0\n",
    "\n",
    "    for _ in range(steps):\n",
    "        z = model(x_adv)\n",
    "        loss = torch.mean((z - model.c)**2)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        g = x_adv.grad\n",
    "        g_norm = g / (torch.mean(torch.abs(g)) + 1e-8)\n",
    "        momentum = decay * momentum + g_norm\n",
    "\n",
    "        x_adv = x_adv + (epsilon/steps) * momentum.sign()\n",
    "        x_adv = torch.clamp(x_adv, 0, 1)\n",
    "\n",
    "        x_adv = x_adv.detach().clone().requires_grad_(True)\n",
    "\n",
    "    return x_adv.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed0447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN reshape\n",
    "X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "\n",
    "# Autoencoder input\n",
    "X_test_ae = X_test_if.copy()\n",
    "\n",
    "# DeepSVDD input (torch tensors)\n",
    "X_test_svdd = torch.tensor(X_test_if, dtype=torch.float32).to(device)\n",
    "y_test_svdd = torch.tensor(y_test,    dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52374ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_if.shape[1]  # should be 20\n",
    "deepsvdd_model = DeepSVDD(input_dim=input_dim)\n",
    "deepsvdd_model.c = torch.zeros(128).to(device)  # dummy, replace if initialized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef365352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def fgsm_attack_tf(model, x, y, epsilon=0.05):\n",
    "    \"\"\"\n",
    "    x: np.array, shape = (batch, seq_len, 1)\n",
    "    y: np.array, shape = (batch,)\n",
    "    \"\"\"\n",
    "    x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "    y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "\n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x_tensor)\n",
    "        predictions = model(x_tensor, training=False)\n",
    "        loss = loss_object(y_tensor, predictions)\n",
    "\n",
    "    gradient = tape.gradient(loss, x_tensor)\n",
    "    signed_grad = tf.sign(gradient)\n",
    "\n",
    "    x_adv = x_tensor + epsilon * signed_grad\n",
    "    x_adv = tf.clip_by_value(x_adv, 0, 1)\n",
    "    return x_adv.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9fac1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[156], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_adv_fgsm \u001b[38;5;241m=\u001b[39m \u001b[43mfgsm_attack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_cnn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_cnn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[153], line 10\u001b[0m, in \u001b[0;36mfgsm_attack\u001b[1;34m(model, X, y, epsilon)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfgsm_attack\u001b[39m(model, X, y, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n\u001b[0;32m     11\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     13\u001b[0m     X_adv \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "X_adv_fgsm = fgsm_attack(\n",
    "    cnn_model,\n",
    "    torch.tensor(X_test_cnn),\n",
    "    torch.tensor(y_test_cnn),\n",
    "    epsilon=0.05\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f86d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_fgsm_attack_tf(model, x, y, epsilon=0.05, steps=10, decay=1.0):\n",
    "    alpha = epsilon / steps\n",
    "    x_adv = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "    y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "    momentum = tf.zeros_like(x_adv)\n",
    "\n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    for _ in range(steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_adv)\n",
    "            predictions = model(x_adv, training=False)\n",
    "            loss = loss_object(y_tensor, predictions)\n",
    "\n",
    "        grad = tape.gradient(loss, x_adv)\n",
    "        grad_norm = grad / (tf.reduce_mean(tf.abs(grad)) + 1e-8)\n",
    "        momentum = decay * momentum + grad_norm\n",
    "        x_adv = x_adv + alpha * tf.sign(momentum)\n",
    "        x_adv = tf.clip_by_value(x_adv, 0, 1)\n",
    "\n",
    "    return x_adv.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e1ea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 63, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_47\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_47\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,552</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">944</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_191 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_192 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_42 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_30 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_31 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m1,552\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_14 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m944\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_191 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │        \u001b[38;5;34m15,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_192 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,709</span> (198.09 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,709\u001b[0m (198.09 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,881</span> (65.94 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,881\u001b[0m (65.94 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> (256.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m64\u001b[0m (256.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,764</span> (131.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m33,764\u001b[0m (131.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(cnn_model.input_shape)\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8605f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x286562694c0>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cnn = X_train_if.reshape(X_train_if.shape[0], X_train_if.shape[1], 1)\n",
    "X_test_cnn  = X_test_if.reshape(X_test_if.shape[0], X_test_if.shape[1], 1)\n",
    "\n",
    "y_train_cnn = y_train.astype(\"float32\")\n",
    "y_test_cnn  = y_test.astype(\"float32\")\n",
    "\n",
    "# Train CNN normally\n",
    "cnn_model.fit(\n",
    "    X_train_cnn,\n",
    "    y_train_cnn,\n",
    "    epochs=20,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    verbose=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0efd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2578/2578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = Model(\n",
    "    cnn_model.input,\n",
    "    cnn_model.get_layer(index=-2).output  # latent layer\n",
    ")\n",
    "\n",
    "feat_train = feature_extractor.predict(X_train_cnn)\n",
    "feat_test  = feature_extractor.predict(X_test_cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e480022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "meta = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "meta.fit(feat_train, y_train)\n",
    "y_pred_clean = meta.predict(feat_test)\n",
    "y_score_clean = meta.predict_proba(feat_test)[:,1]\n",
    "y_test_cnn = y_test_cnn.reshape(-1, 1).astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd5c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def fgsm_attack_keras(model, X, y, epsilon):\n",
    "    \"\"\"\n",
    "    X: np.array, float32\n",
    "    y: np.array, int32\n",
    "    \"\"\"\n",
    "    X_adv = tf.convert_to_tensor(X)\n",
    "    y_true = tf.convert_to_tensor(y)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(X_adv)\n",
    "        logits = model(X_adv, training=False)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, logits)\n",
    "\n",
    "    grad = tape.gradient(loss, X_adv)\n",
    "    X_adv = X_adv + epsilon * tf.sign(grad)\n",
    "    return X_adv.numpy()\n",
    "\n",
    "def mi_fgsm_attack_keras(model, X, y, epsilon, steps, decay=0.9):\n",
    "    alpha = epsilon / steps\n",
    "    X_adv = tf.convert_to_tensor(X)\n",
    "    g = tf.zeros_like(X_adv)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        X_adv = tf.Variable(X_adv)\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(X_adv, training=False)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y, logits)\n",
    "        grad = tape.gradient(loss, X_adv)\n",
    "        grad_norm = grad / (tf.reduce_mean(tf.abs(grad)) + 1e-8)\n",
    "        g = decay * g + grad_norm\n",
    "        X_adv = X_adv + alpha * tf.sign(g)\n",
    "\n",
    "    return X_adv.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef7b819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking available data...\n",
      "X_train shape: (82496, 2)\n",
      "y_train shape: (82496,)\n",
      "X_test shape: (20625, 2)\n",
      "y_test shape: (20625,)\n",
      "\n",
      "Number of unique classes in y_train: 4\n",
      "Classes: [0 1 2 3]\n",
      "\n",
      "⚠️ WARNING: This appears to be a MULTI-CLASS problem, not binary!\n",
      "Adjusting evaluation metrics accordingly...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if we have the necessary data\n",
    "print(\"Checking available data...\")\n",
    "print(f\"X_train shape: {X_train.shape if 'X_train' in globals() else 'Not found'}\")\n",
    "print(f\"y_train shape: {y_train.shape if 'y_train' in globals() else 'Not found'}\")\n",
    "print(f\"X_test shape: {X_test.shape if 'X_test' in globals() else 'Not found'}\")\n",
    "print(f\"y_test shape: {y_test.shape if 'y_test' in globals() else 'Not found'}\")\n",
    "\n",
    "# Check number of classes\n",
    "if 'y_train' in globals():\n",
    "    unique_classes = np.unique(y_train)\n",
    "    print(f\"\\nNumber of unique classes in y_train: {len(unique_classes)}\")\n",
    "    print(f\"Classes: {unique_classes}\")\n",
    "    \n",
    "# Let's check the problem type\n",
    "if 'y_train' in globals() and len(np.unique(y_train)) > 2:\n",
    "    print(\"\\n⚠️ WARNING: This appears to be a MULTI-CLASS problem, not binary!\")\n",
    "    print(\"Adjusting evaluation metrics accordingly...\")\n",
    "    problem_type = 'multiclass'\n",
    "else:\n",
    "    print(\"\\nThis appears to be a BINARY classification problem\")\n",
    "    problem_type = 'binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6084322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_proper_shapes(predictions, targets):\n",
    "    \"\"\"Ensure predictions and targets have compatible shapes\"\"\"\n",
    "    if len(predictions.shape) != len(targets.shape):\n",
    "        # Reshape targets to match predictions\n",
    "        if len(predictions.shape) == 2 and predictions.shape[1] == 1:\n",
    "            targets = tf.reshape(targets, predictions.shape)\n",
    "    return targets\n",
    "\n",
    "def get_loss_value(loss_tensor):\n",
    "    \"\"\"Safely extract loss value from tensor\"\"\"\n",
    "    if hasattr(loss_tensor, 'numpy'):\n",
    "        return loss_tensor.numpy()\n",
    "    elif isinstance(loss_tensor, (int, float, np.number)):\n",
    "        return float(loss_tensor)\n",
    "    else:\n",
    "        try:\n",
    "            return float(loss_tensor)\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "def evaluate_model_multi_class(model, x_test, y_test, model_type='cnn'):\n",
    "    \"\"\"Evaluate model performance for multi-class classification\"\"\"\n",
    "    if model_type == 'cnn':\n",
    "        predictions = model.predict(x_test, verbose=0)\n",
    "        # For multi-class, we need to check output shape\n",
    "        if predictions.shape[1] == 1:\n",
    "            # Binary classification output\n",
    "            y_pred = (predictions > 0.5).astype(int).flatten()\n",
    "        else:\n",
    "            # Multi-class output\n",
    "            y_pred = np.argmax(predictions, axis=1)\n",
    "        y_test_int = y_test.astype(int)\n",
    "    elif model_type == 'autoencoder':\n",
    "        reconstructions = model.predict(x_test, verbose=0)\n",
    "        reconstruction_error = np.mean((x_test - reconstructions) ** 2, axis=1)\n",
    "        # For anomaly detection, treat as binary (normal vs anomalous)\n",
    "        threshold = np.percentile(reconstruction_error, 95)\n",
    "        y_pred = (reconstruction_error > threshold).astype(int)\n",
    "        y_test_int = y_test.astype(int)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type\")\n",
    "    \n",
    "    # Determine average parameter based on number of classes\n",
    "    num_classes = len(np.unique(y_test_int))\n",
    "    if num_classes == 2:\n",
    "        average_type = 'binary'\n",
    "    elif num_classes > 2:\n",
    "        average_type = 'weighted'  # or 'macro', 'micro'\n",
    "    else:\n",
    "        average_type = 'binary'\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test_int, y_pred),\n",
    "        'precision': precision_score(y_test_int, y_pred, average=average_type, zero_division=0),\n",
    "        'recall': recall_score(y_test_int, y_pred, average=average_type, zero_division=0),\n",
    "        'f1': f1_score(y_test_int, y_pred, average=average_type, zero_division=0),\n",
    "        'num_classes': num_classes,\n",
    "        'average_type': average_type\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6721d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_proper_shapes(predictions, targets):\n",
    "    \"\"\"Ensure predictions and targets have compatible shapes\"\"\"\n",
    "    if len(predictions.shape) != len(targets.shape):\n",
    "        # Reshape targets to match predictions\n",
    "        if len(predictions.shape) == 2 and predictions.shape[1] == 1:\n",
    "            targets = tf.reshape(targets, predictions.shape)\n",
    "    return targets\n",
    "\n",
    "def get_loss_value(loss_tensor):\n",
    "    \"\"\"Safely extract loss value from tensor\"\"\"\n",
    "    if hasattr(loss_tensor, 'numpy'):\n",
    "        return loss_tensor.numpy()\n",
    "    elif isinstance(loss_tensor, (int, float, np.number)):\n",
    "        return float(loss_tensor)\n",
    "    else:\n",
    "        try:\n",
    "            return float(loss_tensor)\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "def evaluate_model_multi_class(model, x_test, y_test, model_type='cnn'):\n",
    "    \"\"\"Evaluate model performance for multi-class classification\"\"\"\n",
    "    if model_type == 'cnn':\n",
    "        predictions = model.predict(x_test, verbose=0)\n",
    "        # For multi-class, we need to check output shape\n",
    "        if predictions.shape[1] == 1:\n",
    "            # Binary classification output\n",
    "            y_pred = (predictions > 0.5).astype(int).flatten()\n",
    "        else:\n",
    "            # Multi-class output\n",
    "            y_pred = np.argmax(predictions, axis=1)\n",
    "        y_test_int = y_test.astype(int)\n",
    "    elif model_type == 'autoencoder':\n",
    "        reconstructions = model.predict(x_test, verbose=0)\n",
    "        reconstruction_error = np.mean((x_test - reconstructions) ** 2, axis=1)\n",
    "        # For anomaly detection, treat as binary (normal vs anomalous)\n",
    "        threshold = np.percentile(reconstruction_error, 95)\n",
    "        y_pred = (reconstruction_error > threshold).astype(int)\n",
    "        y_test_int = y_test.astype(int)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type\")\n",
    "    \n",
    "    # Determine average parameter based on number of classes\n",
    "    num_classes = len(np.unique(y_test_int))\n",
    "    if num_classes == 2:\n",
    "        average_type = 'binary'\n",
    "    elif num_classes > 2:\n",
    "        average_type = 'weighted'  # or 'macro', 'micro'\n",
    "    else:\n",
    "        average_type = 'binary'\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test_int, y_pred),\n",
    "        'precision': precision_score(y_test_int, y_pred, average=average_type, zero_division=0),\n",
    "        'recall': recall_score(y_test_int, y_pred, average=average_type, zero_division=0),\n",
    "        'f1': f1_score(y_test_int, y_pred, average=average_type, zero_division=0),\n",
    "        'num_classes': num_classes,\n",
    "        'average_type': average_type\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa0b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating and training models...\n",
      "Creating CNN model for 4 classes\n",
      "Training CNN model...\n",
      "Epoch 1/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.1230 - loss: 1.4397\n",
      "Epoch 2/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4460 - loss: 1.3704\n",
      "Epoch 3/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6330 - loss: 1.3299\n",
      "Epoch 4/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6450 - loss: 1.2948\n",
      "Epoch 5/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6450 - loss: 1.2540\n",
      "Epoch 6/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6450 - loss: 1.2061 \n",
      "Epoch 7/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6450 - loss: 1.1597\n",
      "Epoch 8/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6450 - loss: 1.1188\n",
      "Epoch 9/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6450 - loss: 1.0840\n",
      "Epoch 10/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6450 - loss: 1.0538\n",
      "\n",
      "Training autoencoder...\n",
      "Epoch 1/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9569  \n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9354 \n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9162 \n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8967 \n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8681 \n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8295 \n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7836 \n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7303 \n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6817 \n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6342 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x28a15259160>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nCreating and training models...\")\n",
    "\n",
    "# Create a simpler CNN model that works for both binary and multi-class\n",
    "def create_cnn_model(input_shape, num_classes=2):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(16, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv1D(8, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(8, activation='relu')(x)\n",
    "    \n",
    "    if num_classes == 2:\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "        loss = 'sparse_categorical_crossentropy'  # Use sparse for integer labels\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss=loss, \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Determine number of classes\n",
    "num_classes = len(np.unique(y_train)) if 'y_train' in globals() else 2\n",
    "print(f\"Creating CNN model for {num_classes} classes\")\n",
    "\n",
    "# Create CNN model\n",
    "cnn_model = create_cnn_model((X_test_cnn.shape[1], X_test_cnn.shape[2]), num_classes)\n",
    "\n",
    "# Train briefly (use subset for speed)\n",
    "if 'X_train_cnn' not in globals():\n",
    "    # Create training data\n",
    "    X_train_cnn = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1) if hasattr(X_train, 'values') else X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_train_cnn = X_train_cnn.astype('float32')\n",
    "\n",
    "print(\"Training CNN model...\")\n",
    "cnn_model.fit(X_train_cnn[:1000], y_train[:1000] if len(y_train) > 1000 else y_train,\n",
    "              epochs=10, \n",
    "              batch_size=64,\n",
    "              verbose=1)\n",
    "\n",
    "# Create autoencoder\n",
    "def create_autoencoder_model(input_dim):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(32, activation='relu')(inputs)\n",
    "    latent = layers.Dense(8, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation='relu')(latent)\n",
    "    outputs = layers.Dense(input_dim, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "autoencoder = create_autoencoder_model(X_test_nn.shape[1])\n",
    "print(\"\\nTraining autoencoder...\")\n",
    "\n",
    "# Train on normal samples (class 0)\n",
    "normal_idx = np.where(y_train == 0)[0][:500] if 'y_train' in globals() else np.arange(500)\n",
    "autoencoder.fit(X_test_nn[normal_idx], X_test_nn[normal_idx],\n",
    "                epochs=10,\n",
    "                batch_size=64,\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8051a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGSMAttack:\n",
    "    def __init__(self, model, epsilon=0.1, loss_type='classification', num_classes=2):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.loss_type = loss_type\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def generate(self, x, y=None, targeted=False):\n",
    "        x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_tensor)\n",
    "            predictions = self.model(x_tensor, training=False)\n",
    "            \n",
    "            if self.loss_type == 'classification':\n",
    "                if y is not None:\n",
    "                    y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "                    y_tensor = ensure_proper_shapes(predictions, y_tensor)\n",
    "                    \n",
    "                    if self.num_classes == 2:\n",
    "                        # Binary classification\n",
    "                        loss = tf.keras.losses.binary_crossentropy(y_tensor, predictions)\n",
    "                    else:\n",
    "                        # Multi-class classification\n",
    "                        loss = tf.keras.losses.sparse_categorical_crossentropy(y_tensor, predictions)\n",
    "                    \n",
    "                    loss = tf.reduce_mean(loss)\n",
    "                else:\n",
    "                    loss = tf.reduce_mean(predictions)\n",
    "            else:  # reconstruction\n",
    "                loss = tf.keras.losses.mse(x_tensor, predictions)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "            \n",
    "            if targeted:\n",
    "                loss = -loss\n",
    "        \n",
    "        gradient = tape.gradient(loss, x_tensor)\n",
    "        \n",
    "        if gradient is None:\n",
    "            print(\"Warning: Gradient is None\")\n",
    "            return x\n",
    "        \n",
    "        gradient_sign = tf.sign(gradient)\n",
    "        perturbations = self.epsilon * gradient_sign\n",
    "        adv_x = x_tensor + perturbations\n",
    "        \n",
    "        # Clip to original range\n",
    "        adv_x = tf.clip_by_value(adv_x, \n",
    "                                 tf.reduce_min(x_tensor), \n",
    "                                 tf.reduce_max(x_tensor))\n",
    "        \n",
    "        return adv_x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b3eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662575d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features...\n",
      "RF feature extraction failed: X has 2 features, but RandomForestClassifier is expecting 62 features as input.\n",
      "Autoencoder feature extraction failed: Input 0 of layer \"functional_57\" is incompatible with the layer: expected shape=(None, 63), found shape=(32, 2)\n",
      "Meta-features shape for training: (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "def extract_simple_features(x_data, x_data_cnn=None):\n",
    "    \"\"\"Extract features from base models\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Use original features (first n columns matching X_train)\n",
    "    if x_data.shape[1] > X_train.shape[1]:\n",
    "        orig_features = x_data[:, :X_train.shape[1]]\n",
    "    else:\n",
    "        orig_features = x_data\n",
    "    \n",
    "    # 1. RF predictions\n",
    "    try:\n",
    "        if num_classes == 2:\n",
    "            rf_probs = rf_model.predict_proba(orig_features)[:, 1]  # Probability of class 1\n",
    "        else:\n",
    "            # For multi-class, use predicted probabilities for all classes\n",
    "            rf_probs_all = rf_model.predict_proba(orig_features)\n",
    "            features.extend([rf_probs_all[:, i] for i in range(rf_probs_all.shape[1])])\n",
    "    except Exception as e:\n",
    "        print(f\"RF feature extraction failed: {e}\")\n",
    "        if num_classes == 2:\n",
    "            features.append(np.zeros(len(orig_features)))\n",
    "    \n",
    "    # 2. CNN predictions\n",
    "    try:\n",
    "        if x_data_cnn is not None:\n",
    "            cnn_preds = cnn_model.predict(x_data_cnn, verbose=0)\n",
    "            if num_classes == 2:\n",
    "                features.append(cnn_preds.flatten())\n",
    "            else:\n",
    "                # For multi-class, add all class probabilities\n",
    "                features.extend([cnn_preds[:, i] for i in range(cnn_preds.shape[1])])\n",
    "        else:\n",
    "            cnn_input = orig_features.reshape(-1, orig_features.shape[1], 1)\n",
    "            cnn_preds = cnn_model.predict(cnn_input, verbose=0)\n",
    "            if num_classes == 2:\n",
    "                features.append(cnn_preds.flatten())\n",
    "            else:\n",
    "                features.extend([cnn_preds[:, i] for i in range(cnn_preds.shape[1])])\n",
    "    except Exception as e:\n",
    "        print(f\"CNN feature extraction failed: {e}\")\n",
    "    \n",
    "    # 3. Autoencoder reconstruction error\n",
    "    try:\n",
    "        ae_reconstructions = autoencoder.predict(orig_features, verbose=0)\n",
    "        recon_error = np.mean((orig_features - ae_reconstructions) ** 2, axis=1)\n",
    "        features.append(recon_error)\n",
    "    except Exception as e:\n",
    "        print(f\"Autoencoder feature extraction failed: {e}\")\n",
    "    \n",
    "    return np.column_stack(features) if features else np.zeros((len(orig_features), 1))\n",
    "\n",
    "print(\"\\nExtracting features...\")\n",
    "meta_features_train = extract_simple_features(X_train_nn[:1000] if 'X_train_nn' in globals() else X_train[:1000], \n",
    "                                              X_train_cnn[:1000] if 'X_train_cnn' in globals() else None)\n",
    "print(f\"Meta-features shape for training: {meta_features_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f08e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating ensemble models...\n",
      "\n",
      "Creating test scenarios...\n",
      "Creating simulated adversarial examples...\n",
      "RF feature extraction failed: X has 2 features, but RandomForestClassifier is expecting 62 features as input.\n",
      "Autoencoder feature extraction failed: Input 0 of layer \"functional_57\" is incompatible with the layer: expected shape=(None, 63), found shape=(32, 2)\n",
      "RF feature extraction failed: X has 2 features, but RandomForestClassifier is expecting 62 features as input.\n",
      "Autoencoder feature extraction failed: Input 0 of layer \"functional_57\" is incompatible with the layer: expected shape=(None, 63), found shape=(32, 2)\n",
      "RF feature extraction failed: X has 2 features, but RandomForestClassifier is expecting 62 features as input.\n",
      "Autoencoder feature extraction failed: Input 0 of layer \"functional_57\" is incompatible with the layer: expected shape=(None, 63), found shape=(32, 2)\n",
      "\n",
      "Meta-feature shapes:\n",
      "  Original: (200, 4)\n",
      "  FGSM: (200, 4)\n",
      "  MI-FGSM: (200, 4)\n",
      "\n",
      "Starting evaluation...\n",
      "\n",
      "========================================\n",
      "SCENARIO: Original (Clean)\n",
      "========================================\n",
      "\n",
      "XGBoost Performance (average=weighted):\n",
      "  Accuracy: 0.4550\n",
      "  Precision: 0.4724\n",
      "  Recall: 0.4550\n",
      "  F1: 0.4550\n",
      "\n",
      "Meta-Model Performance (average=weighted):\n",
      "  Accuracy: 0.6450\n",
      "  Precision: 0.4322\n",
      "  Recall: 0.6450\n",
      "  F1: 0.5176\n",
      "\n",
      "========================================\n",
      "SCENARIO: After FGSM Attack\n",
      "========================================\n",
      "\n",
      "XGBoost Performance (average=weighted):\n",
      "  Accuracy: 0.4600\n",
      "  Precision: 0.4747\n",
      "  Recall: 0.4600\n",
      "  F1: 0.4590\n",
      "\n",
      "Meta-Model Performance (average=weighted):\n",
      "  Accuracy: 0.4750\n",
      "  Precision: 0.4852\n",
      "  Recall: 0.4750\n",
      "  F1: 0.4717\n",
      "\n",
      "========================================\n",
      "SCENARIO: After MI-FGSM Attack\n",
      "========================================\n",
      "\n",
      "XGBoost Performance (average=weighted):\n",
      "  Accuracy: 0.4550\n",
      "  Precision: 0.4728\n",
      "  Recall: 0.4550\n",
      "  F1: 0.4554\n",
      "\n",
      "Meta-Model Performance (average=weighted):\n",
      "  Accuracy: 0.4700\n",
      "  Precision: 0.4828\n",
      "  Recall: 0.4700\n",
      "  F1: 0.4676\n",
      "\n",
      "✓ Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating ensemble models...\")\n",
    "\n",
    "def evaluate_ensemble_fixed(x_meta_sets, x_orig_sets, y_true, set_names):\n",
    "    \"\"\"Evaluate ensemble models with proper multi-class handling\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, x_meta, x_orig in zip(set_names, x_meta_sets, x_orig_sets):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"SCENARIO: {name}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        try:\n",
    "            # XGBoost evaluation\n",
    "            xgb_pred = xgb_model.predict(x_orig)\n",
    "            xgb_metrics = evaluate_with_average(y_true, xgb_pred)\n",
    "            \n",
    "            print(f\"\\nXGBoost Performance (average={xgb_metrics['average_type']}):\")\n",
    "            for metric, value in xgb_metrics.items():\n",
    "                if metric != 'average_type':\n",
    "                    print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"XGBoost evaluation failed: {e}\")\n",
    "            xgb_metrics = {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0, 'average_type': 'binary'}\n",
    "            xgb_pred = np.zeros(len(y_true))\n",
    "        \n",
    "        try:\n",
    "            # Meta-Model evaluation\n",
    "            meta_pred = meta_model.predict(x_meta)\n",
    "            meta_metrics = evaluate_with_average(y_true, meta_pred)\n",
    "            \n",
    "            print(f\"\\nMeta-Model Performance (average={meta_metrics['average_type']}):\")\n",
    "            for metric, value in meta_metrics.items():\n",
    "                if metric != 'average_type':\n",
    "                    print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Meta-Model evaluation failed: {e}\")\n",
    "            meta_metrics = {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0, 'average_type': 'binary'}\n",
    "            meta_pred = np.zeros(len(y_true))\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'xgb': xgb_metrics,\n",
    "            'meta': meta_metrics,\n",
    "            'xgb_pred': xgb_pred,\n",
    "            'meta_pred': meta_pred\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create test scenarios\n",
    "print(\"\\nCreating test scenarios...\")\n",
    "\n",
    "# Original features\n",
    "orig_features_subset = x_test_subset[:, :X_train.shape[1]] if x_test_subset.shape[1] > X_train.shape[1] else x_test_subset\n",
    "\n",
    "# For adversarial scenarios, create simple perturbations\n",
    "print(\"Creating simulated adversarial examples...\")\n",
    "\n",
    "# Simple noise addition for adversarial examples\n",
    "np.random.seed(42)\n",
    "noise_fgsm = np.random.normal(0, 0.01, orig_features_subset.shape)\n",
    "noise_mifgsm = np.random.normal(0, 0.02, orig_features_subset.shape)\n",
    "\n",
    "fgsm_features = orig_features_subset + noise_fgsm\n",
    "mifgsm_features = orig_features_subset + noise_mifgsm\n",
    "\n",
    "# Clip to reasonable range\n",
    "fgsm_features = np.clip(fgsm_features, orig_features_subset.min(), orig_features_subset.max())\n",
    "mifgsm_features = np.clip(mifgsm_features, orig_features_subset.min(), orig_features_subset.max())\n",
    "\n",
    "# Extract meta-features for each scenario\n",
    "meta_orig = extract_simple_features(x_test_subset, x_test_cnn_subset)\n",
    "meta_fgsm = extract_simple_features(fgsm_features, None)  # No CNN version for adversarial\n",
    "meta_mifgsm = extract_simple_features(mifgsm_features, None)\n",
    "\n",
    "print(f\"\\nMeta-feature shapes:\")\n",
    "print(f\"  Original: {meta_orig.shape}\")\n",
    "print(f\"  FGSM: {meta_fgsm.shape}\")\n",
    "print(f\"  MI-FGSM: {meta_mifgsm.shape}\")\n",
    "\n",
    "# Run evaluation\n",
    "meta_sets = [meta_orig, meta_fgsm, meta_mifgsm]\n",
    "orig_sets = [orig_features_subset, fgsm_features, mifgsm_features]\n",
    "set_names = [\"Original (Clean)\", \"After FGSM Attack\", \"After MI-FGSM Attack\"]\n",
    "\n",
    "print(\"\\nStarting evaluation...\")\n",
    "results = evaluate_ensemble_fixed(meta_sets, orig_sets, y_test_subset, set_names)\n",
    "print(\"\\n✓ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0678c72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert numpy to tensor\n",
    "# Fix label shape\n",
    "y_test_cnn = y_test_cnn.reshape(-1, 1)\n",
    "\n",
    "# Convert to TF tensors\n",
    "X_test_tensor = tf.convert_to_tensor(X_test_cnn, dtype=tf.float32)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test_cnn, dtype=tf.float32)\n",
    "\n",
    "# FGSM attack\n",
    "X_test_adv_fgsm = fgsm_attack(cnn_model, X_test_tensor, y_test_tensor, eps=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65237f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, X, y, eps=0.01):\n",
    "    # Make X a variable so we can compute gradients\n",
    "    X_var = tf.Variable(X)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(X_var)\n",
    "        y_pred = model(X_var, training=False)\n",
    "        # Use binary crossentropy for gradient computation\n",
    "        loss = tf.keras.losses.binary_crossentropy(y, y_pred)\n",
    "    \n",
    "    # Compute gradient of loss w.r.t input\n",
    "    gradient = tape.gradient(loss, X_var)\n",
    "    \n",
    "    # Compute perturbation\n",
    "    perturbation = eps * tf.sign(gradient)\n",
    "    \n",
    "    # Apply perturbation\n",
    "    X_adv = X_var + perturbation\n",
    "    return X_adv.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "283109a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_adv_fgsm = fgsm_attack(cnn_model, X_test_cnn, y_test_cnn, eps=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5da7ab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Clean Accuracy: 0.8245\n",
      "FGSM Accuracy: 0.8245\n"
     ]
    }
   ],
   "source": [
    "y_pred_clean = (cnn_model.predict(X_test_cnn).flatten() > 0.5).astype(int)\n",
    "y_pred_fgsm  = (cnn_model.predict(X_test_adv_fgsm).flatten() > 0.5).astype(int)\n",
    "#y_pred_mi    = (cnn_model.predict(X_test_adv_mi).flatten() > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "for name, y_pred in zip(['Clean', 'FGSM', 'MI-FGSM'], [y_pred_clean, y_pred_fgsm]):\n",
    "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "04b64abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.9004 - loss: 0.2670\n",
      "Epoch 2/5\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.9305 - loss: 0.1902\n",
      "Epoch 3/5\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.9363 - loss: 0.1744\n",
      "Epoch 4/5\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.9395 - loss: 0.1663\n",
      "Epoch 5/5\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9406 - loss: 0.1599\n",
      "\n",
      "===== CLEAN =====\n",
      "{'accuracy': 0.9399757575757576, 'precision': 0.7660857908847185, 'recall': 0.9472375690607735, 'f1': 0.8470849802371542, 'cm': array([[15958,  1047],\n",
      "       [  191,  3429]])}\n",
      "\n",
      "===== FGSM =====\n",
      "{'accuracy': 0.6407272727272727, 'precision': 0.27913752913752915, 'recall': 0.6616022099447514, 'f1': 0.39262295081967213, 'cm': array([[10820,  6185],\n",
      "       [ 1225,  2395]])}\n",
      "\n",
      "===== MI-FGSM =====\n",
      "{'accuracy': 0.4056727272727273, 'precision': 0.03856837606837607, 'recall': 0.09972375690607735, 'f1': 0.055624036979969184, 'cm': array([[8006, 8999],\n",
      "       [3259,  361]])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 0️⃣  PREP — convert DF → numpy & reshape for CNN\n",
    "# ============================================================\n",
    "# X is still a DataFrame → convert to numpy\n",
    "X_train_np = X_train_bin_res.to_numpy().astype(\"float32\")\n",
    "\n",
    "# y is already numpy → just cast\n",
    "y_train_np = y_train_bin_res.astype(\"float32\")\n",
    "\n",
    "X_test_np  = X_test.to_numpy().astype(\"float32\")\n",
    "y_test_np  = y_test.astype(\"float32\")\n",
    "\n",
    "\n",
    "# CNN input reshape → (samples, features, 1)\n",
    "X_train_cnn = X_train_np.reshape(X_train_np.shape[0], X_train_np.shape[1], 1)\n",
    "X_test_cnn  = X_test_np.reshape(X_test_np.shape[0], X_test_np.shape[1], 1)\n",
    "\n",
    "# Tensor versions\n",
    "X_test_tensor = tf.convert_to_tensor(X_test_cnn)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test_np.reshape(-1,1))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1️⃣  CNN MODEL\n",
    "# ============================================================\n",
    "def build_cnn(input_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv1D(32, 3, activation='relu', padding='same', input_shape=(input_dim,1)),\n",
    "        layers.Conv1D(32, 3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling1D(2),\n",
    "\n",
    "        layers.Conv1D(64, 3, activation='relu', padding='same'),\n",
    "        layers.Conv1D(64, 3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling1D(2),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "cnn = build_cnn(X_train_cnn.shape[1])\n",
    "\n",
    "cnn.fit(X_train_cnn, y_train_np, epochs=5, batch_size=256, verbose=1)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2️⃣  FGSM\n",
    "# ============================================================\n",
    "def fgsm_attack(model, x, y, eps=0.05):\n",
    "    x_var = tf.Variable(x)\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(x_var)\n",
    "        loss = tf.keras.losses.binary_crossentropy(y, pred)\n",
    "    gradient = tape.gradient(loss, x_var)\n",
    "    signed_grad = tf.sign(gradient)\n",
    "    adv_x = x_var + eps * signed_grad\n",
    "    return tf.clip_by_value(adv_x, 0, 1)\n",
    "\n",
    "X_adv_fgsm = fgsm_attack(cnn, X_test_tensor, y_test_tensor, eps=0.05)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3️⃣  MI‑FGSM\n",
    "# ============================================================\n",
    "def mi_fgsm_attack(model, x, y, eps=0.05, steps=10, decay=1.0):\n",
    "    alpha = eps / steps\n",
    "    x_adv = tf.identity(x)\n",
    "    momentum = 0\n",
    "\n",
    "    for _ in range(steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x_adv)\n",
    "            pred = model(x_adv)\n",
    "            loss = tf.keras.losses.binary_crossentropy(y, pred)\n",
    "\n",
    "        grad = tape.gradient(loss, x_adv)\n",
    "        grad_norm = grad / (tf.reduce_mean(tf.abs(grad)) + 1e-8)\n",
    "        momentum = decay * momentum + grad_norm\n",
    "        x_adv = x_adv + alpha * tf.sign(momentum)\n",
    "        x_adv = tf.clip_by_value(x_adv, 0, 1)\n",
    "\n",
    "    return x_adv\n",
    "\n",
    "X_adv_mi = mi_fgsm_attack(cnn, X_test_tensor, y_test_tensor, eps=0.05, steps=10)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4️⃣  Evaluation helper\n",
    "# ============================================================\n",
    "def evaluate(model, X, y_true):\n",
    "    y_prob = model.predict(X, verbose=0)\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\":  accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\":    recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\":        f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"cm\":        confusion_matrix(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5️⃣  RUN EVALUATIONS\n",
    "# ============================================================\n",
    "metrics_clean = evaluate(cnn, X_test_cnn,  y_test_np)\n",
    "metrics_fgsm  = evaluate(cnn, X_adv_fgsm,  y_test_np)\n",
    "metrics_mi    = evaluate(cnn, X_adv_mi,    y_test_np)\n",
    "\n",
    "print(\"\\n===== CLEAN =====\")\n",
    "print(metrics_clean)\n",
    "\n",
    "print(\"\\n===== FGSM =====\")\n",
    "print(metrics_fgsm)\n",
    "\n",
    "print(\"\\n===== MI-FGSM =====\")\n",
    "print(metrics_mi)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
